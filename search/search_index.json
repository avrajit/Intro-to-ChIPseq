{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction to ChIP-seq using high performance computing\n\n\n\n\n\n\n\n\nAudience\n\n\nComputational Skills\n\n\nPrerequisites\n\n\nDuration\n\n\n\n\n\n\n\n\n\n\nBiologists\n\n\nBeginner/Intermediate\n\n\nNone\n\n\n3-day workshop (~19.5 hours of trainer-led time)\n\n\n\n\n\n\n\n\nDescription\n\n\nThis repository has teaching materials for a 3-day Introduction to ChIP-sequencing data analysis workshop. This workshop focuses on teaching basic computational skills to enable the effective use of an high-performance computing environment to implement a ChIP-seq data analysis workflow. It includes an introduction to shell (bash) and shell scripting. In addition to running the ChIP-seq workflow from FASTQ files to peak calls and nearest gene annotations, the workshop covers best practice guidlelines for ChIP-seq experimental design and data organization/management and quality control.\n\n\n\n\nThese materials were developed for a trainer-led workshop, but are also amenable to self-guided learning.\n\n\n\n\nLearning Objectives\n\n\n\n\nUnderstand the necessity for, and use of, the command line interface (bash) and HPC for analyzing high-throughput sequencing data.\n\n\nUnderstand best practices for designing a ChIP-seq experiment and analysis the resulting data.\n\n\n\n\n\n\nThese materials have been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n\n\n\n\nSome materials used in these lessons were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/). \nAll Data Carpentry instructional material is made available under the \nCreative Commons Attribution license\n (CC BY 4.0).", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction-to-chip-seq-using-high-performance-computing", 
            "text": "Audience  Computational Skills  Prerequisites  Duration      Biologists  Beginner/Intermediate  None  3-day workshop (~19.5 hours of trainer-led time)", 
            "title": "Introduction to ChIP-seq using high performance computing"
        }, 
        {
            "location": "/#description", 
            "text": "This repository has teaching materials for a 3-day Introduction to ChIP-sequencing data analysis workshop. This workshop focuses on teaching basic computational skills to enable the effective use of an high-performance computing environment to implement a ChIP-seq data analysis workflow. It includes an introduction to shell (bash) and shell scripting. In addition to running the ChIP-seq workflow from FASTQ files to peak calls and nearest gene annotations, the workshop covers best practice guidlelines for ChIP-seq experimental design and data organization/management and quality control.   These materials were developed for a trainer-led workshop, but are also amenable to self-guided learning.", 
            "title": "Description"
        }, 
        {
            "location": "/#learning-objectives", 
            "text": "Understand the necessity for, and use of, the command line interface (bash) and HPC for analyzing high-throughput sequencing data.  Understand best practices for designing a ChIP-seq experiment and analysis the resulting data.    These materials have been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.   Some materials used in these lessons were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/). \nAll Data Carpentry instructional material is made available under the  Creative Commons Attribution license  (CC BY 4.0).", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/schedule/", 
            "text": "Workshop Schedule\n\n\nDay 1\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00 - 9:40\n\n\nWorkshop Introduction\n\n\nRadhika\n\n\n\n\n\n\n9:40 - 10:30\n\n\nIntroduction to the Shell\n\n\nRadhika\n\n\n\n\n\n\n10:30 - 10:45\n\n\nBreak\n\n\n\n\n\n\n\n\n10:45 - 11:35\n\n\nIntroduction to the Shell (cont.)\n\n\nMeeta\n\n\n\n\n\n\n11:35 - 12:15\n\n\nSearching and Redirection\n\n\nMary\n\n\n\n\n\n\n12:15 - 13:15\n\n\nLunch\n\n\n\n\n\n\n\n\n13:15 - 13:45\n\n\nIntroduction to the Vim Text Editor\n\n\nRadhika\n\n\n\n\n\n\n13:45 - 15:00\n\n\nLoops and Shell Scripts\n\n\nMeeta\n\n\n\n\n\n\n15:00 - 15:15\n\n\nBreak\n\n\n\n\n\n\n\n\n15:15 - 15:45\n\n\nPermissions and Environment Variables\n\n\nMary\n\n\n\n\n\n\n15:45 - 17:00\n\n\nIntroduction to High-Performance Computing for HMS-RC's O2\n\n\nRadhika\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00 - 10:00\n\n\nIntroduction to ChIP-seq\n\n\nMeeta\n\n\n\n\n\n\n10:00 - 10:15\n\n\nBreak\n\n\n\n\n\n\n\n\n10:15 - 11:00\n\n\nProject Organization and Best Practices in Data Management\n\n\nRadhika\n\n\n\n\n\n\n11:00 - 11:50\n\n\nSequencing data QC using FastQC\n\n\nMary\n\n\n\n\n\n\n11:50 - 12:15 5\n\n\nAlignment theory\n\n\nMeeta\n\n\n\n\n\n\n12:15 - 13:15\n\n\nLunch\n\n\n\n\n\n\n\n\n13:15 - 14:00\n\n\nAlignment and filtering of reads\n\n\nMary\n\n\n\n\n\n\n14:00 - 15:20\n\n\nAutomating generation of alignment files\n\n\nRadhika\n\n\n\n\n\n\n15:20 - 15:35\n\n\nBreak\n\n\n\n\n\n\n\n\n15:35 - 15:5\n\n\nChIP-seq File Formats\n\n\nRadhika\n\n\n\n\n\n\n15:55 - 17:00\n\n\nPeak calling with MACS2\n\n\nMeeta\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00 - 9:20\n\n\nPeak calling with MACS2 (contd.)\n\n\nMeeta\n\n\n\n\n\n\n9:20 - 10:20\n\n\nAssessing ChIP quality using cross correlation\n\n\nMary\n\n\n\n\n\n\n10:20 - 10:35\n\n\nBreak\n\n\n\n\n\n\n\n\n10:35 - 11:35\n\n\nAssessing Peak calls and ChIP quality using ChIPQC\n\n\nMeeta\n\n\n\n\n\n\n11:35 - 12:00\n\n\nHandling Replicates with Bedtools\n\n\nRadhika\n\n\n\n\n\n\n12:00 - 13:00\n\n\nLunch\n\n\n\n\n\n\n\n\n13:00 - 13:50\n\n\nHandling Replicates with IDR\n\n\nRadhika\n\n\n\n\n\n\n13:50 - 15:10\n\n\nVisualization and exploration of ChIP-seq data\n\n\nMeeta\n\n\n\n\n\n\n15:10 - 15:25\n\n\nBreak\n\n\n\n\n\n\n\n\n15:25 - 15:50\n\n\nQualitative assessment using IGV\n\n\nMeeta\n\n\n\n\n\n\n15:50 - 16:20\n\n\nFunctional analysis\n\n\nMary\n\n\n\n\n\n\n16:20 - 16:40\n\n\nOverview of ChIP-seq workflow\n\n\nMeeta\n\n\n\n\n\n\n16:30 - 17:00\n\n\nWrap-up and Survey\n\n\nRadhika", 
            "title": "Class schedule"
        }, 
        {
            "location": "/schedule/#workshop-schedule", 
            "text": "", 
            "title": "Workshop Schedule"
        }, 
        {
            "location": "/schedule/#day-1", 
            "text": "Time  Topic  Instructor      9:00 - 9:40  Workshop Introduction  Radhika    9:40 - 10:30  Introduction to the Shell  Radhika    10:30 - 10:45  Break     10:45 - 11:35  Introduction to the Shell (cont.)  Meeta    11:35 - 12:15  Searching and Redirection  Mary    12:15 - 13:15  Lunch     13:15 - 13:45  Introduction to the Vim Text Editor  Radhika    13:45 - 15:00  Loops and Shell Scripts  Meeta    15:00 - 15:15  Break     15:15 - 15:45  Permissions and Environment Variables  Mary    15:45 - 17:00  Introduction to High-Performance Computing for HMS-RC's O2  Radhika", 
            "title": "Day 1"
        }, 
        {
            "location": "/schedule/#day-2", 
            "text": "Time  Topic  Instructor      9:00 - 10:00  Introduction to ChIP-seq  Meeta    10:00 - 10:15  Break     10:15 - 11:00  Project Organization and Best Practices in Data Management  Radhika    11:00 - 11:50  Sequencing data QC using FastQC  Mary    11:50 - 12:15 5  Alignment theory  Meeta    12:15 - 13:15  Lunch     13:15 - 14:00  Alignment and filtering of reads  Mary    14:00 - 15:20  Automating generation of alignment files  Radhika    15:20 - 15:35  Break     15:35 - 15:5  ChIP-seq File Formats  Radhika    15:55 - 17:00  Peak calling with MACS2  Meeta", 
            "title": "Day 2"
        }, 
        {
            "location": "/schedule/#day-3", 
            "text": "Time  Topic  Instructor      9:00 - 9:20  Peak calling with MACS2 (contd.)  Meeta    9:20 - 10:20  Assessing ChIP quality using cross correlation  Mary    10:20 - 10:35  Break     10:35 - 11:35  Assessing Peak calls and ChIP quality using ChIPQC  Meeta    11:35 - 12:00  Handling Replicates with Bedtools  Radhika    12:00 - 13:00  Lunch     13:00 - 13:50  Handling Replicates with IDR  Radhika    13:50 - 15:10  Visualization and exploration of ChIP-seq data  Meeta    15:10 - 15:25  Break     15:25 - 15:50  Qualitative assessment using IGV  Meeta    15:50 - 16:20  Functional analysis  Mary    16:20 - 16:40  Overview of ChIP-seq workflow  Meeta    16:30 - 17:00  Wrap-up and Survey  Radhika", 
            "title": "Day 3"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/", 
            "text": "Approximate time: 45 minutes\n\n\nLearning Objectives\n\n\n\n\nDescribe the best practices for designing a ChIP-seq experiment\n\n\nRecognize the need for data management and project organization\n\n\n\n\nIntroduction to ChIP-Seq\n\n\nChromatin immunoprecipitation (ChIP) experiments are performed to identify DNA bound to specific (chromatin) proteins of interest. The first step involves isolating the chromatin and immunoprecipitating (IP) fragements with an antibody against the protein of interest. In ChIP-seq, the immunoprecipitated DNA fragments are then sequenced, followed by identification of enriched regions of DNA or peaks. These peak calls can then be used to make biological inferences by determining the associated genomic features and/or over-represented sequence motifs. \n\n\n\n\nDuring this session we will be performing a complete workflow for ChIP-Seq analysis, starting with experimental design and generation of the raw sequencing reads and ending with functional enrichment analyses and motif discovery.\n\n\n\n\nExperimental design and library preparation\n\n\nSeveral steps are involved in the library preparation of protein-bound DNA fragments for sequencing: \n\n\n\n\n\n\nAfter the chromatin is isolated from the cell, proteins are cross-linked to the DNA\n\n\nThe DNA is sheared into fragments (sonication)\n\n\nA protein-specific antibody is used to immunoprecipitate the protein-bound DNA fragments\n\n\nThe crosslink is reversed and DNA purified\n\n\nDNA fragments are size selected and amplified using PCR\n\n\n\n\nWithin the DNA fragments enriched for the regions binding to a protein of interest, only a fraction correspond to actual signal. The proportion of DNA fragments containing the actual binding site of the protein depends on the \nnumber of active binding sites, the number of starting genomes, and the efficiency of the IP\n. \n\n\nIn addition, when performing ChIP-Seq, some sequences may appear enriched due to the following:\n\n\n\n\nOpen chromatin regions are fragmented more easily than closed regions\n\n\nRepetitive sequences might seem to be enriched (copy number inaccuracies in genome assembly)\n\n\nUneven distribution of sequence reads across the genome\n\n\n\n\nTherefore, proper controls are essential. A ChIP-Seq peak should be compared with the same region of the genome in a matched control.\n\n\n\n\nThe same starting material should be divided to be used for both the protein-specific IP and the control. The control sample can be generated by one of the following recommended techniques: \n\n\n\n\nNo IP (input DNA) \n\n\nNo antibody (\"mock IP\")\n\n\nNon-specific antibody (IgG \"mock IP\")\n\n\n\n\n\n\nSetting up\n\n\nSince we are going to be working with this data on our remote server, \nO2\n, we first need to log onto the server. \n\n\nType in the following command with your username to login:\n\n\nssh username@o2.hms.harvard.edu\n\n\n\n\nNext we will start an interactive session on O2 with 2 cores (add the \n-n 2\n):\n\n\n$ srun --pty -p short -t 0-12:00 --mem 8G -n 2 --reservation=HBC bash\n\n\n\n\nMake sure that your command prompt is now preceded by a character string that contains the word \"compute\".\n\n\nData Management\n\n\nOne of the most important parts of research that involves large amounts of data, is how best to manage it. We tend to prioritize the analysis, but there are many other important aspects that are  often overlooked in the excitement to get a first look at new data. \n\n\nThe data management lifecycle displayed below, courtesy of the \nHMS Data Management Working Group\n, illustrates some things to consider beyond the data creation and analysis components:\n\n\n\n\nImage aquired from the \nHarvard Biomedical Data Management Website\n\n\nWe will cover some parts of this lifecycle by talking about best practices for the \nResearch\n half of the above lifecycle. Later in this workshop we will talk a little more about the data storage. For more information about the full lifecycle and more guidelines for data management, please look at the resources linked below.\n\n\nResources\n\n\n\n\nThe \nHMS Data Management Working Group's website\n\n\nA guide from the \nHarvard library\n.\n\n\n\n\nPlanning\n\n\nYou should approach your sequencing project in a very similar way to how you do a biological experiment, and ideally, begins with \nexperimental design\n. We're going to assume that you've already designed a beautiful sequencing experiment to address your biological question, collected appropriate samples, and that you have enough statistical power.\n\n\nDuring this stage it is important to keep track of how the experiment was performed and clearly tracking the source of starting materials and kits used. It is also best practice to include information about any small variations within the experiment or variation relative to standard experiments. \n\n\nOrganization\n\n\nEvery computational analysis you do is going to spawn many files, and inevitability you'll want to run some of those analyses again. For each experiment you work on and analyze data for, it is considered best practice to get organized by creating a planned storage space (directory structure).\n\n\nWe will start by creating a directory that we can use for the rest of the ChIP-seq session.\n\n\nFirst, make sure that you are in your home directory.\n\n\n$ cd\n$ pwd\n\n\n\n\nThis should return \n/home/username\n.\n\n\nCreate a \nchipseq\n directory and change directories into it:\n\n\n$ mkdir chipseq\n\n$ cd chipseq\n\n\n\n\nNow that we have a project directory, we can set up the following structure within it to keep files organized.\n\n\nchipseq/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 meta/\n\u251c\u2500\u2500 raw_data/\n\u251c\u2500\u2500 reference_data/\n\u251c\u2500\u2500 results/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bowtie2/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fastqc/\n\u2514\u2500\u2500 scripts/\n\n\n\n\n$ mkdir raw_data reference_data scripts logs meta\n\n$ mkdir -p results/fastqc results/bowtie2\n\n$ tree     # this will show you the directory structure you just created\n\n\n\n\n\n\nNote that we are using the parents flag (\n-p\n or \n--parents\n) with \nmkdir\n to complete the file path by creating any parent directories that do not exist. In our case, we have not yet created the \nresults\n directory and so since it does not exist it will be created. This flag can be very useful when scripting workflows. \n\n\n\n\nThis is a generic directory structure and can be tweaked based on personal preference and analysis workflow.\n\n\n\n\nlogs\n: to keep track of the commands run and the specific parameters used, but also to have a record of any standard output that is generated while running the command. \n\n\nmeta\n: for any information that describes the samples you are using, which we refer to as \nmetadata\n. We will discuss this in more detail as it pertains to our example dataset, later in this lesson.\n\n\nraw_data\n: for any \nunmodified\n (raw) data obtained prior to computational analysis here, e.g. FASTQ files from the sequencing center. We strongly recommend leaving this directory unmodified through the analysis.\n\n\nreference_data\n: for known information related to the reference genome that will be used in the analysis, e.g. genome sequence (FASTA), gene annotation file (GTF) associated with the genome.\n\n\nresults\n: for output from the different tools you implement in your workflow. Create sub-folders specific to each tool/step of the workflow within this folder. \n\n\nscripts\n: for scripts that you write and use to run analyses/workflow.\n\n\n\n\nNow that we have the directory structure created, let's copy over the data to perform our quality control and alignment, including our FASTQ files and reference data files:\n\n\n$ cp /n/groups/hbctraining/chip-seq/raw_fastq/*fastq raw_data/\n\n$ cp /n/groups/hbctraining/chip-seq/reference_data/chr12* reference_data/\n\n\n\n\nNow we are all set up for our analysis!\n\n\n\n\nFile naming conventions\n\n\nAnother aspect of staying organized is making sure that all the filenames in an analysis are as consistent as possible, and are not things like \nalignment1.bam\n, but more like \n20170823_kd_rep1_gmap-1.4.bam\n. \nThis link\n and \nthis slideshow\n have some good guidelines for file naming dos and don'ts.\n\n\n\n\nDocumentation\n\n\nDocumentation doesn't stop at the sequencer!\n Keeping notes on what happened in what order, and what was done, is essential for reproducible research.\n\n\nLog files\n\n\nIn your lab notebook, you likely keep track of the different reagents and kits used for a specific protocol. Similarly, recording information about the tools and parameters is important for documenting your computational experiments. \n\n\n\n\nMake note of the software you use.\n Do your research and find out what tools are best for the data you are working with. Don't just work with tools that you are able to easily install.\n\n\nKeep track of software versions.\n Keep up with the literature and make sure you are using the most up-to-date versions.\n\n\nRecord information on parameters used and summary statistics\n at every step (e.g., how many adapters were removed, how many reads did not align)\n\n\nA general rule of thumb is to test on a single sample or a subset of the data before running your entire dataset through. This will allow you to debug quicker and give you a chance to also get a feel for the tool and the different parameters.\n\n\nDifferent tools have different ways of reporting log messages and you might have to experiment a bit to figure out what output to capture. You can redirect standard output with the \n symbol which is equivalent to \n1\n (standard out)\n; other tools might require you to use \n2\n to re-direct the \nstandard error\n instead.\n\n\n\n\n\n\n\n\nREADME files\n\n\nAfter setting up the directory structure and when the analysis is running it is useful to have a \nREADME file\n within your project directory\n. This file will usually contain a quick one line summary about the project and any other lines that follow will describe the files/directories found within it. An example README is shown below. Within each sub-directory you can also include README files to describe the analysis and the files that were generated.\n\n\n## README ##\n## This directory contains data generated during the Intro to ChIP-seq course\n## Date: \n\nThere are six subdirectories in this directory:\n\nraw_data : contains raw data\nmeta:  contains...\nlogs:\nreference_data:\nresults:\nscripts:\n\n\n\n\n\n\nHomework Exercise\n\n\n\n\nCreate a README for the \nchipseq/\n folder (hint: use \nvim\n to create the file). Give a short description of the project and as homework add brief descriptions of the types of files you will be storing within each of the sub-directories. \n\n\n\n\n\n\nExploring the example dataset\n\n\nOur goal for this session is to compare the the binding profiles of \nNanog\n and \nPou5f1\n (Oct4). The ChIP was performed on H1 human embryonic stem cell line (h1-ESC) cells, and sequenced using Illumina. The datasets were obtained from the \nHAIB TFBS ENCODE collection\n. These 2 transcription factors are involved in \nstem cell pluripotency\n and one of the goals is to understand their roles, individually and together, in transriptional regulation. \n\n\nTwo replicates were collected and each was divided into 3 aliquots for the following:\n\n\n\n\nNanog IP\n\n\nPou5f1 IP\n\n\nControl input DNA\n\n\n\n\n\n\nFor these 6 samples, we will be using reads from only a 32.8 Mb of chromosome 12 (chr12:1,000,000-33,800,000), so we can get through the workflow in a reasonable amount of time. \n\n\nThe ChIP-seq workflow\n\n\nBelow is the workflow that we will be using today. Bioinformatics workflows, like this one, adopt a plug-and-play approach in that the output of one tool can be easily used as input to another tool without any extensive configuration. The tools that are used to analyze data at different stages of the workflow are built under the assumption that the data will be provided in a specific format to facilitate a more streamlined analysis. Hence different tools that can be implemented at specific steps in the workflow will have similar outputs.\n\n\n   \n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Project Organization and Best Practices in Data Management"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#learning-objectives", 
            "text": "Describe the best practices for designing a ChIP-seq experiment  Recognize the need for data management and project organization", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#introduction-to-chip-seq", 
            "text": "Chromatin immunoprecipitation (ChIP) experiments are performed to identify DNA bound to specific (chromatin) proteins of interest. The first step involves isolating the chromatin and immunoprecipitating (IP) fragements with an antibody against the protein of interest. In ChIP-seq, the immunoprecipitated DNA fragments are then sequenced, followed by identification of enriched regions of DNA or peaks. These peak calls can then be used to make biological inferences by determining the associated genomic features and/or over-represented sequence motifs.    During this session we will be performing a complete workflow for ChIP-Seq analysis, starting with experimental design and generation of the raw sequencing reads and ending with functional enrichment analyses and motif discovery.", 
            "title": "Introduction to ChIP-Seq"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#experimental-design-and-library-preparation", 
            "text": "Several steps are involved in the library preparation of protein-bound DNA fragments for sequencing:     After the chromatin is isolated from the cell, proteins are cross-linked to the DNA  The DNA is sheared into fragments (sonication)  A protein-specific antibody is used to immunoprecipitate the protein-bound DNA fragments  The crosslink is reversed and DNA purified  DNA fragments are size selected and amplified using PCR   Within the DNA fragments enriched for the regions binding to a protein of interest, only a fraction correspond to actual signal. The proportion of DNA fragments containing the actual binding site of the protein depends on the  number of active binding sites, the number of starting genomes, and the efficiency of the IP .   In addition, when performing ChIP-Seq, some sequences may appear enriched due to the following:   Open chromatin regions are fragmented more easily than closed regions  Repetitive sequences might seem to be enriched (copy number inaccuracies in genome assembly)  Uneven distribution of sequence reads across the genome   Therefore, proper controls are essential. A ChIP-Seq peak should be compared with the same region of the genome in a matched control.   The same starting material should be divided to be used for both the protein-specific IP and the control. The control sample can be generated by one of the following recommended techniques:    No IP (input DNA)   No antibody (\"mock IP\")  Non-specific antibody (IgG \"mock IP\")", 
            "title": "Experimental design and library preparation"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#setting-up", 
            "text": "Since we are going to be working with this data on our remote server,  O2 , we first need to log onto the server.   Type in the following command with your username to login:  ssh username@o2.hms.harvard.edu  Next we will start an interactive session on O2 with 2 cores (add the  -n 2 ):  $ srun --pty -p short -t 0-12:00 --mem 8G -n 2 --reservation=HBC bash  Make sure that your command prompt is now preceded by a character string that contains the word \"compute\".", 
            "title": "Setting up"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#data-management", 
            "text": "One of the most important parts of research that involves large amounts of data, is how best to manage it. We tend to prioritize the analysis, but there are many other important aspects that are  often overlooked in the excitement to get a first look at new data.   The data management lifecycle displayed below, courtesy of the  HMS Data Management Working Group , illustrates some things to consider beyond the data creation and analysis components:   Image aquired from the  Harvard Biomedical Data Management Website  We will cover some parts of this lifecycle by talking about best practices for the  Research  half of the above lifecycle. Later in this workshop we will talk a little more about the data storage. For more information about the full lifecycle and more guidelines for data management, please look at the resources linked below.  Resources   The  HMS Data Management Working Group's website  A guide from the  Harvard library .", 
            "title": "Data Management"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#planning", 
            "text": "You should approach your sequencing project in a very similar way to how you do a biological experiment, and ideally, begins with  experimental design . We're going to assume that you've already designed a beautiful sequencing experiment to address your biological question, collected appropriate samples, and that you have enough statistical power.  During this stage it is important to keep track of how the experiment was performed and clearly tracking the source of starting materials and kits used. It is also best practice to include information about any small variations within the experiment or variation relative to standard experiments.", 
            "title": "Planning"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#organization", 
            "text": "Every computational analysis you do is going to spawn many files, and inevitability you'll want to run some of those analyses again. For each experiment you work on and analyze data for, it is considered best practice to get organized by creating a planned storage space (directory structure).  We will start by creating a directory that we can use for the rest of the ChIP-seq session.  First, make sure that you are in your home directory.  $ cd\n$ pwd  This should return  /home/username .  Create a  chipseq  directory and change directories into it:  $ mkdir chipseq\n\n$ cd chipseq  Now that we have a project directory, we can set up the following structure within it to keep files organized.  chipseq/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 meta/\n\u251c\u2500\u2500 raw_data/\n\u251c\u2500\u2500 reference_data/\n\u251c\u2500\u2500 results/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bowtie2/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fastqc/\n\u2514\u2500\u2500 scripts/  $ mkdir raw_data reference_data scripts logs meta\n\n$ mkdir -p results/fastqc results/bowtie2\n\n$ tree     # this will show you the directory structure you just created   Note that we are using the parents flag ( -p  or  --parents ) with  mkdir  to complete the file path by creating any parent directories that do not exist. In our case, we have not yet created the  results  directory and so since it does not exist it will be created. This flag can be very useful when scripting workflows.    This is a generic directory structure and can be tweaked based on personal preference and analysis workflow.   logs : to keep track of the commands run and the specific parameters used, but also to have a record of any standard output that is generated while running the command.   meta : for any information that describes the samples you are using, which we refer to as  metadata . We will discuss this in more detail as it pertains to our example dataset, later in this lesson.  raw_data : for any  unmodified  (raw) data obtained prior to computational analysis here, e.g. FASTQ files from the sequencing center. We strongly recommend leaving this directory unmodified through the analysis.  reference_data : for known information related to the reference genome that will be used in the analysis, e.g. genome sequence (FASTA), gene annotation file (GTF) associated with the genome.  results : for output from the different tools you implement in your workflow. Create sub-folders specific to each tool/step of the workflow within this folder.   scripts : for scripts that you write and use to run analyses/workflow.   Now that we have the directory structure created, let's copy over the data to perform our quality control and alignment, including our FASTQ files and reference data files:  $ cp /n/groups/hbctraining/chip-seq/raw_fastq/*fastq raw_data/\n\n$ cp /n/groups/hbctraining/chip-seq/reference_data/chr12* reference_data/  Now we are all set up for our analysis!", 
            "title": "Organization"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#file-naming-conventions", 
            "text": "Another aspect of staying organized is making sure that all the filenames in an analysis are as consistent as possible, and are not things like  alignment1.bam , but more like  20170823_kd_rep1_gmap-1.4.bam .  This link  and  this slideshow  have some good guidelines for file naming dos and don'ts.", 
            "title": "File naming conventions"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#documentation", 
            "text": "Documentation doesn't stop at the sequencer!  Keeping notes on what happened in what order, and what was done, is essential for reproducible research.", 
            "title": "Documentation"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#log-files", 
            "text": "In your lab notebook, you likely keep track of the different reagents and kits used for a specific protocol. Similarly, recording information about the tools and parameters is important for documenting your computational experiments.    Make note of the software you use.  Do your research and find out what tools are best for the data you are working with. Don't just work with tools that you are able to easily install.  Keep track of software versions.  Keep up with the literature and make sure you are using the most up-to-date versions.  Record information on parameters used and summary statistics  at every step (e.g., how many adapters were removed, how many reads did not align)  A general rule of thumb is to test on a single sample or a subset of the data before running your entire dataset through. This will allow you to debug quicker and give you a chance to also get a feel for the tool and the different parameters.  Different tools have different ways of reporting log messages and you might have to experiment a bit to figure out what output to capture. You can redirect standard output with the   symbol which is equivalent to  1  (standard out) ; other tools might require you to use  2  to re-direct the  standard error  instead.", 
            "title": "Log files"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#readme-files", 
            "text": "After setting up the directory structure and when the analysis is running it is useful to have a  README file  within your project directory . This file will usually contain a quick one line summary about the project and any other lines that follow will describe the files/directories found within it. An example README is shown below. Within each sub-directory you can also include README files to describe the analysis and the files that were generated.  ## README ##\n## This directory contains data generated during the Intro to ChIP-seq course\n## Date: \n\nThere are six subdirectories in this directory:\n\nraw_data : contains raw data\nmeta:  contains...\nlogs:\nreference_data:\nresults:\nscripts:", 
            "title": "README files"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#homework-exercise", 
            "text": "Create a README for the  chipseq/  folder (hint: use  vim  to create the file). Give a short description of the project and as homework add brief descriptions of the types of files you will be storing within each of the sub-directories.", 
            "title": "Homework Exercise"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#exploring-the-example-dataset", 
            "text": "Our goal for this session is to compare the the binding profiles of  Nanog  and  Pou5f1  (Oct4). The ChIP was performed on H1 human embryonic stem cell line (h1-ESC) cells, and sequenced using Illumina. The datasets were obtained from the  HAIB TFBS ENCODE collection . These 2 transcription factors are involved in  stem cell pluripotency  and one of the goals is to understand their roles, individually and together, in transriptional regulation.   Two replicates were collected and each was divided into 3 aliquots for the following:   Nanog IP  Pou5f1 IP  Control input DNA    For these 6 samples, we will be using reads from only a 32.8 Mb of chromosome 12 (chr12:1,000,000-33,800,000), so we can get through the workflow in a reasonable amount of time.", 
            "title": "Exploring the example dataset"
        }, 
        {
            "location": "/01_Intro_chipseq_data_organization/#the-chip-seq-workflow", 
            "text": "Below is the workflow that we will be using today. Bioinformatics workflows, like this one, adopt a plug-and-play approach in that the output of one tool can be easily used as input to another tool without any extensive configuration. The tools that are used to analyze data at different stages of the workflow are built under the assumption that the data will be provided in a specific format to facilitate a more streamlined analysis. Hence different tools that can be implemented at specific steps in the workflow will have similar outputs.        This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "The ChIP-seq workflow"
        }, 
        {
            "location": "/02_QC_FASTQC/", 
            "text": "Contributors: Mary Piper, Radhika Khetani\n\n\nApproximate time: 55 minutes\n\n\nLearning Objectives\n\n\n\n\nBecome familiar with the Illumina sequencing technology\n\n\nUnderstanding how to use modules in the cluster environment\n\n\nEvaluate the quality of your sequencing data using FastQC\n\n\n\n\nQuality control of sequence reads\n\n\n\n\nNow that we have our files and directory structure, we are ready to begin our ChIP-Seq analysis. For any NGS analysis method, our first step in the workflow is to explore the quality of our reads prior to aligning them to the reference genome and proceeding with downstream analyses. \n\n\nUnderstanding the Illumina sequencing technology\n\n\nBefore we can assess the quality of our reads, it would be helpful to know a little bit about how these reads were generated. Since our data was sequenced on an Illumina sequencer we will introduce you to their Sequencng by Synthesis methodology, however keep in mind there are other technologies and the way reads are generated will vary (as will the associated biases observed in your data). \n\n\n\n\nAn \nanimation of the Sequencing by Synthesis is most helpful\n (rather than reading through lines of text), and so we would like you to take five minutes and watch \nthis YouTube video\n from Illumina.\n\n\nUnmapped read data (FASTQ)\n\n\nThe \nFASTQ\n file format is the defacto file format for sequence reads generated from next-generation sequencing technologies. This file format evolved from FASTA in that it contains sequence data, but also contains quality information. Similar to FASTA, the FASTQ file begins with a header line. The difference is that the FASTQ header is denoted by a \n@\n character. For a single record (sequence read) there are four lines, each of which are described below:\n\n\n\n\n\n\n\n\nLine\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAlways begins with '@' and then information about the read\n\n\n\n\n\n\n2\n\n\nThe actual DNA sequence\n\n\n\n\n\n\n3\n\n\nAlways begins with a '+' and sometimes the same info in line 1\n\n\n\n\n\n\n4\n\n\nHas a string of characters which represent the quality scores; must have same number of characters as line 2\n\n\n\n\n\n\n\n\nLet's use the following read as an example:\n\n\n@HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\n@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################\n\n\n\n\nAs mentioned previously, line 4 has characters encoding the quality of each nucleotide in the read. The legend below provides the mapping of quality scores (Phred-33) to the quality encoding characters. \nDifferent quality encoding scales exist (differing by offset in the ASCII table), but note the most commonly used one is fastqsanger.\n\n\n```\n Quality encoding: !\"#$%\n'()*+,-./0123456789:;\n=\n?@ABCDEFGHI\n                   |         |         |         |         |\n    Quality score: 0........10........20........30........40                                \n\n\n\nUsing the quality encoding character legend, the first nucelotide in the read (C) is called with a quality score of 31 and our Ns are called with a score of 2. **As you can tell by now, this is a bad read.** \n\nEach quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based and is calculated as:\n\n    Q = -10 x log10(P), where P is the probability that a base call is erroneous\n\nThese probabaility values are the results from the base calling algorithm and dependent on how much signal was captured for the base incorporation. The score values can be interpreted as follows:\n\n|Phred Quality Score |Probability of incorrect base call |Base call accuracy|\n|:-------------------|:---------------------------------:|-----------------:|\n|10 |1 in 10 |  90%|\n|20 |1 in 100|  99%|\n|30 |1 in 1000| 99.9%|\n|40 |1 in 10,000|   99.99%|\n|50 |1 in 100,000|  99.999%|\n|60 |1 in 1,000,000|    99.9999%|\n\nTherefore, for the first nucleotide in the read (C), there is less than a 1 in 1000 chance that the base was called incorrectly. Whereas, for the the end of the read there is greater than 50% probabaility that the base is called incorrectly.\n\n## Assessing quality with FastQC\n\nNow we understand what information is stored in a FASTQ file, the next step is to examine quality metrics for our data.\n\n[FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) provides a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\n\nThe main functions of FastQC are:\n\n* Import of data from BAM, SAM or FastQ files (any variant)\n* Providing a quick overview to tell you in which areas there may be problems\n* Summary graphs and tables to quickly assess your data\n* Export of results to an HTML based permanent report\n* Offline operation to allow automated generation of reports without running the interactive application\n\n### Run FastQC  \n\nLet's run FastQC on all of our files. \n\nChange directories to the `raw_data` folder and check the contents\n\n```bash\n$ cd ~/chipseq/raw_data \n\n$ ls -l\n\n\n\n\nBefore we start using any software, we either have to check if it's available on the cluster, and if it is we have to load it into our environment (or \n$PATH\n). On the O2 cluster, we can check for, and load packages (or modules) using the \nLMOD\n system. \n\n\nIf we check which modules we currently have loaded, we should not see FastQC.\n\n\n$ module list\n\n\n\n\nThis is because the FastQC program is not in our $PATH (i.e. its not in a directory that unix will automatically check to run commands/programs).\n\n\n$ echo $PATH\n\n\n\n\nTo find the FastQC module to load we need to search the versions available:\n\n\n$ module spider\n\n\n\n\nThen we can load the FastQC module:\n\n\n$ module load fastqc/0.11.3\n\n\n\n\nOnce a module for a tool is loaded, you have essentially made it directly available to you like any other basic UNIX command.\n\n\n$ module list\n\n$ echo $PATH\n\n\n\n\nFastQC will accept multiple file names as input, so we can use the \n*.fq\n wildcard.\n\n\n$ fastqc *.fastq\n\n\n\n\nDid you notice how each file was processed serially? How do we speed this up?\n\n\nExit the interactive session and once you are on a \"login node,\" start a new interactive session with 6 cores. Now we can use the multi-threading functionality of FastQC to speed this up by running 6 jobs at once, one job for one file.\n\n\n$ exit  #exit the current interactive session\n\n$ srun --pty -n 6 -p short -t 0-12:00 --mem 8G --reservation=HBC /bin/bash  #start a new one with 6 cpus (-n 6) and 8G RAM (--mem 8G)\n\n$ module load fastqc/0.11.3  #reload the module for the new session\n\n$ cd ~/chipseq/raw_data\n\n$ fastqc -t 6 *.fastq  #note the extra parameter we specified for 6 threads\n\n\n\n\nHow did I know about the -t argument for FastQC?\n\n\n$ fastqc --help\n\n\n\n\nNow, move all of the \nfastqc\n files to the \nresults/fastqc\n directory:\n\n\n$ mv *fastqc* ../results/fastqc/\n\n\n\n\nFastQC Results\n\n\nLet's take a closer look at the files generated by FastQC:\n\n\n$ ls -lh ../results/fastqc/\n\n\nHTML reports\n\n\nThe .html files contain the final reports generated by fastqc, let's take a closer look at them. Transfer the file for \nH1hesc_Input_Rep1_chr12.fastq\n over to your laptop via \nFileZilla\n.\n\n\nFilezilla - Step 1\n\n\nOpen \nFileZilla\n, and click on the File tab. Choose 'Site Manager'.\n\n\n\n\nFilezilla - Step 2\n\n\nWithin the 'Site Manager' window, do the following: \n\n\n\n\nClick on 'New Site', and name it something intuitive (e.g. O2)\n\n\nHost: transfer.rc.hms.harvard.edu \n\n\nProtocol: SFTP - SSH File Transfer Protocol\n\n\nLogon Type: Normal\n\n\nUser: training_account\n\n\nPassword: password for training_account\n\n\nClick 'Connect'\n\n\n\n\n  \n\n\nThe \n\"Per base sequence quality\"\n plot is the most important analysis module in FastQC for ChIP-Seq; it provides the distribution of quality scores across all bases at each position in the reads. This information can help determine whether there were any problems at the sequencing facility during the sequencing of your data. Generally, we expect a decrease in quality towards the ends of the reads, but we shouldn't see any quality drops at the beginning or in the middle of the reads.\n\n\n\n\nBased on the sequence quality plot, we see the majority of the reads have high quality, but the whiskers drop into the poor quality regions, indicating that a significant number of reads have low quality bases across the reads. The poor quality reads in the middle of the sequence would be concerning if this was our dataset, and we would probably want to contact the sequencing facility. However, this dataset was created artifically, so does not indicate a problem at the sequencing facility. Trimming could be performed from both ends of the sequences, or we can use an alignment tool that can ignore these poor quality bases at the ends of reads (soft clip). \n\n\nThis is the main plot explored for ChIP-seq, but if you would like to go through the remaining plots/metrics, FastQC has a really well documented \nmanual page\n with \nmore details\n about all the plots in the report. We recommend looking at \nthis post\n for more information on what bad plots look like and what they mean for your data. Also, FastQC is just an indicator of what's going on with your data, don't take the \"PASS\"es and \"FAIL\"s too seriously.\n\n\n\n\nWe also have a \nslidedeck\n of error profiles for Illumina sequencing, where we discuss specific FASTQC plots and possible sources of these types of errors.\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Sequencing data QC using FastQC"
        }, 
        {
            "location": "/02_QC_FASTQC/#learning-objectives", 
            "text": "Become familiar with the Illumina sequencing technology  Understanding how to use modules in the cluster environment  Evaluate the quality of your sequencing data using FastQC", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/02_QC_FASTQC/#quality-control-of-sequence-reads", 
            "text": "Now that we have our files and directory structure, we are ready to begin our ChIP-Seq analysis. For any NGS analysis method, our first step in the workflow is to explore the quality of our reads prior to aligning them to the reference genome and proceeding with downstream analyses.", 
            "title": "Quality control of sequence reads"
        }, 
        {
            "location": "/02_QC_FASTQC/#understanding-the-illumina-sequencing-technology", 
            "text": "Before we can assess the quality of our reads, it would be helpful to know a little bit about how these reads were generated. Since our data was sequenced on an Illumina sequencer we will introduce you to their Sequencng by Synthesis methodology, however keep in mind there are other technologies and the way reads are generated will vary (as will the associated biases observed in your data).    An  animation of the Sequencing by Synthesis is most helpful  (rather than reading through lines of text), and so we would like you to take five minutes and watch  this YouTube video  from Illumina.", 
            "title": "Understanding the Illumina sequencing technology"
        }, 
        {
            "location": "/02_QC_FASTQC/#unmapped-read-data-fastq", 
            "text": "The  FASTQ  file format is the defacto file format for sequence reads generated from next-generation sequencing technologies. This file format evolved from FASTA in that it contains sequence data, but also contains quality information. Similar to FASTA, the FASTQ file begins with a header line. The difference is that the FASTQ header is denoted by a  @  character. For a single record (sequence read) there are four lines, each of which are described below:     Line  Description      1  Always begins with '@' and then information about the read    2  The actual DNA sequence    3  Always begins with a '+' and sometimes the same info in line 1    4  Has a string of characters which represent the quality scores; must have same number of characters as line 2     Let's use the following read as an example:  @HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\n@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################  As mentioned previously, line 4 has characters encoding the quality of each nucleotide in the read. The legend below provides the mapping of quality scores (Phred-33) to the quality encoding characters.  Different quality encoding scales exist (differing by offset in the ASCII table), but note the most commonly used one is fastqsanger.  ```\n Quality encoding: !\"#$% '()*+,-./0123456789:; = ?@ABCDEFGHI\n                   |         |         |         |         |\n    Quality score: 0........10........20........30........40                                  \nUsing the quality encoding character legend, the first nucelotide in the read (C) is called with a quality score of 31 and our Ns are called with a score of 2. **As you can tell by now, this is a bad read.** \n\nEach quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based and is calculated as:\n\n    Q = -10 x log10(P), where P is the probability that a base call is erroneous\n\nThese probabaility values are the results from the base calling algorithm and dependent on how much signal was captured for the base incorporation. The score values can be interpreted as follows:\n\n|Phred Quality Score |Probability of incorrect base call |Base call accuracy|\n|:-------------------|:---------------------------------:|-----------------:|\n|10 |1 in 10 |  90%|\n|20 |1 in 100|  99%|\n|30 |1 in 1000| 99.9%|\n|40 |1 in 10,000|   99.99%|\n|50 |1 in 100,000|  99.999%|\n|60 |1 in 1,000,000|    99.9999%|\n\nTherefore, for the first nucleotide in the read (C), there is less than a 1 in 1000 chance that the base was called incorrectly. Whereas, for the the end of the read there is greater than 50% probabaility that the base is called incorrectly.\n\n## Assessing quality with FastQC\n\nNow we understand what information is stored in a FASTQ file, the next step is to examine quality metrics for our data.\n\n[FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) provides a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\n\nThe main functions of FastQC are:\n\n* Import of data from BAM, SAM or FastQ files (any variant)\n* Providing a quick overview to tell you in which areas there may be problems\n* Summary graphs and tables to quickly assess your data\n* Export of results to an HTML based permanent report\n* Offline operation to allow automated generation of reports without running the interactive application\n\n### Run FastQC  \n\nLet's run FastQC on all of our files. \n\nChange directories to the `raw_data` folder and check the contents\n\n```bash\n$ cd ~/chipseq/raw_data \n\n$ ls -l  Before we start using any software, we either have to check if it's available on the cluster, and if it is we have to load it into our environment (or  $PATH ). On the O2 cluster, we can check for, and load packages (or modules) using the  LMOD  system.   If we check which modules we currently have loaded, we should not see FastQC.  $ module list  This is because the FastQC program is not in our $PATH (i.e. its not in a directory that unix will automatically check to run commands/programs).  $ echo $PATH  To find the FastQC module to load we need to search the versions available:  $ module spider  Then we can load the FastQC module:  $ module load fastqc/0.11.3  Once a module for a tool is loaded, you have essentially made it directly available to you like any other basic UNIX command.  $ module list\n\n$ echo $PATH  FastQC will accept multiple file names as input, so we can use the  *.fq  wildcard.  $ fastqc *.fastq  Did you notice how each file was processed serially? How do we speed this up?  Exit the interactive session and once you are on a \"login node,\" start a new interactive session with 6 cores. Now we can use the multi-threading functionality of FastQC to speed this up by running 6 jobs at once, one job for one file.  $ exit  #exit the current interactive session\n\n$ srun --pty -n 6 -p short -t 0-12:00 --mem 8G --reservation=HBC /bin/bash  #start a new one with 6 cpus (-n 6) and 8G RAM (--mem 8G)\n\n$ module load fastqc/0.11.3  #reload the module for the new session\n\n$ cd ~/chipseq/raw_data\n\n$ fastqc -t 6 *.fastq  #note the extra parameter we specified for 6 threads  How did I know about the -t argument for FastQC?  $ fastqc --help  Now, move all of the  fastqc  files to the  results/fastqc  directory:  $ mv *fastqc* ../results/fastqc/", 
            "title": "Unmapped read data (FASTQ)"
        }, 
        {
            "location": "/02_QC_FASTQC/#fastqc-results", 
            "text": "Let's take a closer look at the files generated by FastQC:  $ ls -lh ../results/fastqc/", 
            "title": "FastQC Results"
        }, 
        {
            "location": "/02_QC_FASTQC/#html-reports", 
            "text": "The .html files contain the final reports generated by fastqc, let's take a closer look at them. Transfer the file for  H1hesc_Input_Rep1_chr12.fastq  over to your laptop via  FileZilla .", 
            "title": "HTML reports"
        }, 
        {
            "location": "/02_QC_FASTQC/#filezilla-step-1", 
            "text": "Open  FileZilla , and click on the File tab. Choose 'Site Manager'.", 
            "title": "Filezilla - Step 1"
        }, 
        {
            "location": "/02_QC_FASTQC/#filezilla-step-2", 
            "text": "Within the 'Site Manager' window, do the following:    Click on 'New Site', and name it something intuitive (e.g. O2)  Host: transfer.rc.hms.harvard.edu   Protocol: SFTP - SSH File Transfer Protocol  Logon Type: Normal  User: training_account  Password: password for training_account  Click 'Connect'       The  \"Per base sequence quality\"  plot is the most important analysis module in FastQC for ChIP-Seq; it provides the distribution of quality scores across all bases at each position in the reads. This information can help determine whether there were any problems at the sequencing facility during the sequencing of your data. Generally, we expect a decrease in quality towards the ends of the reads, but we shouldn't see any quality drops at the beginning or in the middle of the reads.   Based on the sequence quality plot, we see the majority of the reads have high quality, but the whiskers drop into the poor quality regions, indicating that a significant number of reads have low quality bases across the reads. The poor quality reads in the middle of the sequence would be concerning if this was our dataset, and we would probably want to contact the sequencing facility. However, this dataset was created artifically, so does not indicate a problem at the sequencing facility. Trimming could be performed from both ends of the sequences, or we can use an alignment tool that can ignore these poor quality bases at the ends of reads (soft clip).   This is the main plot explored for ChIP-seq, but if you would like to go through the remaining plots/metrics, FastQC has a really well documented  manual page  with  more details  about all the plots in the report. We recommend looking at  this post  for more information on what bad plots look like and what they mean for your data. Also, FastQC is just an indicator of what's going on with your data, don't take the \"PASS\"es and \"FAIL\"s too seriously.   We also have a  slidedeck  of error profiles for Illumina sequencing, where we discuss specific FASTQC plots and possible sources of these types of errors.    This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Filezilla - Step 2"
        }, 
        {
            "location": "/03_align_and_filtering/", 
            "text": "Contributors: Mary Piper, Radhika Khetani, Meeta Mistry\n\n\nApproximate time: 45 minutes\n\n\nLearning Objectives\n\n\n\n\nPerform alignment of reads to the genome using Bowtie2\n\n\nExamining a SAM file and understanding the information stored in it\n\n\nFiltering aligned reads to keep only uniquely mapped ones\n\n\n\n\nAlignment to Genome\n\n\nNow that we have assessed the quality of our sequence data, we are ready to align the reads to the reference genome. \nBowtie2\n is a fast and accurate alignment tool that indexes the genome with an FM Index based on the Burrows-Wheeler Transform method to keep memory requirements low for the alignment process. \nBowtie2\n supports gapped, local and paired-end alignment modes and works best for reads that are at least 50 bp (shorter read lengths should use Bowtie1). By default, Bowtie2 will perform a global end-to-end read alignment, which is best for quality-trimmed reads. However, it also has a local alignment mode, which will perform soft-clipping for the removal of poor quality bases or adapters from untrimmed reads. We will use this option since we did not trim our reads.\n\n\n\n\nNOTE:\n Our reads are only 36 bp, so technically we should explore alignment with \nbwa\n or Bowtie1 to see if it is better. However, since it is rare that you will have sequencing reads with less than 50 bp, we will show you how to perform alignment using Bowtie2.\n\n\n\n\nCreating a Bowtie2 index\n\n\nTo perform the Bowtie2 alignment, a genome index is required. The index is analagous to the index in the back of a book. By indexing the genome, we have organized it in a manner that now allows for efficient search and retrieval of matches of the query (sequence read) to the genome. \nWe previously generated the genome indices for you\n, and they exist in the \nreference_data\n directory.\n\n\nHowever, if you needed to create a genome index yourself, you would use the following command:\n\n\n# DO NOT RUN\n\nbowtie2-build \npath_to_reference_genome.fa\n \nprefix_to_name_indexes\n\n\n\n\n\n\n\nA quick note on shared databases for human and other commonly used model organisms. The O2 cluster has a designated directory at \n/n/groups/shared_databases/\n in which there are files that can be accessed by any user. These files contain, but are not limited to, genome indices for various tools, reference sequences, tool specific data, and data from public databases, such as NCBI and PDB. So when using a tool that requires a reference of sorts, it is worth taking a quick look here because chances are it's already been taken care of for you. \n\n\nbash\n$ ls -l /n/groups/shared_databases/igenome/\n\n\n\n\nAligning reads to the genome with Bowtie2\n\n\nSince we have our indices already created, we can get started with read alignment. Change directories to the \nbowtie2\n folder:\n\n\n$ cd ~/chipseq/results/bowtie2\n\n\n\n\nNow let's load the module. We can find out more on the module on O2:\n\n\n$ module spider bowtie2\n\n\n\n\nYou will notice that before we load this module we also need to load the gcc compiler (as will be the case for many of the NGS analysis tools on O2. Always check \nmodule spider\n first.)\n\n\n$ module load gcc/6.2.0 bowtie2/2.2.9\n\n\n\n\nWe will perform alignment on our single raw FASTQ file, \nH1hesc_Input_Rep1_chr12.fastq\n. Details on Bowtie2 and its functionality can be found in the \nuser manual\n; we encourage you to peruse through to get familiar with all available options.\n\n\nThe basic options for aligning reads to the genome using Bowtie2 are:\n\n\n\n\n-p\n: number of processors / cores\n\n\n-q\n: reads are in FASTQ format\n\n\n--local\n: local alignment feature to perform soft-clipping\n\n\n-x\n: /path/to/genome_indices_directory\n\n\n-U\n: /path/to/FASTQ_file\n\n\n-S\n: /path/to/output/SAM_file\n\n\n\n\n$ bowtie2 -p 2 -q --local \\\n-x ~/chipseq/reference_data/chr12 \\\n-U ~/chipseq/raw_data/H1hesc_Input_Rep1_chr12.fastq \\\n-S ~/chipseq/results/bowtie2/H1hesc_Input_Rep1_chr12_aln_unsorted.sam\n\n\n\n\nAlignment file format: SAM/BAM\n\n\nThe output we requested from the Bowtie2 aligner is an unsorted SAM file, also known as \nSequence Alignment Map format\n. The SAM file, is a \ntab-delimited text file\n that contains information for each individual read and its alignment to the genome. While we will go into some features of the SAM format, the paper by \nHeng Li et al\n provides a lot more detail on the specification.\n\n\nThe file begins with a \nheader\n, which is optional. The header is used to describe source of data, reference sequence, method of alignment, etc., this will change depending on the aligner being used. Each section begins with character \u2018@\u2019 followed by \na two-letter record type code\n.  These are followed by two-letter tags and values. Example of some common sections are provided below:\n\n\n@HD  The header line\nVN: format version\nSO: Sorting order of alignments\n\n@SQ  Reference sequence dictionary\nSN: reference sequence name\nLN: reference sequence length\nSP: species\n\n@PG  Program\nPN: program name\nVN: program version\n\n\n\n\nFollowing the header is the \nalignment section\n. Each line that follows corresponds to alignment information for a single read. Each alignment line has \n11 mandatory fields for essential mapping information\n and a variable number of other fields for aligner specific information. \n\n\n\n\nAn example read mapping is displayed above. \nNote that the example above spans two lines, but in the file it is a single line.\n Let's go through the fields one at a time. First, you have the read name (\nQNAME\n), followed by a \nFLAG\n \n\n\nThe \nFLAG\n value that is displayed can be translated into information about the mapping. \n\n\n\n\n\n\n\n\nFlag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nread is mapped\n\n\n\n\n\n\n2\n\n\nread is mapped as part of a pair\n\n\n\n\n\n\n4\n\n\nread is unmapped\n\n\n\n\n\n\n8\n\n\nmate is unmapped\n\n\n\n\n\n\n16\n\n\nread reverse strand\n\n\n\n\n\n\n32\n\n\nmate reverse strand\n\n\n\n\n\n\n64\n\n\nfirst in pair\n\n\n\n\n\n\n128\n\n\nsecond in pair\n\n\n\n\n\n\n256\n\n\nnot primary alignment\n\n\n\n\n\n\n512\n\n\nread fails platform/vendor quality checks\n\n\n\n\n\n\n1024\n\n\nread is PCR or optical duplicate\n\n\n\n\n\n\n\n\n\n\nFor a given alignment, each of these flags are either \non or off\n indicating the condition is \ntrue or false\n. \n\n\nThe \nFLAG\n is a combination of all of the individual flags (from the table above) that are true for the alignment \n\n\nThe beauty of the flag values is that \nany combination of flags can only result in one sum\n.\n\n\n\n\nThere are tools that help you translate the bitwise flag, for example \nthis one from Picard\n\n\nMoving along the fields of the SAM file, we then have \nRNAME\n which is the reference sequence name. The example read is from chromosome 1 which explains why we see 'chr1'. \nPOS\n refers to the 1-based leftmost position of the alignment. \nMAPQ\n is giving us the alignment quality, the scale of which will depend on the aligner being used. \n\n\nCIGAR\n is a sequence of letters and numbers that represent the \nedits or operations\n required to match the read to the reference. The letters are operations that are used to indicate which bases align to the reference (i.e. match, mismatch, deletion, insertion), and the numbers indicate the associated base lengths for each 'operation'.\n\n\n\n\n\n\n\n\nOperation\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nM\n\n\nsequence match or mismatch\n\n\n\n\n\n\nI\n\n\ninsertion to the reference\n\n\n\n\n\n\nD\n\n\ndeletion from reference\n\n\n\n\n\n\nN\n\n\nskipped region from the reference\n\n\n\n\n\n\n\n\nNow to the remaning fields in our SAM file:\n\n\n\n\nThe next three fields are more pertinent to paired-end data. \nMRNM\n is the mate reference name. \nMPOS\n is the mate position (1-based, leftmost). \nISIZE\n is the inferred insert size.\n\n\nFinally, you have the data from the original FASTQ file stored for each read. That is the raw sequence (\nSEQ\n) and the associated quality values for each position in the read (\nQUAL\n).\n\n\nLet;'s take a quick peek at our SAM file that we just generated. Since it is just a text file, we can browse through it using \nless\n:\n\n\n$ less H1hesc_Input_Rep1_chr12_aln_unsorted.sam\n\n\n\n\nDoes the information you see line up with the fields we described above?\n\n\nFiltering reads\n\n\nAn important issue with ChIP-seq data concerns the inclusion of multiple mapped reads (reads mapped to multiple loci on the reference genome). \nAllowing for multiple mapped reads increases the number of usable reads and the sensitivity of peak detection; however, the number of false positives may also increase\n \n[1]\n. Therefore we need to filter our alignment files to \ncontain only uniquely mapping reads\n in order to increase confidence in site discovery and improve reproducibility. Since there is no parameter in Bowtie2 to keep only uniquely mapping reads, we will need to perform the following steps to generate alignment files containing only the uniquely mapping reads:\n\n\n\n\nChange alignment file format from SAM to BAM\n\n\nSort BAM file by read coordinate locations\n\n\nFilter to keep only uniquely mapping reads (this will also remove any unmapped reads)\n\n\n\n\n1. Changing file format from SAM to BAM\n\n\nWhile the SAM alignment file output by Bowtie2 is human readable, we need a BAM alignment file for downstream tools. Therefore, we will use \nSamtools\n to convert the file formats.\n\n\nTo use \nsamtools\n we will need to load the module:\n\n\nmodule load samtools/1.3.1\n\n\n\n\nThe command we will use is \nsamtools view\n with the following parameters:\n\n\n\n\n-h\n: include header in output\n\n\n-S\n: input is in SAM format\n\n\n-b\n: output BAM format\n\n\n-o\n: /path/to/output/file\n\n\n\n\n$ samtools view -h -S -b \\\n-o H1hesc_Input_Rep1_chr12_aln_unsorted.bam \\\nH1hesc_Input_Rep1_chr12_aln_unsorted.sam\n\n\n\n\nYou can find additional parameters for the samtools functions in the \nmanual\n.\n\n\n2. Sorting BAM files by genomic coordinates\n\n\nBefore we can filter to keep the uniquely mapping reads, we need to sort our BAM alignment files by genomic coordinates (instead of by name). To perform this sort, we will use \nSambamba\n, which is a tool that quickly processes BAM and SAM files.\n\n\nThe command we will use is \nsambamba sort\n with the following parameters:\n\n\n\n\n-t\n: number of threads / cores\n\n\n-o\n: /path/to/output/file\n\n\n\n\n$ sambamba sort -t 2 \\\n-o H1hesc_Input_Rep1_chr12_aln_sorted.bam \\\nH1hesc_Input_Rep1_chr12_aln_unsorted.bam \n\n\n\n\n\n\nNOTE: This tool is not available as a module on O2.\n You will only be able to use this as part of the tools available in the \nbcbio\n pipeline. In a previous lesson, you had added this to your $PATH by modifying your \n.bashrc\n file. \nIf the command above does not work for you, run this line below:\n\n\nexport PATH=/n/app/bcbio/tools/bin:$PATH\n\n\n\n\nWe could have also used \nsamtools\n to perform the above sort, however using \nsambamba\n gives us dual functionality. List the contents of the directory -- what do you see? The advantage to using \nsambamba\n is that along with the newly sorted file, an index file is generated. If we used \nsamtools\n this would have been a two-step process.\n\n\n3. Filtering uniquely mapping reads\n\n\nFinally, we can filter the uniquely mapped reads. We will use the \nsambamba view\n command with the following parameters:\n\n\n\n\n-t\n: number of threads / cores\n\n\n-h\n: print SAM header before reads\n\n\n-f\n: format of output file (default is SAM)\n\n\n-F\n: set \ncustom filter\n - we will be using the filter to remove multimappers and unmapped reads.\n\n\n\n\n$ sambamba view -h -t 2 -f bam \\\n-F \n[XS] == null and not unmapped \n \\\nH1hesc_Input_Rep1_chr12_aln_sorted.bam \n H1hesc_Input_Rep1_chr12_aln.bam\n\n\n\n\nWe filtered out unmapped reads by specifying in the filter \nnot unmapped\n. Also, among the reads that were aligned, we filtered out multimappers by specifying \n[XS] == null\n. 'XS' is a tag generated by Bowtie2 that gives an alignment score for the second-best alignment, and it is only present if the read is aligned and more than one alignment was found for the read.\n\n\nNow that the alignment files contain only uniquely mapping reads, we are ready to perform peak calling.\n\n\n\n\nNOTE:\n After performing read alignment, it's useful to generate QC metrics for the alignment using tools such as \nMultiQC\n prior to moving on to the next steps of the analysis.\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Alignment and filtering of reads"
        }, 
        {
            "location": "/03_align_and_filtering/#learning-objectives", 
            "text": "Perform alignment of reads to the genome using Bowtie2  Examining a SAM file and understanding the information stored in it  Filtering aligned reads to keep only uniquely mapped ones", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/03_align_and_filtering/#alignment-to-genome", 
            "text": "Now that we have assessed the quality of our sequence data, we are ready to align the reads to the reference genome.  Bowtie2  is a fast and accurate alignment tool that indexes the genome with an FM Index based on the Burrows-Wheeler Transform method to keep memory requirements low for the alignment process.  Bowtie2  supports gapped, local and paired-end alignment modes and works best for reads that are at least 50 bp (shorter read lengths should use Bowtie1). By default, Bowtie2 will perform a global end-to-end read alignment, which is best for quality-trimmed reads. However, it also has a local alignment mode, which will perform soft-clipping for the removal of poor quality bases or adapters from untrimmed reads. We will use this option since we did not trim our reads.   NOTE:  Our reads are only 36 bp, so technically we should explore alignment with  bwa  or Bowtie1 to see if it is better. However, since it is rare that you will have sequencing reads with less than 50 bp, we will show you how to perform alignment using Bowtie2.", 
            "title": "Alignment to Genome"
        }, 
        {
            "location": "/03_align_and_filtering/#creating-a-bowtie2-index", 
            "text": "To perform the Bowtie2 alignment, a genome index is required. The index is analagous to the index in the back of a book. By indexing the genome, we have organized it in a manner that now allows for efficient search and retrieval of matches of the query (sequence read) to the genome.  We previously generated the genome indices for you , and they exist in the  reference_data  directory.  However, if you needed to create a genome index yourself, you would use the following command:  # DO NOT RUN\n\nbowtie2-build  path_to_reference_genome.fa   prefix_to_name_indexes    A quick note on shared databases for human and other commonly used model organisms. The O2 cluster has a designated directory at  /n/groups/shared_databases/  in which there are files that can be accessed by any user. These files contain, but are not limited to, genome indices for various tools, reference sequences, tool specific data, and data from public databases, such as NCBI and PDB. So when using a tool that requires a reference of sorts, it is worth taking a quick look here because chances are it's already been taken care of for you.   bash\n$ ls -l /n/groups/shared_databases/igenome/", 
            "title": "Creating a Bowtie2 index"
        }, 
        {
            "location": "/03_align_and_filtering/#aligning-reads-to-the-genome-with-bowtie2", 
            "text": "Since we have our indices already created, we can get started with read alignment. Change directories to the  bowtie2  folder:  $ cd ~/chipseq/results/bowtie2  Now let's load the module. We can find out more on the module on O2:  $ module spider bowtie2  You will notice that before we load this module we also need to load the gcc compiler (as will be the case for many of the NGS analysis tools on O2. Always check  module spider  first.)  $ module load gcc/6.2.0 bowtie2/2.2.9  We will perform alignment on our single raw FASTQ file,  H1hesc_Input_Rep1_chr12.fastq . Details on Bowtie2 and its functionality can be found in the  user manual ; we encourage you to peruse through to get familiar with all available options.  The basic options for aligning reads to the genome using Bowtie2 are:   -p : number of processors / cores  -q : reads are in FASTQ format  --local : local alignment feature to perform soft-clipping  -x : /path/to/genome_indices_directory  -U : /path/to/FASTQ_file  -S : /path/to/output/SAM_file   $ bowtie2 -p 2 -q --local \\\n-x ~/chipseq/reference_data/chr12 \\\n-U ~/chipseq/raw_data/H1hesc_Input_Rep1_chr12.fastq \\\n-S ~/chipseq/results/bowtie2/H1hesc_Input_Rep1_chr12_aln_unsorted.sam", 
            "title": "Aligning reads to the genome with Bowtie2"
        }, 
        {
            "location": "/03_align_and_filtering/#alignment-file-format-sambam", 
            "text": "The output we requested from the Bowtie2 aligner is an unsorted SAM file, also known as  Sequence Alignment Map format . The SAM file, is a  tab-delimited text file  that contains information for each individual read and its alignment to the genome. While we will go into some features of the SAM format, the paper by  Heng Li et al  provides a lot more detail on the specification.  The file begins with a  header , which is optional. The header is used to describe source of data, reference sequence, method of alignment, etc., this will change depending on the aligner being used. Each section begins with character \u2018@\u2019 followed by  a two-letter record type code .  These are followed by two-letter tags and values. Example of some common sections are provided below:  @HD  The header line\nVN: format version\nSO: Sorting order of alignments\n\n@SQ  Reference sequence dictionary\nSN: reference sequence name\nLN: reference sequence length\nSP: species\n\n@PG  Program\nPN: program name\nVN: program version  Following the header is the  alignment section . Each line that follows corresponds to alignment information for a single read. Each alignment line has  11 mandatory fields for essential mapping information  and a variable number of other fields for aligner specific information.    An example read mapping is displayed above.  Note that the example above spans two lines, but in the file it is a single line.  Let's go through the fields one at a time. First, you have the read name ( QNAME ), followed by a  FLAG    The  FLAG  value that is displayed can be translated into information about the mapping.      Flag  Description      1  read is mapped    2  read is mapped as part of a pair    4  read is unmapped    8  mate is unmapped    16  read reverse strand    32  mate reverse strand    64  first in pair    128  second in pair    256  not primary alignment    512  read fails platform/vendor quality checks    1024  read is PCR or optical duplicate      For a given alignment, each of these flags are either  on or off  indicating the condition is  true or false .   The  FLAG  is a combination of all of the individual flags (from the table above) that are true for the alignment   The beauty of the flag values is that  any combination of flags can only result in one sum .   There are tools that help you translate the bitwise flag, for example  this one from Picard  Moving along the fields of the SAM file, we then have  RNAME  which is the reference sequence name. The example read is from chromosome 1 which explains why we see 'chr1'.  POS  refers to the 1-based leftmost position of the alignment.  MAPQ  is giving us the alignment quality, the scale of which will depend on the aligner being used.   CIGAR  is a sequence of letters and numbers that represent the  edits or operations  required to match the read to the reference. The letters are operations that are used to indicate which bases align to the reference (i.e. match, mismatch, deletion, insertion), and the numbers indicate the associated base lengths for each 'operation'.     Operation  Description      M  sequence match or mismatch    I  insertion to the reference    D  deletion from reference    N  skipped region from the reference     Now to the remaning fields in our SAM file:   The next three fields are more pertinent to paired-end data.  MRNM  is the mate reference name.  MPOS  is the mate position (1-based, leftmost).  ISIZE  is the inferred insert size.  Finally, you have the data from the original FASTQ file stored for each read. That is the raw sequence ( SEQ ) and the associated quality values for each position in the read ( QUAL ).  Let;'s take a quick peek at our SAM file that we just generated. Since it is just a text file, we can browse through it using  less :  $ less H1hesc_Input_Rep1_chr12_aln_unsorted.sam  Does the information you see line up with the fields we described above?", 
            "title": "Alignment file format: SAM/BAM"
        }, 
        {
            "location": "/03_align_and_filtering/#filtering-reads", 
            "text": "An important issue with ChIP-seq data concerns the inclusion of multiple mapped reads (reads mapped to multiple loci on the reference genome).  Allowing for multiple mapped reads increases the number of usable reads and the sensitivity of peak detection; however, the number of false positives may also increase   [1] . Therefore we need to filter our alignment files to  contain only uniquely mapping reads  in order to increase confidence in site discovery and improve reproducibility. Since there is no parameter in Bowtie2 to keep only uniquely mapping reads, we will need to perform the following steps to generate alignment files containing only the uniquely mapping reads:   Change alignment file format from SAM to BAM  Sort BAM file by read coordinate locations  Filter to keep only uniquely mapping reads (this will also remove any unmapped reads)", 
            "title": "Filtering reads"
        }, 
        {
            "location": "/03_align_and_filtering/#1-changing-file-format-from-sam-to-bam", 
            "text": "While the SAM alignment file output by Bowtie2 is human readable, we need a BAM alignment file for downstream tools. Therefore, we will use  Samtools  to convert the file formats.  To use  samtools  we will need to load the module:  module load samtools/1.3.1  The command we will use is  samtools view  with the following parameters:   -h : include header in output  -S : input is in SAM format  -b : output BAM format  -o : /path/to/output/file   $ samtools view -h -S -b \\\n-o H1hesc_Input_Rep1_chr12_aln_unsorted.bam \\\nH1hesc_Input_Rep1_chr12_aln_unsorted.sam  You can find additional parameters for the samtools functions in the  manual .", 
            "title": "1. Changing file format from SAM to BAM"
        }, 
        {
            "location": "/03_align_and_filtering/#2-sorting-bam-files-by-genomic-coordinates", 
            "text": "Before we can filter to keep the uniquely mapping reads, we need to sort our BAM alignment files by genomic coordinates (instead of by name). To perform this sort, we will use  Sambamba , which is a tool that quickly processes BAM and SAM files.  The command we will use is  sambamba sort  with the following parameters:   -t : number of threads / cores  -o : /path/to/output/file   $ sambamba sort -t 2 \\\n-o H1hesc_Input_Rep1_chr12_aln_sorted.bam \\\nH1hesc_Input_Rep1_chr12_aln_unsorted.bam    NOTE: This tool is not available as a module on O2.  You will only be able to use this as part of the tools available in the  bcbio  pipeline. In a previous lesson, you had added this to your $PATH by modifying your  .bashrc  file.  If the command above does not work for you, run this line below:  export PATH=/n/app/bcbio/tools/bin:$PATH   We could have also used  samtools  to perform the above sort, however using  sambamba  gives us dual functionality. List the contents of the directory -- what do you see? The advantage to using  sambamba  is that along with the newly sorted file, an index file is generated. If we used  samtools  this would have been a two-step process.", 
            "title": "2. Sorting BAM files by genomic coordinates"
        }, 
        {
            "location": "/03_align_and_filtering/#3-filtering-uniquely-mapping-reads", 
            "text": "Finally, we can filter the uniquely mapped reads. We will use the  sambamba view  command with the following parameters:   -t : number of threads / cores  -h : print SAM header before reads  -f : format of output file (default is SAM)  -F : set  custom filter  - we will be using the filter to remove multimappers and unmapped reads.   $ sambamba view -h -t 2 -f bam \\\n-F  [XS] == null and not unmapped   \\\nH1hesc_Input_Rep1_chr12_aln_sorted.bam   H1hesc_Input_Rep1_chr12_aln.bam  We filtered out unmapped reads by specifying in the filter  not unmapped . Also, among the reads that were aligned, we filtered out multimappers by specifying  [XS] == null . 'XS' is a tag generated by Bowtie2 that gives an alignment score for the second-best alignment, and it is only present if the read is aligned and more than one alignment was found for the read.  Now that the alignment files contain only uniquely mapping reads, we are ready to perform peak calling.   NOTE:  After performing read alignment, it's useful to generate QC metrics for the alignment using tools such as  MultiQC  prior to moving on to the next steps of the analysis.    This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "3. Filtering uniquely mapping reads"
        }, 
        {
            "location": "/04_automation/", 
            "text": "Contributors: Meeta Mistry, Radhika Khetani\n\n\nApproximate time: 80 minutes\n\n\nLearning Objectives\n\n\n\n\nWrite a shell script to generate filtered BAM files for all samples\n\n\nDescribe the difference between serial and parallel jobs\n\n\n\n\nAutomating the ChIP-seq analysis path from sequence reads to BAM files\n\n\nOnce you have optimized tools and parameters using a single sample (using an interactive session), you can write a script to run the whole or a portion of the workflow on multiple samples in parallel (batch submission with a shell script).\n\n\nWriting a reusable shell script ensures that every sample is run with the exact same parameters, and helps to keep track of all the tools and their versions. The shell script is like a lab notebook; in the future, you (or your colleagues) can go back and check the workflow for methods and versions, which goes a long way to making your work more efficient and reproducible.\n\n\nBefore we start with the script, let's check how many cores our interactive session has by using \nsacct\n. \n\n\n$ sacct\n\n\n\n\nWe need to have an interactive session with 6 cores, if you already have one you are set. If you have a session with fewer cores then \nexit\n out of your current interactive session and start a new one with \n-n 6\n.\n\n\n$ srun --pty -p short -t 0-12:00 -n 6 --mem 8G --reservation=HBC /bin/bash\n\n\n\n\nMore Flexibility with variables\n\n\nWe can write a shell script that will run on a specific file, but to make it more flexible and efficient we would prefer that it lets us give it an input fastq file when we run the script. To be able to provide an input to any shell script, we need to use \nPositional Parameters\n.\n\n\nFor example, we can refer to the components of the following command as numbered variables \nwithin\n the actual script:\n\n\n# * DO NOT RUN *\nsh  run_analysis.sh  input.fastq  input.gtf  12\n\n\n\n\n$0\n =\n run_analysis.sh\n\n\n$1\n =\n input.fastq\n\n\n$2\n =\n input.gtf\n\n\n$3\n =\n 12\n\n\nThe variables $1, $2, $3,...$9 and so on are \npositional parameters\n in the context of the shell script, and can be used within the script to refer to the files/number specified on the command line. Basically, the script is written with the expectation that $1 will be a fastq file and $2 will be a GTF file, and so on.\n\n\nThere can be virtually unlimited numbers of inputs to a shell script, but it is wise to only have a few inputs to avoid errors and confusion when running a script that used positional parameters.\n\n\n\n\nThis is an example of a simple script that used the concept of positional parameters and the associated variables\n. You should try this script out after the class to get a better handle on positional parameters for shell scripting.\n\n\n\n\nLet's use this new concept in the script we are writing. We want the first positional parameter ($1) to be the name of our fastq file. We could just use the variable \n$1\n throughout the script to refer to the fastq file, but this variable name is not intuitive, so we want to create a new variable called \nfq\n and copy the contents of \n$1\n into it.\n\n\nFirst, we need to start a new script called \nchipseq_analysis_on_input_file.sh\n in the \n~/chipseq/scripts/\n directory:\n\n\n$ cd ~/chipseq/scripts/\n\n$ vim chipseq_analysis_on_input_file.sh\n\n\n\n\n#!/bin/bash/\n\n# initialize a variable with an intuitive name to store the name of the input fastq file\nfq=$1\n\n\n\n\n\n\nWhen we set up variables we do not use the \n$\n before it, but when we \nuse the variable\n, we always have to have the \n$\n before it. \n\n\nFor example: \n\n\ninitializing the \nfq\n variable =\n \nfq=$1\n\n\nusing the \nfq\n variable =\n \nfastqc $fq\n\n\n\n\nTo ensure that all the output files from the workflow are properly named with sample IDs we should extract the \"base name\" (or sample ID) from the name of the input file.\n\n\n# grab base of filename for naming outputs\nbase=`basename $fq _chr12.fastq`\necho \nSample name is $base\n           \n\n\n\n\n\n\nRemember \nbasename\n?\n\n\n\n\nthe \nbasename\n command: this command takes a path or a name and trims away all the information before the last \n\\\n and if you specify the string to clear away at the end, it will do that as well. In this case, if the variable \n$fq\n contains the path \n\"~/chipseq/raw_data/H1hesc_Nanog_Rep1_chr12.fastq\"\n, \nbasename $fq _chr12.fastq\n will output \"H1hesc_Nanog_Rep1\".\n\n\nto assign the value of the \nbasename\n command to the \nbase\n variable, we encapsulate the \nbasename...\n command in backticks. This syntax is necessary for assigning the output of a command to a variable.\n\n\n\n\n\n\nNext we want to specify how many cores the script should use to run the analysis. This provides us with an easy way to modify the script to run with more or fewer cores without have to replace the number within all commands where cores are specified.\n\n\n# directory with bowtie genome index\ngenome=~/chipseq/reference_data/chr12\n\n\n\n\nWe'll create output directories, but with the \n-p\n option. This will make sure that \nmkdir\n will create the directory only if it does not exist, and it won't throw an error if it does exist.\n\n\n# make all of the output directories\n# The -p option means mkdir will create the whole path if it \n# does not exist and refrain from complaining if it does exist\nmkdir -p ~/chipseq/results/fastqc\nmkdir -p ~/chipseq/results/bowtie2/intermediate_bams\n\n\n\n\nNow that we have already created our output directories, we can now specify variables with the path to those directories both for convenience but also to make it easier to see what is going on in a long command.\n\n\n# set up output filenames and locations\nfastqc_out=~/chipseq/results/fastqc/\n\n## set up file names\nalign_out=~/chipseq/results/bowtie2/${base}_unsorted.sam\nalign_bam=~/chipseq/results/bowtie2/${base}_unsorted.bam\nalign_sorted=~/chipseq/results/bowtie2/${base}_sorted.bam\nalign_filtered=~/chipseq/results/bowtie2/${base}_aln.bam\n\n## set up more variables for 2 additional directoties to help clean up the results folder\nbowtie_results=~/chipseq/results/bowtie2\nintermediate_bams=~/chipseq/results/bowtie2/intermediate_bams\n\n\n\n\nKeeping track of tool versions\n\n\nAll of our variables are now staged. Next, let's make sure all the modules are loaded. This is also a good way to keep track of the versions of tools that you are using in the script:\n\n\n# set up the software environment\nmodule load fastqc/0.11.3\nmodule load gcc/6.2.0  \nmodule load bowtie2/2.2.9\nmodule load samtools/1.3.1\nexport PATH=/n/app/bcbio/tools/bin:$PATH    # for using 'sambamba'\n\n\n\n\nPreparing for future debugging\n\n\nIn the script, it is a good idea to use \necho\n for debugging. \necho\n basically displays the string of characters specified within the quotations. When you have strategically place \necho\n commands specifying what stage of the analysis is next, in case of failure you can determine the last \necho\n statement displayed to troubleshoot the script.\n\n\necho \nProcessing file $fq\n\n\n\n\n\n\n\nYou can also use \nset -x\n:\n\n\nset -x\n is a debugging tool that will make bash display the command before executing it. In case of an issue with the commands in the shell script, this type of debugging lets you quickly pinpoint the step that is throwing an error. Often, tools will display the error that caused the program to stop running, so keep this in mind for times when you are running into issues where this is not available.\nYou can turn this functionality off by saying \nset +x\n\n\n\n\nRunning the tools\n\n\nLet's write up the commands to run the tools we have already tested, with a couple of modifications:\n\n use variable names instead of actual file names\n\n multithread when possible (\nbowtie2\n, \nsambamba\n)\n\n\n# Run FastQC\nfastqc $fq\n\n# Run bowtie2\nbowtie2 -p 6 -q --local -x $genome -U $fq -S $align_out\n\n# Create BAM from SAM\nsamtools view -h -S -b -@ 6 -o $align_bam $align_out\n\n# Sort BAM file by genomic coordinates\nsambamba sort -t 6 -o $align_sorted $align_bam\n\n# Filter out duplicates\nsambamba view -h -t 6 -f bam -F \n[XS] == null and not unmapped \n $align_sorted \n $align_filtered\n\n# Move intermediate files out of the bowtie2 directory\nmv $bowtie_results/${base}*sorted* $intermediate_bams\n\n\n\n\nLast addition to the script\n\n\nIt is best practice to have the script \nusage\n specified at the top any script. This should have information such that when your future self, or a co-worker, uses the script they know what it will do and what input(s) are needed. For our script, we should have the following lines of comments right at the top after \n#!/bin/bash/\n:\n\n\n# This script takes a fastq file of ChIP-Seq data, runs FastQC and outputs a BAM file for it that is ready for peak calling. Bowtie2 is the aligner used, and the outputted BAM file is sorted by genomic coordinates and has duplicate reads removed using sambamba.\n# USAGE: sh chipseq_analysis_on_input_file.sh \npath to the fastq file\n\n\n\n\n\nIt is okay to specify this after everything else is set up, since you will have most clarity about the script only once it is fully done.\n\n\nYour script should now look like this:\n\n\n#!/bin/bash/\n\n# This script takes a fastq file of ChIP-Seq data, runs FastQC and outputs a BAM file for it that is ready for peak calling. Bowtie2 is the aligner used, and the outputted BAM file is sorted by genomic coordinates and has duplicate reads removed using sambamba.\n# USAGE: sh chipseq_analysis_on_input_file.sh \npath to the fastq file\n\n\n# initialize a variable with an intuitive name to store the name of the input fastq file\nfq=$1\n\n# grab base of filename for naming outputs\nbase=`basename $fq .fastq`\necho \nSample name is $base\n    \n\n# directory with bowtie genome index\ngenome=~/chipseq/reference_data/chr12\n\n# make all of the output directories\n# The -p option means mkdir will create the whole path if it \n# does not exist and refrain from complaining if it does exist\nmkdir -p ~/chipseq/results/fastqc\nmkdir -p ~/chipseq/results/bowtie2/intermediate_bams\n\n# set up output filenames and locations\nfastqc_out=~/chipseq/results/fastqc/\n\n## set up file names\nalign_out=~/chipseq/results/bowtie2/${base}_unsorted.sam\nalign_bam=~/chipseq/results/bowtie2/${base}_unsorted.bam\nalign_sorted=~/chipseq/results/bowtie2/${base}_sorted.bam\nalign_filtered=~/chipseq/results/bowtie2/${base}_aln.bam\n\n## set up more variables for 2 additional directoties to help clean up the results folder\nbowtie_results=~/chipseq/results/bowtie2\nintermediate_bams=~/chipseq/results/bowtie2/intermediate_bams\n\n# set up the software environment\nmodule load fastqc/0.11.3\nmodule load gcc/6.2.0  \nmodule load bowtie2/2.2.9\nmodule load samtools/1.3.1\nexport PATH=/n/app/bcbio/tools/bin:$PATH    # for using 'sambamba'\n\necho \nProcessing file $fq\n\n\n# Run FastQC and move output to the appropriate folder\nfastqc $fq\nmv *_fastqc.* ~/chipseq/results/fastqc/\n\n# Run bowtie2\nbowtie2 -p 6 -q --local -x $genome -U $fq -S $align_out\n\n# Create BAM from SAM\nsamtools view -h -S -b -@ 6 -o $align_bam $align_out\n\n# Sort BAM file by genomic coordinates\nsambamba sort -t 6 -o $align_sorted $align_bam\n\n# Filter out duplicates\nsambamba view -h -t 6 -f bam -F \n[XS] == null and not unmapped \n $align_sorted \n $align_filtered\n\n# Move intermediate files out of the bowtie2 directory\nmv $bowtie_results/${base}*sorted* $intermediate_bams\n\n\n\n\nSaving and running script\n\n\nWe should all have an interactive session with 6 cores, so we can run the script as follows:\n\n\n$ sh chipseq_analysis_on_input_file.sh ~/chipseq/raw_data/H1hesc_Nanog_Rep1_chr12.fastq\n\n\n\n\nSubmitting jobs \nin serial\n to the SLURM scheduler\n\n\nThe above script will run in an interactive session \none file at a time\n. But the whole point of writing this script was to run it on all files at once. How do you think we can do this?\n\n\nTo run the above script \n\"in serial\"\n for all of the files on a worker node via the job scheduler, we can create a separate submission script that will need 2 components:\n\n\n\n\nSLURM directives\n at the \nbeginning\n of the script. This is so that the scheduler knows what resources we need in order to run our job on the compute node(s).\n\n\na \nfor\n loop that iterates through and runs the above script for all the fastq files.\n\n\n\n\nBelow is what this second script (\nchipseq_analysis_on_allfiles.slurm\n) would look like \n[DO NOT RUN THIS]\n:\n\n\n# **\\[DO NOT RUN THIS\\]**\n\n#!/bin/bash\n\n#SBATCH -p short        # partition name\n#SBATCH -t 0-2:00       # hours:minutes runlimit after which job will be killed\n#SBATCH -n 6        # number of cores requested -- this needs to be greater than or equal to the number of cores you plan to use to run your job\n#SBATCH --job-name Nanog_Rep1       # Job name\n#SBATCH -o %j.out           # File to which standard out will be written\n#SBATCH -e %j.err       # File to which standard error will be written\n\n# this `for` loop, will take chip-seq fastq files as input and output filtered BAM files ready for peak calling.\n\nfor fq in ~/chipseq/raw_data/*.fastq\ndo\n  echo \nrunning analysis on $fq\n\n  sh chipseq_analysis_on_input_file.sh $fq\ndone\n\n\n\n\nBut we don't want to run the analysis on these 6 samples one after the other!\n We want to run them \"in parallel\" as 6 separate jobs. \n\n\n\n\nNote:\n If you create and run the above script, or something similar to it, i.e. with SLURM directives at the top, you should give the script name \n.run\n or \n.slurm\n as the extension. This will make it obvious that it is meant to submit jobs to the SLURM scheduler. \n\n\n\n\nTo the run the above script you would have used the following command: \nsbatch chipseq_analysis_on_allfiles.slurm\n.  \n\n\nSubmitting jobs \nin parallel\n to the SLURM scheduler\n\n\nParallelization will save you a lot of time with real (large) datasets. To parallelize our analysis, we will still need to write a second script that will call the original script we just wrote. We will still use a \nfor\n loop, but we will be creating a regular shell script and we will be specifying the SLURM directives a little differently. \n\n\nUse \nvim\n to start a new shell script called \nchipseq_analysis_on_allfiles-for_slurm.sh\n: \n\n\n$ vim chipseq_analysis_on_allfiles_for-slurm.sh\n\n\n\n\nThis script loops through the same files as in the previous (demo) script, but the command being submitted within the \nfor\n loop is \nsbatch\n with SLURM directives specified on the same line:\n\n\n#! /bin/bash\n\nfor fq in ~/chipseq/raw_data/*.fastq\ndo\n\nsbatch -p short -t 0-2:00 -n 6 --job-name chipseq-analysis --wrap=\nsh ~/chipseq/scripts/chipseq_analysis_on_input_file.sh $fq\n\nsleep 1     # wait 1 second between each job submission\n\ndone\n\n\n\n\n\n\nPlease note that after the \nsbatch\n directives the command \nsh ~/chipseq/scripts/chipseq_analysis_on_input_file.sh $fq\n is in quotes.\n\n\n\n\nWhat you should see on the output of your screen would be the jobIDs that are returned from the scheduler for each of the jobs that your script submitted.\n\n\nYou can use \nsacct login_ID\n to check progress.\n\n\nDon't forget about the \nscancel\n command, should something go wrong and you need to cancel your jobs.\n\n\n\n\nNOTE:\n All job schedulers are similar, but not the same. Once you understand how one works, you can transition to another one without too much trouble. They all have their pros and cons which are considered by the system administrators when picking one for a given HPC environment. \n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Automating generation of alignment files"
        }, 
        {
            "location": "/04_automation/#learning-objectives", 
            "text": "Write a shell script to generate filtered BAM files for all samples  Describe the difference between serial and parallel jobs", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/04_automation/#automating-the-chip-seq-analysis-path-from-sequence-reads-to-bam-files", 
            "text": "Once you have optimized tools and parameters using a single sample (using an interactive session), you can write a script to run the whole or a portion of the workflow on multiple samples in parallel (batch submission with a shell script).  Writing a reusable shell script ensures that every sample is run with the exact same parameters, and helps to keep track of all the tools and their versions. The shell script is like a lab notebook; in the future, you (or your colleagues) can go back and check the workflow for methods and versions, which goes a long way to making your work more efficient and reproducible.  Before we start with the script, let's check how many cores our interactive session has by using  sacct .   $ sacct  We need to have an interactive session with 6 cores, if you already have one you are set. If you have a session with fewer cores then  exit  out of your current interactive session and start a new one with  -n 6 .  $ srun --pty -p short -t 0-12:00 -n 6 --mem 8G --reservation=HBC /bin/bash", 
            "title": "Automating the ChIP-seq analysis path from sequence reads to BAM files"
        }, 
        {
            "location": "/04_automation/#more-flexibility-with-variables", 
            "text": "We can write a shell script that will run on a specific file, but to make it more flexible and efficient we would prefer that it lets us give it an input fastq file when we run the script. To be able to provide an input to any shell script, we need to use  Positional Parameters .  For example, we can refer to the components of the following command as numbered variables  within  the actual script:  # * DO NOT RUN *\nsh  run_analysis.sh  input.fastq  input.gtf  12  $0  =  run_analysis.sh  $1  =  input.fastq  $2  =  input.gtf  $3  =  12  The variables $1, $2, $3,...$9 and so on are  positional parameters  in the context of the shell script, and can be used within the script to refer to the files/number specified on the command line. Basically, the script is written with the expectation that $1 will be a fastq file and $2 will be a GTF file, and so on.  There can be virtually unlimited numbers of inputs to a shell script, but it is wise to only have a few inputs to avoid errors and confusion when running a script that used positional parameters.   This is an example of a simple script that used the concept of positional parameters and the associated variables . You should try this script out after the class to get a better handle on positional parameters for shell scripting.   Let's use this new concept in the script we are writing. We want the first positional parameter ($1) to be the name of our fastq file. We could just use the variable  $1  throughout the script to refer to the fastq file, but this variable name is not intuitive, so we want to create a new variable called  fq  and copy the contents of  $1  into it.  First, we need to start a new script called  chipseq_analysis_on_input_file.sh  in the  ~/chipseq/scripts/  directory:  $ cd ~/chipseq/scripts/\n\n$ vim chipseq_analysis_on_input_file.sh  #!/bin/bash/\n\n# initialize a variable with an intuitive name to store the name of the input fastq file\nfq=$1   When we set up variables we do not use the  $  before it, but when we  use the variable , we always have to have the  $  before it.   For example:   initializing the  fq  variable =   fq=$1  using the  fq  variable =   fastqc $fq   To ensure that all the output files from the workflow are properly named with sample IDs we should extract the \"base name\" (or sample ID) from the name of the input file.  # grab base of filename for naming outputs\nbase=`basename $fq _chr12.fastq`\necho  Sample name is $base               Remember  basename ?   the  basename  command: this command takes a path or a name and trims away all the information before the last  \\  and if you specify the string to clear away at the end, it will do that as well. In this case, if the variable  $fq  contains the path  \"~/chipseq/raw_data/H1hesc_Nanog_Rep1_chr12.fastq\" ,  basename $fq _chr12.fastq  will output \"H1hesc_Nanog_Rep1\".  to assign the value of the  basename  command to the  base  variable, we encapsulate the  basename...  command in backticks. This syntax is necessary for assigning the output of a command to a variable.    Next we want to specify how many cores the script should use to run the analysis. This provides us with an easy way to modify the script to run with more or fewer cores without have to replace the number within all commands where cores are specified.  # directory with bowtie genome index\ngenome=~/chipseq/reference_data/chr12  We'll create output directories, but with the  -p  option. This will make sure that  mkdir  will create the directory only if it does not exist, and it won't throw an error if it does exist.  # make all of the output directories\n# The -p option means mkdir will create the whole path if it \n# does not exist and refrain from complaining if it does exist\nmkdir -p ~/chipseq/results/fastqc\nmkdir -p ~/chipseq/results/bowtie2/intermediate_bams  Now that we have already created our output directories, we can now specify variables with the path to those directories both for convenience but also to make it easier to see what is going on in a long command.  # set up output filenames and locations\nfastqc_out=~/chipseq/results/fastqc/\n\n## set up file names\nalign_out=~/chipseq/results/bowtie2/${base}_unsorted.sam\nalign_bam=~/chipseq/results/bowtie2/${base}_unsorted.bam\nalign_sorted=~/chipseq/results/bowtie2/${base}_sorted.bam\nalign_filtered=~/chipseq/results/bowtie2/${base}_aln.bam\n\n## set up more variables for 2 additional directoties to help clean up the results folder\nbowtie_results=~/chipseq/results/bowtie2\nintermediate_bams=~/chipseq/results/bowtie2/intermediate_bams", 
            "title": "More Flexibility with variables"
        }, 
        {
            "location": "/04_automation/#keeping-track-of-tool-versions", 
            "text": "All of our variables are now staged. Next, let's make sure all the modules are loaded. This is also a good way to keep track of the versions of tools that you are using in the script:  # set up the software environment\nmodule load fastqc/0.11.3\nmodule load gcc/6.2.0  \nmodule load bowtie2/2.2.9\nmodule load samtools/1.3.1\nexport PATH=/n/app/bcbio/tools/bin:$PATH    # for using 'sambamba'", 
            "title": "Keeping track of tool versions"
        }, 
        {
            "location": "/04_automation/#preparing-for-future-debugging", 
            "text": "In the script, it is a good idea to use  echo  for debugging.  echo  basically displays the string of characters specified within the quotations. When you have strategically place  echo  commands specifying what stage of the analysis is next, in case of failure you can determine the last  echo  statement displayed to troubleshoot the script.  echo  Processing file $fq    You can also use  set -x :  set -x  is a debugging tool that will make bash display the command before executing it. In case of an issue with the commands in the shell script, this type of debugging lets you quickly pinpoint the step that is throwing an error. Often, tools will display the error that caused the program to stop running, so keep this in mind for times when you are running into issues where this is not available.\nYou can turn this functionality off by saying  set +x", 
            "title": "Preparing for future debugging"
        }, 
        {
            "location": "/04_automation/#running-the-tools", 
            "text": "Let's write up the commands to run the tools we have already tested, with a couple of modifications:  use variable names instead of actual file names  multithread when possible ( bowtie2 ,  sambamba )  # Run FastQC\nfastqc $fq\n\n# Run bowtie2\nbowtie2 -p 6 -q --local -x $genome -U $fq -S $align_out\n\n# Create BAM from SAM\nsamtools view -h -S -b -@ 6 -o $align_bam $align_out\n\n# Sort BAM file by genomic coordinates\nsambamba sort -t 6 -o $align_sorted $align_bam\n\n# Filter out duplicates\nsambamba view -h -t 6 -f bam -F  [XS] == null and not unmapped   $align_sorted   $align_filtered\n\n# Move intermediate files out of the bowtie2 directory\nmv $bowtie_results/${base}*sorted* $intermediate_bams", 
            "title": "Running the tools"
        }, 
        {
            "location": "/04_automation/#last-addition-to-the-script", 
            "text": "It is best practice to have the script  usage  specified at the top any script. This should have information such that when your future self, or a co-worker, uses the script they know what it will do and what input(s) are needed. For our script, we should have the following lines of comments right at the top after  #!/bin/bash/ :  # This script takes a fastq file of ChIP-Seq data, runs FastQC and outputs a BAM file for it that is ready for peak calling. Bowtie2 is the aligner used, and the outputted BAM file is sorted by genomic coordinates and has duplicate reads removed using sambamba.\n# USAGE: sh chipseq_analysis_on_input_file.sh  path to the fastq file   It is okay to specify this after everything else is set up, since you will have most clarity about the script only once it is fully done.  Your script should now look like this:  #!/bin/bash/\n\n# This script takes a fastq file of ChIP-Seq data, runs FastQC and outputs a BAM file for it that is ready for peak calling. Bowtie2 is the aligner used, and the outputted BAM file is sorted by genomic coordinates and has duplicate reads removed using sambamba.\n# USAGE: sh chipseq_analysis_on_input_file.sh  path to the fastq file \n\n# initialize a variable with an intuitive name to store the name of the input fastq file\nfq=$1\n\n# grab base of filename for naming outputs\nbase=`basename $fq .fastq`\necho  Sample name is $base     \n\n# directory with bowtie genome index\ngenome=~/chipseq/reference_data/chr12\n\n# make all of the output directories\n# The -p option means mkdir will create the whole path if it \n# does not exist and refrain from complaining if it does exist\nmkdir -p ~/chipseq/results/fastqc\nmkdir -p ~/chipseq/results/bowtie2/intermediate_bams\n\n# set up output filenames and locations\nfastqc_out=~/chipseq/results/fastqc/\n\n## set up file names\nalign_out=~/chipseq/results/bowtie2/${base}_unsorted.sam\nalign_bam=~/chipseq/results/bowtie2/${base}_unsorted.bam\nalign_sorted=~/chipseq/results/bowtie2/${base}_sorted.bam\nalign_filtered=~/chipseq/results/bowtie2/${base}_aln.bam\n\n## set up more variables for 2 additional directoties to help clean up the results folder\nbowtie_results=~/chipseq/results/bowtie2\nintermediate_bams=~/chipseq/results/bowtie2/intermediate_bams\n\n# set up the software environment\nmodule load fastqc/0.11.3\nmodule load gcc/6.2.0  \nmodule load bowtie2/2.2.9\nmodule load samtools/1.3.1\nexport PATH=/n/app/bcbio/tools/bin:$PATH    # for using 'sambamba'\n\necho  Processing file $fq \n\n# Run FastQC and move output to the appropriate folder\nfastqc $fq\nmv *_fastqc.* ~/chipseq/results/fastqc/\n\n# Run bowtie2\nbowtie2 -p 6 -q --local -x $genome -U $fq -S $align_out\n\n# Create BAM from SAM\nsamtools view -h -S -b -@ 6 -o $align_bam $align_out\n\n# Sort BAM file by genomic coordinates\nsambamba sort -t 6 -o $align_sorted $align_bam\n\n# Filter out duplicates\nsambamba view -h -t 6 -f bam -F  [XS] == null and not unmapped   $align_sorted   $align_filtered\n\n# Move intermediate files out of the bowtie2 directory\nmv $bowtie_results/${base}*sorted* $intermediate_bams", 
            "title": "Last addition to the script"
        }, 
        {
            "location": "/04_automation/#saving-and-running-script", 
            "text": "We should all have an interactive session with 6 cores, so we can run the script as follows:  $ sh chipseq_analysis_on_input_file.sh ~/chipseq/raw_data/H1hesc_Nanog_Rep1_chr12.fastq", 
            "title": "Saving and running script"
        }, 
        {
            "location": "/04_automation/#submitting-jobs-in-serial-to-the-slurm-scheduler", 
            "text": "The above script will run in an interactive session  one file at a time . But the whole point of writing this script was to run it on all files at once. How do you think we can do this?  To run the above script  \"in serial\"  for all of the files on a worker node via the job scheduler, we can create a separate submission script that will need 2 components:   SLURM directives  at the  beginning  of the script. This is so that the scheduler knows what resources we need in order to run our job on the compute node(s).  a  for  loop that iterates through and runs the above script for all the fastq files.   Below is what this second script ( chipseq_analysis_on_allfiles.slurm ) would look like  [DO NOT RUN THIS] :  # **\\[DO NOT RUN THIS\\]**\n\n#!/bin/bash\n\n#SBATCH -p short        # partition name\n#SBATCH -t 0-2:00       # hours:minutes runlimit after which job will be killed\n#SBATCH -n 6        # number of cores requested -- this needs to be greater than or equal to the number of cores you plan to use to run your job\n#SBATCH --job-name Nanog_Rep1       # Job name\n#SBATCH -o %j.out           # File to which standard out will be written\n#SBATCH -e %j.err       # File to which standard error will be written\n\n# this `for` loop, will take chip-seq fastq files as input and output filtered BAM files ready for peak calling.\n\nfor fq in ~/chipseq/raw_data/*.fastq\ndo\n  echo  running analysis on $fq \n  sh chipseq_analysis_on_input_file.sh $fq\ndone  But we don't want to run the analysis on these 6 samples one after the other!  We want to run them \"in parallel\" as 6 separate jobs.    Note:  If you create and run the above script, or something similar to it, i.e. with SLURM directives at the top, you should give the script name  .run  or  .slurm  as the extension. This will make it obvious that it is meant to submit jobs to the SLURM scheduler.    To the run the above script you would have used the following command:  sbatch chipseq_analysis_on_allfiles.slurm .", 
            "title": "Submitting jobs in serial to the SLURM scheduler"
        }, 
        {
            "location": "/04_automation/#submitting-jobs-in-parallel-to-the-slurm-scheduler", 
            "text": "Parallelization will save you a lot of time with real (large) datasets. To parallelize our analysis, we will still need to write a second script that will call the original script we just wrote. We will still use a  for  loop, but we will be creating a regular shell script and we will be specifying the SLURM directives a little differently.   Use  vim  to start a new shell script called  chipseq_analysis_on_allfiles-for_slurm.sh :   $ vim chipseq_analysis_on_allfiles_for-slurm.sh  This script loops through the same files as in the previous (demo) script, but the command being submitted within the  for  loop is  sbatch  with SLURM directives specified on the same line:  #! /bin/bash\n\nfor fq in ~/chipseq/raw_data/*.fastq\ndo\n\nsbatch -p short -t 0-2:00 -n 6 --job-name chipseq-analysis --wrap= sh ~/chipseq/scripts/chipseq_analysis_on_input_file.sh $fq \nsleep 1     # wait 1 second between each job submission\n\ndone   Please note that after the  sbatch  directives the command  sh ~/chipseq/scripts/chipseq_analysis_on_input_file.sh $fq  is in quotes.   What you should see on the output of your screen would be the jobIDs that are returned from the scheduler for each of the jobs that your script submitted.  You can use  sacct login_ID  to check progress.  Don't forget about the  scancel  command, should something go wrong and you need to cancel your jobs.   NOTE:  All job schedulers are similar, but not the same. Once you understand how one works, you can transition to another one without too much trouble. They all have their pros and cons which are considered by the system administrators when picking one for a given HPC environment.     This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Submitting jobs in parallel to the SLURM scheduler"
        }, 
        {
            "location": "/05_peak_calling_macs/", 
            "text": "Contributors: Meeta Mistry, Radhika Khetani\n\n\nApproximate time: 80 minutes\n\n\nLearning Objectives\n\n\n\n\nDescribe the different components of the MACS2 peak calling algorithm\n\n\nDescribe the parameters involved in running MACS2\n\n\nList and describe the output files from MACS2\n\n\n\n\nPeak Calling\n\n\nPeak calling, the next step in our workflow, is a computational method used to identify areas in the genome that have been enriched with aligned reads as a consequence of performing a ChIP-sequencing experiment. \n\n\n\n\nFor ChIP-seq experiments, what we observe from the alignment files is a \nstrand asymmetry with read densities on the +/- strand, centered around the binding site\n. The 5' ends of the selected fragments will form groups on the positive- and negative-strand. The distributions of these groups are then assessed using statistical measures and compared against background (input or mock IP samples) to determine if the site of enrichment is likely to be a real binding site.\n\n\n\n\nThere are various tools that are available for peak calling. One of the more commonly used pack callers is MACS2, and we will demonstrate it in this session. \nNote that in this Session the term 'tag' and sequence 'read' are used interchangeably.\n\n\n\n\nNOTE:\n Our dataset is investigating two transcription factors and so our focus is on identifying short degenerate sequences that present as punctate binding sites. ChIP-seq analysis algorithms are specialized in identifying one of \ntwo types of enrichment\n (or have specific methods for each): \nbroad peaks\n or broad domains (i.e. histone modifications that cover entire gene bodies) or \nnarrow peaks\n (i.e. a transcription factor binding). Narrow peaks are easier to detect as we are looking for regions that have higher amplitude and are easier to distinguish from the background, compared to broad or dispersed marks. There are also 'mixed' binding profiles which can be hard for algorithms to discern. An example of this is the binding properties of PolII, which binds at promotor and across the length of the gene resulting in mixed signals (narrow and broad).\n\n\n\n\nMACS2\n\n\nA commonly used tool for identifying transcription factor binding sites is named \nModel-based Analysis of ChIP-Seq (MACS)\n. The \nMACS algorithm\n captures the influence of genome complexity to evaluate the significance of enriched ChIP regions. Although it was developed for the detection of transcription factor binding sites it is also suited for larger regions.\n\n\nMACS improves the spatial resolution of binding sites through \ncombining the information of both sequencing tag position and orientation.\n MACS can be easily used either for the ChIP sample alone, or along with a control sample which increases specificity of the peak calls. The MACS workflow is depicted below. In this lesson, we will describe the steps in more detail.\n\n\n\n\nRemoving redundancy\n\n\nMACS provides different options for dealing with \nduplicate tags\n at the exact same location, that is tags with the \nsame coordination and the same strand\n. The default is to keep a single read at each location. The \nauto\n option, which is very commonly used, tells MACS to calculate the maximum tags at the exact same location based on binomal distribution using 1e-5 as the pvalue cutoff. An alternative is to set the \nall\n option, which keeps every tag. If an \ninteger\n is specified, then at most that many tags will be kept at the same location. This redundancy is consistently applied for both the ChIP and input samples.\n\n\n\n\nWhy worry about duplicates?\n\nReads with the same start position are considered duplicates. These duplicates can arise from experimental artifacts, but can also contribute to genuine ChIP-signal.\n\n\n\n\nThe bad kind of duplicates:\n If initial starting material is low this can lead to overamplification of this material before sequencing. Any biases in PCR will compound this problem and can lead to artificially enriched regions. Also blacklisted (repeat) regions with ultra high signal will also be high in duplicates. Masking these regions prior to analysis can help remove this problem.\n\n\nThe good kind of duplicates:\n Duplicates will also exist within highly efficient (or even inefficient ChIP) when deeply sequenced. Removal of these duplicates can lead to a saturation and so underestimation of the ChIP signal.\n\n\n\n\nThe take-home:\n Consider your enrichment efficiency and sequencing depth. But, because we cannot distinguish between the good and the bad, best practice is to remove duplicates prior to peak calling.  Retain duplicates for differential binding analysis. Also, if you are expecting binding in repetitive regions keep duplicates and multiple mappers.\n\n\n\n\nModeling the shift size\n\n\nThe tag density around a true binding site should show a \nbimodal enrichment pattern\n (or paired peaks). MACS takes advantage of this bimodal pattern to empirically model the shifting size to better locate the precise binding sites.\n\n\nTo find paired peaks to \nbuild the model\n, MACS first scans the whole dataset searching for highly significant enriched regions. \nThis is done only using the ChIP sample!\n Given a sonication size (\nbandwidth\n) and a high-confidence fold-enrichment (\nmfold\n), MACS slides two \nbandwidth\n windows across the genome to find regions with \ntags more than \nmfold\n enriched relative to a random tag genome distribution\n. \n\n\n\n\nMACS randomly \nsamples 1,000 of these high-quality peaks\n, separates their Watson and Crick tags, and aligns them by the midpoint between their Watson and Crick tag centers. The \ndistance between the modes of the Watson and Crick peaks in the alignment is defined as 'd'\n and represents the estimated fragment length. MACS shifts all the tags by d/2 toward the 3' ends to the most likely protein-DNA interaction sites.\n\n\nScaling libraries\n\n\nFor experiments in which sequence depth differs between input and treatment samples, MACS linearly scales the \ntotal control tag count to be the same as the total ChIP tag count\n. The default behaviour is for the larger sample to be scaled down.\n\n\nEffective genome length\n\n\nTo calculate \u03bbBG from tag count, MAC2 requires the \neffective genome size\n or the size of the genome that is mappable. Mappability is related to the uniqueness of the k-mers at a  particular position the genome. Low-complexity and repetitive regions have low uniqueness, which means low mappability. Therefore we need to provide the effective genome length to \ncorrect for the loss of true signals in low-mappable regions\n.\n\n\n\n\nThe mappability or uniqueness influences the average mapped depth (i.e if the effective genome length is small, the proportion of reads that map will be small). As shown in the table below \nmappability improves with increased read length\n. When low-mappable regions (e.g. a ratio \u2009\n \u20090.25) are of interest, it might be better to include multiple mapped reads or use paired-end reads.\n\n\n\n\nPeak detection\n\n\nFor ChIP-Seq experiments, tag distribution along the genome can be modeled by a Poisson distribution. After MACS shifts every tag, it then slides 2d windows across the genome to find candidate peaks with a significant tag enrichment (default is p \n 10e-5). This is a Poisson distribution p-value based on \u03bb. The Poisson is a one parameter model, where the parameter \n\u03bb is the expected number of reads in that window\n.\n\n\n\n\nInstead of using a uniform \u03bb estimated from the whole genome, MACS uses a dynamic parameter, \u03bblocal, defined for each candidate peak. The lambda parameter is estimated from the control sample and is deduced by \ntaking the maximum value across various window sizes:\n \n\n\n\u03bblocal = max(\u03bbBG, \u03bb1k, \u03bb5k, \u03bb10k).\n \n\n\nIn this way lambda captures the influence of local biases, and is \nrobust against occasional low tag counts at small local regions\n. Possible sources for these biases include local chromatin structure, DNA amplification and sequencing bias, and genome copy number variation.\n\n\n\n\nOverlapping enriched peaks are merged, and each tag position is extended 'd' bases from its center. The location with the highest fragment pileup, hereafter referred to as the summit, is predicted as the precise binding location. The ratio between the ChIP-Seq tag count and \u03bblocal is reported as the fold enrichment.\n\n\nEstimation of false discovery rate\n\n\nEach peak is considered an independent test and thus, when we encounter thousands of significant peaks detected in a sample we have a multiple testing problem. In MACSv1.4, the FDR was determined empirically by exchanging the ChIP and control samples. However, in MACS2, p-values are now corrected for multiple comparison using the \nBenjamini-Hochberg correction\n.\n\n\nRunning MACS2\n\n\nWe will be using the newest version of this tool, MACS2. The underlying algorithm for peak calling remains the same as before, but it comes with some enhancements in functionality. \n\n\nSetting up\n\n\nTo run MACS2, we will first start an interactive session using 1 core (do this only if you don't already have one) and load the macs2 library:\n\n\n$ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash\n\n$ module load gcc/6.2.0  python/2.7.12 macs2/2.1.1.20160309\n\n\n\n\nWe will also need to create a directory for the output generated from MACS2:\n\n\n$ mkdir -p ~/chipseq/results/macs2\n\n\n\n\nNow change directories to the \nresults\n folder:\n\n\n$ cd ~/chipseq/results/\n\n\n\n\n\n\nNOTE:\n If your automation script was successful, you should have alignment information for \nall 6 files\n. However, if you do not have these BAM files then you can copy them over using the command below:\n\n\n$ cp /n/groups/hbctraining/chip-seq/bowtie2/*.bam ~/chipseq/results/bowtie2/\n\n\n\n\nMACS2 parameters\n\n\nThere are seven \nmajor functions\n available in MACS2 serving as sub-commands. We will only cover \ncallpeak\n in this lesson, but you can use \nmacs2 COMMAND -h\n to find out more, if you are interested.\n\n\ncallpeak\n is the main function in MACS2 and can be invoked by typing \nmacs2 callpeak\n. If you type this command without parameters, you will see a full description of commandline options. Here is a shorter list of the commonly used ones: \n\n\nInput file options\n\n\n\n\n-t\n: The IP data file (this is the only REQUIRED parameter for MACS)\n\n\n-c\n: The control or mock data file\n\n\n-f\n: format of input file; Default is \"AUTO\" which will allow MACS to decide the format automatically.\n\n\n-g\n: mappable genome size which is defined as the genome size which can be sequenced; some precompiled values provided.\n\n\n\n\nOutput arguments\n\n\n\n\n--outdir\n: MACS2 will save all output files into speficied folder for this option\n\n\n-n\n: The prefix string for output files\n\n\n-B/--bdg\n: store the fragment pileup, control lambda, -log10pvalue and -log10qvalue scores in bedGraph files\n\n\n\n\nShifting model arguments\n\n\n\n\n-s\n: size of sequencing tags. Default, MACS will use the first 10 sequences from your input treatment file to determine it\n\n\n--bw\n: The bandwidth which is used to scan the genome ONLY for model building. Can be set to the expected sonication fragment size.\n\n\n--mfold\n: upper and lower limit for model building\n\n\n\n\nPeak calling arguments\n\n\n\n\n-q\n: q-value (minimum FDR) cutoff\n\n\n-p\n: p-value cutoff (instead of q-value cutoff)\n\n\n--nolambda\n: do not consider the local bias/lambda at peak candidate regions\n\n\n--broad\n: broad peak calling\n\n\n\n\n\n\nNOTE:\n Relaxing the q-value does not behave as expected in this case since it is partially tied to peak widths. Ideally, if you relaxed the thresholds, you would simply get more peaks but with MACS2 relaxing thresholds also results in wider peaks.\n\n\n\n\nNow that we have a feel for the different ways we can tweak our command, let's set up the command for our run on Nanog-rep1:\n\n\n$ macs2 callpeak -t bowtie2/H1hesc_Nanog_Rep1_aln.bam \\\n    -c bowtie2/H1hesc_Input_Rep1_aln.bam \\\n    -f BAM -g 1.3e+8 \\\n    -n Nanog-rep1 \\\n    --outdir macs2\n\n\n\n\nThe tool is quite verbose so you should see lines of text being printed to the terminal, describing each step that is being carried out. If that runs successfully, go ahead and \nre-run the same command but this time let's capture that information into a log file using \n2\n to re-direct the stadard error to file\n:\n\n\n$ macs2 callpeak -t bowtie2/H1hesc_Nanog_Rep1_aln.bam \\\n    -c bowtie2/H1hesc_Input_Rep1_aln.bam \\\n    -f BAM -g 1.3e+8 \\\n    -n Nanog-rep1 \\\n    --outdir macs2 2\n macs2/Nanog-rep1-macs2.log\n\n\n\n\nOk, now let's do the same peak calling for the rest of our samples:\n\n\nmacs2 callpeak -t bowtie2/H1hesc_Nanog_Rep2_aln.bam -c bowtie2/H1hesc_Input_Rep2_aln.bam -f BAM -g 1.3e+8 --outdir macs2 -n Nanog-rep2 2\n macs2/Nanog-rep2-macs2.log\n\nmacs2 callpeak -t bowtie2/H1hesc_Pou5f1_Rep1_aln.bam -c bowtie2/H1hesc_Input_Rep1_aln.bam -f BAM -g 1.3e+8 --outdir macs2 -n Pou5f1-rep1 2\n macs2/Pou5f1-rep1-macs2.log\n\nmacs2 callpeak -t bowtie2/H1hesc_Pou5f1_Rep2_aln.bam -c bowtie2/H1hesc_Input_Rep2_aln.bam -f BAM -g 1.3e+8 --outdir macs2 -n Pou5f1-rep2 2\n macs2/Pou5f1-rep2-macs2.log\n\n\n\n\n\nMACS2 Output files\n\n\nFile formats\n\n\nBefore we start exploring the output of MACS2, we'll briefly talk about the new file formats you will encounter.\n\n\nnarrowPeak:\n\n\nA narrowPeak (.narrowPeak) file is used by the ENCODE project to provide called peaks of signal enrichment based on pooled, normalized (interpreted) data. It is a BED 6+4 format, which means \nthe first 6 columns of a standard BED file  with 4 additional fields\n:\n\n\n\n\nchromosome\n\n\nstart coordinate\n\n\nend coordinate\n\n\nname\n\n\nscore\n\n\nstrand\n\n\nsignalValue - Measurement of overall enrichment for the region\n\n\npValue - Statistical significance (-log10)\n\n\nqValue - Statistical significance using false discovery rate (-log10)\n\n\npeak - Point-source called for this peak; 0-based offset from chromStart\n\n\n\n\nWIG format:\n\n\nWiggle format (WIG) allows the display of continuous-valued data in a track format. Wiggle format is line-oriented. It is composed of declaration lines and data lines, and require a separate wiggle track definition line. There are two options for formatting wiggle data: variableStep and fixedStep. These formats were developed to allow the file to be written as compactly as possible.\n\n\nBedGraph format:\n\n\nThe BedGraph format also allows display of continuous-valued data in track format. This display type is useful for probability scores and transcriptome data. This track type is similar to the wiggle (WIG) format, but unlike the wiggle format, data exported in the bedGraph format are preserved in their original state. For the purposes of visualization, these can be interchangeable.\n\n\nMACS2 output files\n\n\n$ cd macs2/\n\n$ ls -lh\n\n\n\nLet's first move the log files to the \nlog\n directory:\n\n\n$ mv *.log ../../logs/\n\n\n\nNow, there should be 6 files output to the results directory for each of the 4 samples, so a total of 24 files:\n\n\n\n\n_peaks.narrowPeak\n: BED6+4 format file which contains the peak locations together with peak summit, pvalue and qvalue\n\n\n_peaks.xls\n: a tabular file which contains information about called peaks. Additional information includes pileup and fold enrichment\n\n\n_summits.bed\n: peak summits locations for every peak. To find the motifs at the binding sites, this file is recommended\n\n\n_model.R\n: an R script which you can use to produce a PDF image about the model based on your data and cross-correlation plot\n\n\n_control_lambda.bdg\n: bedGraph format for input sample\n\n\n_treat_pileup.bdg\n: bedGraph format for treatment sample\n\n\n\n\nLet's first obtain a summary of how many peaks were called in each sample. We can do this by counting the lines in the \n.narrowPeak\n files:\n\n\n$ wc -l *.narrowPeak\n\n\n\nWe can also generate plots using the R script file that was output by MACS2. There is a \n_model.R\n script in the directory. Let's load the R module and run the R script in the command line using the \nRscript\n command as demonstrated below:\n\n\n$ module load gcc/6.2.0 R/3.4.1\n\n$ Rscript Nanog-rep1_model.r\n\n\n\n\n\nNOTE:\n We need to load the \ngcc/6.2.0\n before loading R. You can find out which modules need to be loaded first by using module spider R/3.4.1` \n\n\n\n\nNow you should see a pdf file in your current directory by the same name. Create the plots for each of the samples and move them over to your laptop using \nFilezilla\n. \n\n\nOpen up the pdf file for Nanog-rep1. The first plot illustrates \nthe distance between the modes from which the shift size was determined\n. \n\n\n\n\nThe second plot is the  \ncross-correlation plot\n. This is a graphical representation of the Pearson correlation of positive- and negative- strand tag densities, shifting the strands relative to each other by increasing distance. We will talk about this in more detail in the next lesson.\n\n\n\n\nNOTE:\n \nSPP\n is another very commonly used tool for \nnarrow\n peak calling. While we will not be going through the steps for this peak caller in this workshop, we do have \na lesson on SPP\n that we encourage you to browse through if you are interested in learning more.\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Peak calling with MACS2"
        }, 
        {
            "location": "/05_peak_calling_macs/#learning-objectives", 
            "text": "Describe the different components of the MACS2 peak calling algorithm  Describe the parameters involved in running MACS2  List and describe the output files from MACS2", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/05_peak_calling_macs/#peak-calling", 
            "text": "Peak calling, the next step in our workflow, is a computational method used to identify areas in the genome that have been enriched with aligned reads as a consequence of performing a ChIP-sequencing experiment.    For ChIP-seq experiments, what we observe from the alignment files is a  strand asymmetry with read densities on the +/- strand, centered around the binding site . The 5' ends of the selected fragments will form groups on the positive- and negative-strand. The distributions of these groups are then assessed using statistical measures and compared against background (input or mock IP samples) to determine if the site of enrichment is likely to be a real binding site.   There are various tools that are available for peak calling. One of the more commonly used pack callers is MACS2, and we will demonstrate it in this session.  Note that in this Session the term 'tag' and sequence 'read' are used interchangeably.   NOTE:  Our dataset is investigating two transcription factors and so our focus is on identifying short degenerate sequences that present as punctate binding sites. ChIP-seq analysis algorithms are specialized in identifying one of  two types of enrichment  (or have specific methods for each):  broad peaks  or broad domains (i.e. histone modifications that cover entire gene bodies) or  narrow peaks  (i.e. a transcription factor binding). Narrow peaks are easier to detect as we are looking for regions that have higher amplitude and are easier to distinguish from the background, compared to broad or dispersed marks. There are also 'mixed' binding profiles which can be hard for algorithms to discern. An example of this is the binding properties of PolII, which binds at promotor and across the length of the gene resulting in mixed signals (narrow and broad).", 
            "title": "Peak Calling"
        }, 
        {
            "location": "/05_peak_calling_macs/#macs2", 
            "text": "A commonly used tool for identifying transcription factor binding sites is named  Model-based Analysis of ChIP-Seq (MACS) . The  MACS algorithm  captures the influence of genome complexity to evaluate the significance of enriched ChIP regions. Although it was developed for the detection of transcription factor binding sites it is also suited for larger regions.  MACS improves the spatial resolution of binding sites through  combining the information of both sequencing tag position and orientation.  MACS can be easily used either for the ChIP sample alone, or along with a control sample which increases specificity of the peak calls. The MACS workflow is depicted below. In this lesson, we will describe the steps in more detail.", 
            "title": "MACS2"
        }, 
        {
            "location": "/05_peak_calling_macs/#removing-redundancy", 
            "text": "MACS provides different options for dealing with  duplicate tags  at the exact same location, that is tags with the  same coordination and the same strand . The default is to keep a single read at each location. The  auto  option, which is very commonly used, tells MACS to calculate the maximum tags at the exact same location based on binomal distribution using 1e-5 as the pvalue cutoff. An alternative is to set the  all  option, which keeps every tag. If an  integer  is specified, then at most that many tags will be kept at the same location. This redundancy is consistently applied for both the ChIP and input samples.   Why worry about duplicates? \nReads with the same start position are considered duplicates. These duplicates can arise from experimental artifacts, but can also contribute to genuine ChIP-signal.   The bad kind of duplicates:  If initial starting material is low this can lead to overamplification of this material before sequencing. Any biases in PCR will compound this problem and can lead to artificially enriched regions. Also blacklisted (repeat) regions with ultra high signal will also be high in duplicates. Masking these regions prior to analysis can help remove this problem.  The good kind of duplicates:  Duplicates will also exist within highly efficient (or even inefficient ChIP) when deeply sequenced. Removal of these duplicates can lead to a saturation and so underestimation of the ChIP signal.   The take-home:  Consider your enrichment efficiency and sequencing depth. But, because we cannot distinguish between the good and the bad, best practice is to remove duplicates prior to peak calling.  Retain duplicates for differential binding analysis. Also, if you are expecting binding in repetitive regions keep duplicates and multiple mappers.", 
            "title": "Removing redundancy"
        }, 
        {
            "location": "/05_peak_calling_macs/#modeling-the-shift-size", 
            "text": "The tag density around a true binding site should show a  bimodal enrichment pattern  (or paired peaks). MACS takes advantage of this bimodal pattern to empirically model the shifting size to better locate the precise binding sites.  To find paired peaks to  build the model , MACS first scans the whole dataset searching for highly significant enriched regions.  This is done only using the ChIP sample!  Given a sonication size ( bandwidth ) and a high-confidence fold-enrichment ( mfold ), MACS slides two  bandwidth  windows across the genome to find regions with  tags more than  mfold  enriched relative to a random tag genome distribution .    MACS randomly  samples 1,000 of these high-quality peaks , separates their Watson and Crick tags, and aligns them by the midpoint between their Watson and Crick tag centers. The  distance between the modes of the Watson and Crick peaks in the alignment is defined as 'd'  and represents the estimated fragment length. MACS shifts all the tags by d/2 toward the 3' ends to the most likely protein-DNA interaction sites.", 
            "title": "Modeling the shift size"
        }, 
        {
            "location": "/05_peak_calling_macs/#scaling-libraries", 
            "text": "For experiments in which sequence depth differs between input and treatment samples, MACS linearly scales the  total control tag count to be the same as the total ChIP tag count . The default behaviour is for the larger sample to be scaled down.", 
            "title": "Scaling libraries"
        }, 
        {
            "location": "/05_peak_calling_macs/#effective-genome-length", 
            "text": "To calculate \u03bbBG from tag count, MAC2 requires the  effective genome size  or the size of the genome that is mappable. Mappability is related to the uniqueness of the k-mers at a  particular position the genome. Low-complexity and repetitive regions have low uniqueness, which means low mappability. Therefore we need to provide the effective genome length to  correct for the loss of true signals in low-mappable regions .   The mappability or uniqueness influences the average mapped depth (i.e if the effective genome length is small, the proportion of reads that map will be small). As shown in the table below  mappability improves with increased read length . When low-mappable regions (e.g. a ratio \u2009  \u20090.25) are of interest, it might be better to include multiple mapped reads or use paired-end reads.", 
            "title": "Effective genome length"
        }, 
        {
            "location": "/05_peak_calling_macs/#peak-detection", 
            "text": "For ChIP-Seq experiments, tag distribution along the genome can be modeled by a Poisson distribution. After MACS shifts every tag, it then slides 2d windows across the genome to find candidate peaks with a significant tag enrichment (default is p   10e-5). This is a Poisson distribution p-value based on \u03bb. The Poisson is a one parameter model, where the parameter  \u03bb is the expected number of reads in that window .   Instead of using a uniform \u03bb estimated from the whole genome, MACS uses a dynamic parameter, \u03bblocal, defined for each candidate peak. The lambda parameter is estimated from the control sample and is deduced by  taking the maximum value across various window sizes:    \u03bblocal = max(\u03bbBG, \u03bb1k, \u03bb5k, \u03bb10k).    In this way lambda captures the influence of local biases, and is  robust against occasional low tag counts at small local regions . Possible sources for these biases include local chromatin structure, DNA amplification and sequencing bias, and genome copy number variation.   Overlapping enriched peaks are merged, and each tag position is extended 'd' bases from its center. The location with the highest fragment pileup, hereafter referred to as the summit, is predicted as the precise binding location. The ratio between the ChIP-Seq tag count and \u03bblocal is reported as the fold enrichment.", 
            "title": "Peak detection"
        }, 
        {
            "location": "/05_peak_calling_macs/#estimation-of-false-discovery-rate", 
            "text": "Each peak is considered an independent test and thus, when we encounter thousands of significant peaks detected in a sample we have a multiple testing problem. In MACSv1.4, the FDR was determined empirically by exchanging the ChIP and control samples. However, in MACS2, p-values are now corrected for multiple comparison using the  Benjamini-Hochberg correction .", 
            "title": "Estimation of false discovery rate"
        }, 
        {
            "location": "/05_peak_calling_macs/#running-macs2", 
            "text": "We will be using the newest version of this tool, MACS2. The underlying algorithm for peak calling remains the same as before, but it comes with some enhancements in functionality.", 
            "title": "Running MACS2"
        }, 
        {
            "location": "/05_peak_calling_macs/#setting-up", 
            "text": "To run MACS2, we will first start an interactive session using 1 core (do this only if you don't already have one) and load the macs2 library:  $ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash\n\n$ module load gcc/6.2.0  python/2.7.12 macs2/2.1.1.20160309  We will also need to create a directory for the output generated from MACS2:  $ mkdir -p ~/chipseq/results/macs2  Now change directories to the  results  folder:  $ cd ~/chipseq/results/   NOTE:  If your automation script was successful, you should have alignment information for  all 6 files . However, if you do not have these BAM files then you can copy them over using the command below:  $ cp /n/groups/hbctraining/chip-seq/bowtie2/*.bam ~/chipseq/results/bowtie2/", 
            "title": "Setting up"
        }, 
        {
            "location": "/05_peak_calling_macs/#macs2-parameters", 
            "text": "There are seven  major functions  available in MACS2 serving as sub-commands. We will only cover  callpeak  in this lesson, but you can use  macs2 COMMAND -h  to find out more, if you are interested.  callpeak  is the main function in MACS2 and can be invoked by typing  macs2 callpeak . If you type this command without parameters, you will see a full description of commandline options. Here is a shorter list of the commonly used ones:   Input file options   -t : The IP data file (this is the only REQUIRED parameter for MACS)  -c : The control or mock data file  -f : format of input file; Default is \"AUTO\" which will allow MACS to decide the format automatically.  -g : mappable genome size which is defined as the genome size which can be sequenced; some precompiled values provided.   Output arguments   --outdir : MACS2 will save all output files into speficied folder for this option  -n : The prefix string for output files  -B/--bdg : store the fragment pileup, control lambda, -log10pvalue and -log10qvalue scores in bedGraph files   Shifting model arguments   -s : size of sequencing tags. Default, MACS will use the first 10 sequences from your input treatment file to determine it  --bw : The bandwidth which is used to scan the genome ONLY for model building. Can be set to the expected sonication fragment size.  --mfold : upper and lower limit for model building   Peak calling arguments   -q : q-value (minimum FDR) cutoff  -p : p-value cutoff (instead of q-value cutoff)  --nolambda : do not consider the local bias/lambda at peak candidate regions  --broad : broad peak calling    NOTE:  Relaxing the q-value does not behave as expected in this case since it is partially tied to peak widths. Ideally, if you relaxed the thresholds, you would simply get more peaks but with MACS2 relaxing thresholds also results in wider peaks.   Now that we have a feel for the different ways we can tweak our command, let's set up the command for our run on Nanog-rep1:  $ macs2 callpeak -t bowtie2/H1hesc_Nanog_Rep1_aln.bam \\\n    -c bowtie2/H1hesc_Input_Rep1_aln.bam \\\n    -f BAM -g 1.3e+8 \\\n    -n Nanog-rep1 \\\n    --outdir macs2  The tool is quite verbose so you should see lines of text being printed to the terminal, describing each step that is being carried out. If that runs successfully, go ahead and  re-run the same command but this time let's capture that information into a log file using  2  to re-direct the stadard error to file :  $ macs2 callpeak -t bowtie2/H1hesc_Nanog_Rep1_aln.bam \\\n    -c bowtie2/H1hesc_Input_Rep1_aln.bam \\\n    -f BAM -g 1.3e+8 \\\n    -n Nanog-rep1 \\\n    --outdir macs2 2  macs2/Nanog-rep1-macs2.log  Ok, now let's do the same peak calling for the rest of our samples:  macs2 callpeak -t bowtie2/H1hesc_Nanog_Rep2_aln.bam -c bowtie2/H1hesc_Input_Rep2_aln.bam -f BAM -g 1.3e+8 --outdir macs2 -n Nanog-rep2 2  macs2/Nanog-rep2-macs2.log\n\nmacs2 callpeak -t bowtie2/H1hesc_Pou5f1_Rep1_aln.bam -c bowtie2/H1hesc_Input_Rep1_aln.bam -f BAM -g 1.3e+8 --outdir macs2 -n Pou5f1-rep1 2  macs2/Pou5f1-rep1-macs2.log\n\nmacs2 callpeak -t bowtie2/H1hesc_Pou5f1_Rep2_aln.bam -c bowtie2/H1hesc_Input_Rep2_aln.bam -f BAM -g 1.3e+8 --outdir macs2 -n Pou5f1-rep2 2  macs2/Pou5f1-rep2-macs2.log", 
            "title": "MACS2 parameters"
        }, 
        {
            "location": "/05_peak_calling_macs/#macs2-output-files", 
            "text": "", 
            "title": "MACS2 Output files"
        }, 
        {
            "location": "/05_peak_calling_macs/#file-formats", 
            "text": "Before we start exploring the output of MACS2, we'll briefly talk about the new file formats you will encounter.  narrowPeak:  A narrowPeak (.narrowPeak) file is used by the ENCODE project to provide called peaks of signal enrichment based on pooled, normalized (interpreted) data. It is a BED 6+4 format, which means  the first 6 columns of a standard BED file  with 4 additional fields :   chromosome  start coordinate  end coordinate  name  score  strand  signalValue - Measurement of overall enrichment for the region  pValue - Statistical significance (-log10)  qValue - Statistical significance using false discovery rate (-log10)  peak - Point-source called for this peak; 0-based offset from chromStart   WIG format:  Wiggle format (WIG) allows the display of continuous-valued data in a track format. Wiggle format is line-oriented. It is composed of declaration lines and data lines, and require a separate wiggle track definition line. There are two options for formatting wiggle data: variableStep and fixedStep. These formats were developed to allow the file to be written as compactly as possible.  BedGraph format:  The BedGraph format also allows display of continuous-valued data in track format. This display type is useful for probability scores and transcriptome data. This track type is similar to the wiggle (WIG) format, but unlike the wiggle format, data exported in the bedGraph format are preserved in their original state. For the purposes of visualization, these can be interchangeable.", 
            "title": "File formats"
        }, 
        {
            "location": "/05_peak_calling_macs/#macs2-output-files_1", 
            "text": "$ cd macs2/\n\n$ ls -lh  Let's first move the log files to the  log  directory:  $ mv *.log ../../logs/  Now, there should be 6 files output to the results directory for each of the 4 samples, so a total of 24 files:   _peaks.narrowPeak : BED6+4 format file which contains the peak locations together with peak summit, pvalue and qvalue  _peaks.xls : a tabular file which contains information about called peaks. Additional information includes pileup and fold enrichment  _summits.bed : peak summits locations for every peak. To find the motifs at the binding sites, this file is recommended  _model.R : an R script which you can use to produce a PDF image about the model based on your data and cross-correlation plot  _control_lambda.bdg : bedGraph format for input sample  _treat_pileup.bdg : bedGraph format for treatment sample   Let's first obtain a summary of how many peaks were called in each sample. We can do this by counting the lines in the  .narrowPeak  files:  $ wc -l *.narrowPeak  We can also generate plots using the R script file that was output by MACS2. There is a  _model.R  script in the directory. Let's load the R module and run the R script in the command line using the  Rscript  command as demonstrated below:  $ module load gcc/6.2.0 R/3.4.1\n\n$ Rscript Nanog-rep1_model.r   NOTE:  We need to load the  gcc/6.2.0  before loading R. You can find out which modules need to be loaded first by using module spider R/3.4.1`    Now you should see a pdf file in your current directory by the same name. Create the plots for each of the samples and move them over to your laptop using  Filezilla .   Open up the pdf file for Nanog-rep1. The first plot illustrates  the distance between the modes from which the shift size was determined .    The second plot is the   cross-correlation plot . This is a graphical representation of the Pearson correlation of positive- and negative- strand tag densities, shifting the strands relative to each other by increasing distance. We will talk about this in more detail in the next lesson.   NOTE:   SPP  is another very commonly used tool for  narrow  peak calling. While we will not be going through the steps for this peak caller in this workshop, we do have  a lesson on SPP  that we encourage you to browse through if you are interested in learning more.    This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "MACS2 output files"
        }, 
        {
            "location": "/06_QC_cross_correlation/", 
            "text": "Approximate time: 60 minutes\n\n\nLearning Objectives\n\n\n\n\nDiscuss sources of low quality ChIP-seq data\n\n\nUnderstand strand cross-correlation\n\n\nUse \nphantompeakqualtools\n to compute cross-correlation and associated QC metrics\n\n\nEvaluate and interpret QC metrics and the cross-correlation plot\n\n\n\n\nChIP-Seq quality assessment\n\n\nPrior to performing any downstream analyses with the results from a peak caller, it is best practice to assess the quality of your ChIP-Seq data. What we are looking for is good   quality ChIP-seq enrichment over background.\n\n\n\n\nStrand cross-correlation\n\n\nA very useful ChIP-seq quality metric that is independent of peak calling is strand cross-correlation. It is based on the fact that a high-quality ChIP-seq experiment will produce significant clustering of enriched DNA sequence tags at locations bound by the protein of interest, that present as a bimodal enrichment of reads on the forward and reverse strands.\n\n\nThe bimodal enrichment of reads is due to the following:\n\n\nDuring the ChIP-seq experiment, the DNA is fragmented and the protein-bound fragments are immunoprecipitated. This generates DNA fragments containing the protein-bound region. \n\n\nThe + strand of DNA is sequenced from the 5' end, generating the red reads in the figure below, and the - strand of DNA is sequenced from the 5' end, generating the blue reads in the figure below. \n\n\n\n\nNat Biotechnol. 2008 Dec; 26(12): 1351\u20131359\n\n\nDue to the sequencing of the 5' ends of the fragments, this results in an enrichment of reads from the + strand (blue in the image below) being slightly offset from the enrichment of reads from the - strand (red in the image below). We need to \ndetermine the number of bases to shift the peaks to yield maximum correlation between the two peaks\n, which \nshould\n correspond to the predominant \nfragment length\n. We can calculate the shift yielding the maximum correlation using the \ncross-correlation metric\n.\n\n\n\n\nCross-correlation metric\n\n\nThe cross-correlation metric is computed as the \nPearson's linear correlation between the Crick strand and the Watson strand, after shifting Watson by k base pairs.\n Using a small genomic window as an example, let's walk through the details of the cross-correlation below.\n\n\nAt strand shift of zero, the Pearson correlation between the two vectors is 0.539.\n\n\n\n\nAt strand shift of 5bp, the Pearson correlation between the two vectors is 0.931\n\n\n\n\nKeep shifting the vectors and for each strand shift compute a correlation value.\n \n\n\n\n\nIn the end, we will have a table of values mapping each base pair shift to a Pearson correlation value. This is computed for every peak for each chromosome and values are multiplied by a scaling factor and then summed across all chromosomes. We can then \nplot cross-correlation values (y-axis) against the shift value (x-axis)\n to generate a cross-correlation plot.\n\n\nThe cross-correlation plot \ntypically produces two peaks\n: a peak of enrichment corresponding to the predominant \nfragment length\n (highest correlation value) and a peak corresponding to the \nread length\n (\u201cphantom\u201d peak).\n\n\nHigh-quality ChIP-seq data sets tend to have a larger fragment-length peak compared with the read-length peak. An example of a \nstrong signal\n is shown below using data from \nCTCF (zinc-finger transcription factor)\n in human cells. With a good antibody, transcription factors will typically result in 45,000 - 60,000 peaks. The red vertical line shows the dominant peak at the true peak shift, with a small bump at the blue vertical line representing the read length.\n\n\n \n\n\nAn example of \nweaker signal\n is demonstrated below with a \nPol2\n data. Here, this particular antibody is not very efficient and these are broad scattered peaks. We observe two peaks in the cross-correlation profile: one at the true peak shift (~185-200 bp) and the other at read length. For weak signal datasets, the \nread-length peak will start to dominate\n.\n\n\n\n\nA failed experiment will resemble a cross-correlation plot using \ninput only\n, in which we observe little or no peak for fragment length. Note in the example below the \nstrongest peak is the blue line (read length)\n and there is basically no other significant peak in the profile. The absence of a peak is expected since there should be no significant clustering of fragments around specific target sites (except potentially weak biases in open chromatin regions depending on the protocol used). The read-length peak occurs due to unique mappability properties of the mapped reads.\n\n\n \n\n\nCross-correlation quality metrics\n\n\nUsing the cross-correlation plot we can \ncompute metrics for assessing signal-to-noise ratios in a ChIP-seq experiment\n and to ensure the fragment length is accurate based on the experimental design. Poor signal-to-noise and inaccurate fragment lengths can indicate problems with the ChIP-Seq data. These metrics are described in more detail below:\n\n\nNormalized strand cross-correlation coefficent (NSC):\n\n\nThe ratio of the maximal cross-correlation value divided by the background cross-correlation (minimum cross-correlation value over all possible strand shifts). \nHigher values indicate more enrichment, values less than 1.1 are relatively low NSC scores, and the minimum possible value is 1 (no enrichment).\n Datasets with NSC values much less than 1.05 tend to have low signal to noise or few peaks (this could be biological, such as a factor that truly binds only a few sites in a particular tissue type or it could be due to poor quality).\n\n\nRelative strand cross-correlation coefficient (RSC):\n\n\nThe ratio of the fragment-length cross-correlation value minus the background cross-correlation value, divided by the phantom-peak cross-correlation value minus the background cross-correlation value. \nThe minimum possible value is 0 (no signal), highly enriched experiments have values greater than 1, and values much less than 1 may indicate low quality.\n RSC values significantly low (\n 0.8) tend to have low signal to noise and can be due to failed and poor quality ChIP, low read sequence quality and hence lots of mismappings, shallow sequencing depth or a combination of these. Like the NSC, datasets with few binding sites (\n 200) which are biologically justifiable also show low RSC scores.\n\n\nphantompeakqualtools\n\n\nThe \nphantompeakqualtools\n package is a tool used to compute enrichment and quality measures for ChIP-Seq data [\n1\n]. We will be using the package to compute the predominant insert-size (fragment length) based on strand cross-correlation peak and data quality measures based on relative phantom peak.\n\n\nSet up\n\n\nThe \nphantompeakqualtools\n package is written as an R script, that uses \nsamtools\n as a dependency. The package has various options that need to be specified when running from the command line. To get set up, we will need to start an interactive session, load the necessary modules and set up the directory structure:\n\n\n$ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash    \n\n$ module load gcc/6.2.0 R/3.4.1 samtools/1.3.1\n\n$ cd ~/chipseq/results\n\n$ mkdir chip_qc\n\n$ cd chip_qc\n\n\n\n\nWe have downloaded this for you, and have a copy you can use.\n  The directory contains several files.\n\n\n$ ls -l /n/groups/hbctraining/chip-seq/phantompeakqualtools/\n\n\n\n\n\n\nNOTE:\n  You can download the \nphantompeakqualtools\n package, directly from \nGitHub\n, if you wanted your own local version. This repo is actively maintained by the developer Anshul Kundaje.\n\n\n\n\nIn this folder there should be a \nREADME.txt\n which contains all the commands, options, and output descriptions. Let's check out the \nREADME.txt\n:\n\n\n$ less /n/groups/hbctraining/chip-seq/phantompeakqualtools/README.md\n\n\n\n\nNote that there are two R scripts that are described in the README file. Both will compute the fragment length, and data quality characteristics based on cross-correlation analysis, but one is for use in situations where the duplicates have been removed (\nrun_spp_nodups.R\n). \nThis is the script we will be using.\n\n\nUsing R libraries\n\n\nIn the README you will have noticed an \nINSTALLATION\n section. We will need to install the R package, \nspp\n and \ncaTools\n, into our personal R library to run the script. Since this is a bit more involved, in the interest of time we have created the libraries and shared them for you to use. To use our libraries, you will need to setup an environmental variable called \nR_LIBS_USER\n and point it to the location on O2 where our libraries reside:\n\n\n$  export R_LIBS_USER=\n/n/groups/hbctraining/R/library/\n\n\n\n\n\n\n\nNOTE: Testing libraries\n\n\nIf you want to check and see that this is working, you can open up R by typing R and pressing Enter:\n\n\n\n\n\n$ R\n\n\n\n\n\n\nAnd then once in R, try loading the libraries:\n\n\n\n\nR version 3.4.1 (2017-06-30) -- \nSingle Candle\n\nCopyright (C) 2017 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n\n library(spp)\n\n library(caTools)\n\n\n\n\n\n\nTo exit, type: \nq()\n in the console.\n\n\n\n\nRunning \nphantompeakqualtools\n\n\nTo obtain quality measures based on cross-correlation plots, we will be running the \nrun_spp_nodups.R\n script from the command line which is a package built on SPP. This modified SPP package allows for determination of the cross-correlation peak and predominant fragment length in addition to peak calling. We will be using this package solely for obtaining these quality measures (no peak calling). \n\n\nThe options that we will be using include:\n\n\n\n\n-c\n: full path and name (or URL) of tagAlign/BAM file\n\n\n-savp\n: save cross-correlation plot\n\n\n-out\n: will create and/or append to a file several important characteristics of the dataset described in more detail below.\n\n\n\n\n## DO NOT RUN THIS\n## THIS SCRIPT IS FOR COMPUTING METRICS ON A SINGLE FILE\n$ Rscript /n/groups/hbctraining/chip-seq/phantompeakqualtools/run_spp.R -c=\ntagAlign/BAMfile\n -savp -out=\noutFile\n\n\n\n\n\n\n\nNOTE:\n Even though the script is called \nrun_spp.R\n, we aren't actually performing peak calling with SPP.\n\n\n\n\nFrom within the \nphantompeakqualtools\n directory, we will create output directories and use a 'for loop' to \nrun the script on every Nanog and Pouf51 BAM file\n:\n\n\n$ mkdir -p logs qual\n\n$ for bam in ../bowtie2/*Nanog*aln.bam ../bowtie2/*Pou5f1*aln.bam\ndo \nbam2=`basename $bam _aln.bam`\nRscript /n/groups/hbctraining/chip-seq/phantompeakqualtools/run_spp.R -c=$bam -savp -out=qual/${bam2}.qual \n logs/${bam2}.Rout\ndone\n\n\n\n\nThe for loop generates \nthree output files\n. The \nquality metrics\n are written in a tab-delimited text file, and the \nlog files\n contains the standard output text. A third file is created in the same directory as the BAM files. These are pdf files that contain the \ncross-correlation\n plot for each sample. Let's move those files into the appropriate output directory:\n\n\n$ mv ../bowtie2/*pdf qual  \n\n\n\n\n\nTo visualize the quality metrics (.qual) files more easily, we will concatenate the files together to create a single summary file that you can move over locally and open up with Excel.\n\n\n$ cat qual/*qual \n qual/phantompeaks_summary.xls\n\n\n\n\nLet's use Filezilla or \nscp\n to move the summary file over to our local machine for viewing. Open up the file in Excel and take a look at our NSC and RSC values. \n\n\nphantompeakqualtools\n: quality metrics output\n\n\nThe qual files are tab-delimited with the columns containing the following information:\n\n\n\n\nCOL1: Filename: tagAlign/BAM filename \n\n\nCOL2: numReads: effective sequencing depth (i.e. total number of mapped reads in input file)\n\n\nCOL3: estFragLen: comma separated strand cross-correlation peak(s) in decreasing order of correlation. (\nNOTE:\n The top 3 local maxima locations that are within 90% of the maximum cross-correlation value are output. In almost all cases, the top (first) value in the list represents the predominant fragment length.) \n\n\nCOL4: corr_estFragLen: comma separated strand cross-correlation value(s) in decreasing order (col2 follows the same order) \n\n\nCOL5: phantomPeak: Read length/phantom peak strand shift \n\n\nCOL6: corr_phantomPeak: Correlation value at phantom peak \n\n\nCOL7: argmin_corr: strand shift at which cross-correlation is lowest \n\n\nCOL8: min_corr: minimum value of cross-correlation \n\n\nCOL9: Normalized strand cross-correlation coefficient (NSC) = COL4 / COL8 \n\n\nCOL10: Relative strand cross-correlation coefficient (RSC) = (COL4 - COL8) / (COL6 - COL8) \n\n\nCOL11: QualityTag: Quality tag based on thresholded RSC (codes: -2:veryLow,-1:Low,0:Medium,1:High,2:veryHigh)\n\n\n\n\n\n\nNOTE:\n The most important metrics we are interested in are the values in columns 9 through 11, however these numbers are computed from values in the other columns.\n\n\n\n\nHow do the values compare to the thresholds mentioned above?\n All samples have quite high NSC values indicating more enrichment, a good signal to noise and a fair number of peaks. Nanog-rep2 has a comparably higher NSC value which might explain the increased number of peaks for that sample compared to the others. The RSC and quality tags further indicate good chip signal and a quality IP, yielding a very high quality tag. Based on these metrics, the samples look good for further analysis.\n\n\nCross-correlation plots\n\n\nThe cross-correlation plots show the best estimate for strand shift and the cross-correlation values. This file can be viewed by transferring it to your local machine using FileZilla. Copy \nH1hesc_Nanog_Rep1_chr12_aln.pdf\n to your machine to view the strand shift. The cross correlation peak shows the highest cross-correlation at fragment length 115. \nHow does this compare to the one we generated using MACS?\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Assessing ChIP quality using cross correlation"
        }, 
        {
            "location": "/06_QC_cross_correlation/#learning-objectives", 
            "text": "Discuss sources of low quality ChIP-seq data  Understand strand cross-correlation  Use  phantompeakqualtools  to compute cross-correlation and associated QC metrics  Evaluate and interpret QC metrics and the cross-correlation plot", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/06_QC_cross_correlation/#chip-seq-quality-assessment", 
            "text": "Prior to performing any downstream analyses with the results from a peak caller, it is best practice to assess the quality of your ChIP-Seq data. What we are looking for is good   quality ChIP-seq enrichment over background.", 
            "title": "ChIP-Seq quality assessment"
        }, 
        {
            "location": "/06_QC_cross_correlation/#strand-cross-correlation", 
            "text": "A very useful ChIP-seq quality metric that is independent of peak calling is strand cross-correlation. It is based on the fact that a high-quality ChIP-seq experiment will produce significant clustering of enriched DNA sequence tags at locations bound by the protein of interest, that present as a bimodal enrichment of reads on the forward and reverse strands.  The bimodal enrichment of reads is due to the following:  During the ChIP-seq experiment, the DNA is fragmented and the protein-bound fragments are immunoprecipitated. This generates DNA fragments containing the protein-bound region.   The + strand of DNA is sequenced from the 5' end, generating the red reads in the figure below, and the - strand of DNA is sequenced from the 5' end, generating the blue reads in the figure below.    Nat Biotechnol. 2008 Dec; 26(12): 1351\u20131359  Due to the sequencing of the 5' ends of the fragments, this results in an enrichment of reads from the + strand (blue in the image below) being slightly offset from the enrichment of reads from the - strand (red in the image below). We need to  determine the number of bases to shift the peaks to yield maximum correlation between the two peaks , which  should  correspond to the predominant  fragment length . We can calculate the shift yielding the maximum correlation using the  cross-correlation metric .", 
            "title": "Strand cross-correlation"
        }, 
        {
            "location": "/06_QC_cross_correlation/#cross-correlation-metric", 
            "text": "The cross-correlation metric is computed as the  Pearson's linear correlation between the Crick strand and the Watson strand, after shifting Watson by k base pairs.  Using a small genomic window as an example, let's walk through the details of the cross-correlation below.  At strand shift of zero, the Pearson correlation between the two vectors is 0.539.   At strand shift of 5bp, the Pearson correlation between the two vectors is 0.931   Keep shifting the vectors and for each strand shift compute a correlation value.     In the end, we will have a table of values mapping each base pair shift to a Pearson correlation value. This is computed for every peak for each chromosome and values are multiplied by a scaling factor and then summed across all chromosomes. We can then  plot cross-correlation values (y-axis) against the shift value (x-axis)  to generate a cross-correlation plot.  The cross-correlation plot  typically produces two peaks : a peak of enrichment corresponding to the predominant  fragment length  (highest correlation value) and a peak corresponding to the  read length  (\u201cphantom\u201d peak).  High-quality ChIP-seq data sets tend to have a larger fragment-length peak compared with the read-length peak. An example of a  strong signal  is shown below using data from  CTCF (zinc-finger transcription factor)  in human cells. With a good antibody, transcription factors will typically result in 45,000 - 60,000 peaks. The red vertical line shows the dominant peak at the true peak shift, with a small bump at the blue vertical line representing the read length.     An example of  weaker signal  is demonstrated below with a  Pol2  data. Here, this particular antibody is not very efficient and these are broad scattered peaks. We observe two peaks in the cross-correlation profile: one at the true peak shift (~185-200 bp) and the other at read length. For weak signal datasets, the  read-length peak will start to dominate .   A failed experiment will resemble a cross-correlation plot using  input only , in which we observe little or no peak for fragment length. Note in the example below the  strongest peak is the blue line (read length)  and there is basically no other significant peak in the profile. The absence of a peak is expected since there should be no significant clustering of fragments around specific target sites (except potentially weak biases in open chromatin regions depending on the protocol used). The read-length peak occurs due to unique mappability properties of the mapped reads.", 
            "title": "Cross-correlation metric"
        }, 
        {
            "location": "/06_QC_cross_correlation/#cross-correlation-quality-metrics", 
            "text": "Using the cross-correlation plot we can  compute metrics for assessing signal-to-noise ratios in a ChIP-seq experiment  and to ensure the fragment length is accurate based on the experimental design. Poor signal-to-noise and inaccurate fragment lengths can indicate problems with the ChIP-Seq data. These metrics are described in more detail below:", 
            "title": "Cross-correlation quality metrics"
        }, 
        {
            "location": "/06_QC_cross_correlation/#normalized-strand-cross-correlation-coefficent-nsc", 
            "text": "The ratio of the maximal cross-correlation value divided by the background cross-correlation (minimum cross-correlation value over all possible strand shifts).  Higher values indicate more enrichment, values less than 1.1 are relatively low NSC scores, and the minimum possible value is 1 (no enrichment).  Datasets with NSC values much less than 1.05 tend to have low signal to noise or few peaks (this could be biological, such as a factor that truly binds only a few sites in a particular tissue type or it could be due to poor quality).", 
            "title": "Normalized strand cross-correlation coefficent (NSC):"
        }, 
        {
            "location": "/06_QC_cross_correlation/#relative-strand-cross-correlation-coefficient-rsc", 
            "text": "The ratio of the fragment-length cross-correlation value minus the background cross-correlation value, divided by the phantom-peak cross-correlation value minus the background cross-correlation value.  The minimum possible value is 0 (no signal), highly enriched experiments have values greater than 1, and values much less than 1 may indicate low quality.  RSC values significantly low (  0.8) tend to have low signal to noise and can be due to failed and poor quality ChIP, low read sequence quality and hence lots of mismappings, shallow sequencing depth or a combination of these. Like the NSC, datasets with few binding sites (  200) which are biologically justifiable also show low RSC scores.", 
            "title": "Relative strand cross-correlation coefficient (RSC):"
        }, 
        {
            "location": "/06_QC_cross_correlation/#phantompeakqualtools", 
            "text": "The  phantompeakqualtools  package is a tool used to compute enrichment and quality measures for ChIP-Seq data [ 1 ]. We will be using the package to compute the predominant insert-size (fragment length) based on strand cross-correlation peak and data quality measures based on relative phantom peak.", 
            "title": "phantompeakqualtools"
        }, 
        {
            "location": "/06_QC_cross_correlation/#set-up", 
            "text": "The  phantompeakqualtools  package is written as an R script, that uses  samtools  as a dependency. The package has various options that need to be specified when running from the command line. To get set up, we will need to start an interactive session, load the necessary modules and set up the directory structure:  $ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash    \n\n$ module load gcc/6.2.0 R/3.4.1 samtools/1.3.1\n\n$ cd ~/chipseq/results\n\n$ mkdir chip_qc\n\n$ cd chip_qc  We have downloaded this for you, and have a copy you can use.   The directory contains several files.  $ ls -l /n/groups/hbctraining/chip-seq/phantompeakqualtools/   NOTE:   You can download the  phantompeakqualtools  package, directly from  GitHub , if you wanted your own local version. This repo is actively maintained by the developer Anshul Kundaje.   In this folder there should be a  README.txt  which contains all the commands, options, and output descriptions. Let's check out the  README.txt :  $ less /n/groups/hbctraining/chip-seq/phantompeakqualtools/README.md  Note that there are two R scripts that are described in the README file. Both will compute the fragment length, and data quality characteristics based on cross-correlation analysis, but one is for use in situations where the duplicates have been removed ( run_spp_nodups.R ).  This is the script we will be using.", 
            "title": "Set up"
        }, 
        {
            "location": "/06_QC_cross_correlation/#using-r-libraries", 
            "text": "In the README you will have noticed an  INSTALLATION  section. We will need to install the R package,  spp  and  caTools , into our personal R library to run the script. Since this is a bit more involved, in the interest of time we have created the libraries and shared them for you to use. To use our libraries, you will need to setup an environmental variable called  R_LIBS_USER  and point it to the location on O2 where our libraries reside:  $  export R_LIBS_USER= /n/groups/hbctraining/R/library/    NOTE: Testing libraries  If you want to check and see that this is working, you can open up R by typing R and pressing Enter:   $ R   And then once in R, try loading the libraries:   R version 3.4.1 (2017-06-30) --  Single Candle \nCopyright (C) 2017 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.  library(spp)  library(caTools)   To exit, type:  q()  in the console.", 
            "title": "Using R libraries"
        }, 
        {
            "location": "/06_QC_cross_correlation/#running-phantompeakqualtools", 
            "text": "To obtain quality measures based on cross-correlation plots, we will be running the  run_spp_nodups.R  script from the command line which is a package built on SPP. This modified SPP package allows for determination of the cross-correlation peak and predominant fragment length in addition to peak calling. We will be using this package solely for obtaining these quality measures (no peak calling).   The options that we will be using include:   -c : full path and name (or URL) of tagAlign/BAM file  -savp : save cross-correlation plot  -out : will create and/or append to a file several important characteristics of the dataset described in more detail below.   ## DO NOT RUN THIS\n## THIS SCRIPT IS FOR COMPUTING METRICS ON A SINGLE FILE\n$ Rscript /n/groups/hbctraining/chip-seq/phantompeakqualtools/run_spp.R -c= tagAlign/BAMfile  -savp -out= outFile    NOTE:  Even though the script is called  run_spp.R , we aren't actually performing peak calling with SPP.   From within the  phantompeakqualtools  directory, we will create output directories and use a 'for loop' to  run the script on every Nanog and Pouf51 BAM file :  $ mkdir -p logs qual\n\n$ for bam in ../bowtie2/*Nanog*aln.bam ../bowtie2/*Pou5f1*aln.bam\ndo \nbam2=`basename $bam _aln.bam`\nRscript /n/groups/hbctraining/chip-seq/phantompeakqualtools/run_spp.R -c=$bam -savp -out=qual/${bam2}.qual   logs/${bam2}.Rout\ndone  The for loop generates  three output files . The  quality metrics  are written in a tab-delimited text file, and the  log files  contains the standard output text. A third file is created in the same directory as the BAM files. These are pdf files that contain the  cross-correlation  plot for each sample. Let's move those files into the appropriate output directory:  $ mv ../bowtie2/*pdf qual    To visualize the quality metrics (.qual) files more easily, we will concatenate the files together to create a single summary file that you can move over locally and open up with Excel.  $ cat qual/*qual   qual/phantompeaks_summary.xls  Let's use Filezilla or  scp  to move the summary file over to our local machine for viewing. Open up the file in Excel and take a look at our NSC and RSC values.", 
            "title": "Running phantompeakqualtools"
        }, 
        {
            "location": "/06_QC_cross_correlation/#phantompeakqualtools-quality-metrics-output", 
            "text": "The qual files are tab-delimited with the columns containing the following information:   COL1: Filename: tagAlign/BAM filename   COL2: numReads: effective sequencing depth (i.e. total number of mapped reads in input file)  COL3: estFragLen: comma separated strand cross-correlation peak(s) in decreasing order of correlation. ( NOTE:  The top 3 local maxima locations that are within 90% of the maximum cross-correlation value are output. In almost all cases, the top (first) value in the list represents the predominant fragment length.)   COL4: corr_estFragLen: comma separated strand cross-correlation value(s) in decreasing order (col2 follows the same order)   COL5: phantomPeak: Read length/phantom peak strand shift   COL6: corr_phantomPeak: Correlation value at phantom peak   COL7: argmin_corr: strand shift at which cross-correlation is lowest   COL8: min_corr: minimum value of cross-correlation   COL9: Normalized strand cross-correlation coefficient (NSC) = COL4 / COL8   COL10: Relative strand cross-correlation coefficient (RSC) = (COL4 - COL8) / (COL6 - COL8)   COL11: QualityTag: Quality tag based on thresholded RSC (codes: -2:veryLow,-1:Low,0:Medium,1:High,2:veryHigh)    NOTE:  The most important metrics we are interested in are the values in columns 9 through 11, however these numbers are computed from values in the other columns.   How do the values compare to the thresholds mentioned above?  All samples have quite high NSC values indicating more enrichment, a good signal to noise and a fair number of peaks. Nanog-rep2 has a comparably higher NSC value which might explain the increased number of peaks for that sample compared to the others. The RSC and quality tags further indicate good chip signal and a quality IP, yielding a very high quality tag. Based on these metrics, the samples look good for further analysis.", 
            "title": "phantompeakqualtools: quality metrics output"
        }, 
        {
            "location": "/06_QC_cross_correlation/#cross-correlation-plots", 
            "text": "The cross-correlation plots show the best estimate for strand shift and the cross-correlation values. This file can be viewed by transferring it to your local machine using FileZilla. Copy  H1hesc_Nanog_Rep1_chr12_aln.pdf  to your machine to view the strand shift. The cross correlation peak shows the highest cross-correlation at fragment length 115.  How does this compare to the one we generated using MACS?    This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Cross-correlation plots"
        }, 
        {
            "location": "/07_QC_quality_metrics/", 
            "text": "Contributors: Mary Piper and Meeta Mistry\n\n\nApproximate time: 60 minutes\n\n\nLearning Objectives\n\n\n\n\nDiscuss other quality metrics for evaluating ChIP-Seq data\n\n\nUnderstand the steps required to generate a QC report using the R Bioonductor package \nChIPQC\n\n\nInterpretation of a report containing quality metrics and associated figures\n\n\nLearn to run R scripts from the command-line\n\n\nIdentify sources of low quality data\n\n\n\n\nAdditional Quality Metrics for ChIP-seq data\n\n\n\n\nThe \nENCODE consortium\n analyzes the quality of the data produced using a variety of metrics. We have already discussed metrics related to strand cross-correlation such as NSC and RSC. In this section, we will provide descriptions of additional metrics that \nassess the distribution of signal within enriched regions, within/across expected annotations, across the whole genome, and within known artefact regions.\n\n\n\n\nNOTE\n: For some of the metrics we give examples of what is considered a 'good measure' indicative of good quality data. Keep in mind that passing this threshold does not automatically mean that an experiment is successful and a values that fall below the threshold does not automatically mean failure!\n\n\n\n\nSSD\n\n\nThe SSD score is a measure used to indicate evidence of enrichment. It provides a measure of pileup across the genome and is computed by looking at the \nstandard deviation of signal pile-up along the genome normalised to the total number of reads\n. An enriched sample typically has regions of significant pile-up so a higher SSD is more indicative of better enrichment. SSD scores are dependent on the degree of total genome wide signal pile-up and so are sensitive to regions of high signal found with Blacklisted regions as well as genuine ChIP enrichment. \n\n\nFRiP: Fraction of reads in peaks\n\n\nThis value reports the percentage of reads that overlap within called peaks.  This is another good indication of how \u201denriched\u201d the sample is, or the success of the immunoprecipitation. It can be considered a \n\u201dsignal-to-noise\u201d measure of what proportion of the library consists of fragments from binding sites vs. background reads\n. FRiP values will vary depending on the protein of interest. A typical good quality TF with successful enrichment would exhibit a FRiP around 5% or higher. A good quality PolII would exhibit a FRiP of 30% or higher. There are also known examples of good data with FRiP \n 1% (i.e. RNAPIII).\n\n\nRelative Enrichment of Genomic Intervals (REGI)\n\n\nUsing the genomic regions identified as called peaks, we can obtain \ngenomic annotation to show where reads map in terms of various genomic features\n. We then evaluate the relative enrichment across these regions and make note of how this compares to what we expect for enrichment for our protein of interest.\n\n\nRiBL: Reads overlapping in Blacklisted Regions\n\n\nIt is important to keep track of and filter artifact regions that tend to show \nartificially high signal\n (excessive unstructured anomalous reads mapping). As such the DAC Blacklisted Regions track was generated for the ENCODE modENCODE consortia. The blacklisted regions \ntypically appear uniquely mappable so simple mappability filters do not remove them\n. These regions are often found at specific types of repeats such as centromeres, telomeres and satellite repeats. \n\n\n\n\nThese regions tend to have a very high ratio of multi-mapping to unique mapping reads and high variance in mappability. \nThe signal from blacklisted regions has been shown to contribute to confound peak callers and fragment length estimation.\n The RiBL score then may act as a guide for the level of background signal in a ChIP or input and is found to be correlated with SSD in input samples and the read length cross coverage score in both input and ChIP samples. These regions represent around 0.5% of genome, yet can account for high proportion of total signal (\n 10%).\n\n\n\n\nHow were the 'blacklists compiled?\n These blacklists were empirically derived from large compendia of data using a combination of automated heuristics and manual curation. Blacklists were generated for various species including and genome versions including human, mouse, worm and fly. The lists can be \ndownloaded here.\n. For human, they used 80 open chromatin tracks (DNase and FAIRE datasets) and 12 ChIP-seq input/control tracks spanning ~60 cell lines in total. These blacklists are applicable to functional genomic data based on short-read sequencing (20-100bp reads). These are not directly applicable to RNA-seq or any other transcriptome data types. \n\n\n\n\nChIPQC\n: Quality metrics report\n\n\nChIPQC\n is a Bioconductor package that takes as input BAM files and peak calls to automatically \ncompute a number of quality metrics and generates a ChIPseq\nexperiment quality report\n. \n\n\n\n\nNOTE:\n In order to generate the report on \nO2 you require the X11 system\n, which we are currently not setup to do with the training accounts. If you are interested in learning more about using X11 applications you can \nfind out more on the O2 wiki page\n. \n\n\nWe will not be running ChIPQC in class today, because of the limitation described above.\n We will however, walk you through the script and at the end, we also provide detailed instructions on how to run it if you have our own personal account on O2 set up with X11 mode.\n\n\n\n\n\nBefore running \nChIPQC\n, you will need to \ncreate a samplesheet\n. This is the most time consuming part, as it means collecting quite a bit of information about the samples and files and assembling that into a very specific format. We have created a samplesheet for you, let's copy it over and take a quick look at it:\n\n\n$ mkdir ~/chipseq/results/chip_qc/ChIPQC\n\n$ cp /n/groups/hbctraining/chip-seq/ChIPQC/samplesheet.csv ~/chipseq/results/chip_qc/ChIPQC\n\n$ less ~/chipseq/results/chip_qc/ChIPQC/samplesheet.csv\n\n\n\n\nThe \nsample sheet\n contains metadata information for our dataset. Each row represents a peak set (which in most cases is every ChIP sample) and several columns of required information, which allows us to easily load the associated data in one single command. \n\n\n\n\nNOTE: The column headers have specific names that are expected by ChIPQC!!\n. \n\n\n\n\n\n\nSampleID\n: Identifier string for sample\n\n\nTissue, Factor, Condition\n: Identifier strings for up to three different factors (You will need to have all columns listed. If you don't have infomation, then set values to NA)\n\n\nReplicate\n: Replicate number of sample\n\n\nbamReads\n: file path for BAM file containing aligned reads for ChIP sample\n\n\nControlID\n: an identifier string for the control sample\n\n\nbamControl\n: file path for bam file containing aligned reads for control sample\n\n\nPeaks\n: path for file containing peaks for sample\n\n\nPeakCaller\n: Identifier string for peak caller used. Possible values include \u201craw\u201d, \u201cbed\u201d, \u201cnarrow\u201d, \u201cmacs\u201d\n\n\n\n\nR script\n\n\nWe are going to \nwalk through an R script which contains the lines of code required to generate the report.\n There are very few lines of code and so we will briefly explain what each line is doing, so the script is not a complete black box. \n\n\nLet's start with a \nshebang line\n. Note that this is different from that which we used for our bash shell scripts. \n\n\n#!/usr/bin/env Rscript\n\n\n\n\nNext we will load the \nChIPQC\n library so we have access to all the package functions and then load the samplesheet into R. \n\n\n## Load libraries\nlibrary(ChIPQC)\n\n## Load sample data\nsamples \n- read.csv('~/chipseq/results/chip_qc/ChIPQC/samplesheet.csv')\n\n\n\n\nChIPQC\n will use the samplesheet to read in the data for each sample (BAM files and narrowPeak files) and compute quality metrics. The results will be stored into an R object   called\nchipObj\n. \n\n\n## Create ChIPQC object\nchipObj \n- ChIPQC(samples, annotation=\nhg19\n) \n\n\n\n\nThe next line of code will export the \nchipObj\n from the R environment to an actual physical R data file. This file can be loaded into R at a later time (on your local computer or on the cluster) and you will have access to all of the metrics that were computed and stored in there. \n\n\n## Save the chipObj to file\nsave(chipObj, file=\n~/chipseq/results/chip_qc/ChIPQC/chipObj.RData\n)\n\n\n\n\nThe final step is taking those quality metrics and summarizing the information into an HTML report with tables and figures.\n\n\n## Create ChIPQC report\nChIPQCreport(chipObj, reportName=\nChIP QC report: Nanog and Pou5f1\n, reportFolder=\n~/chipseq/results/chip_qc/ChIPQC/ChIPQCreport\n)\n\n\n\n\nYour final script will look like this:\n\n\n#!/usr/bin/env Rscript\n\n## Load libraries\nlibrary(ChIPQC)\n\n## Load sample data\nsamples \n- read.csv('~/chipseq/results/chip_qc/ChIPQC/samplesheet.csv')\n\n## Create ChIPQC object\nchipObj \n- ChIPQC(samples, annotation=\nhg19\n)\n\n## Save the chipObj to file\nsave(chipObj, file=\n~/chipseq/results/chip_qc/ChIPQC/chipObj.RData\n)\n\n## Create ChIPQC report\nChIPQCreport(chipObj, reportName=\nNanog_and_Pou5f1\n, reportFolder=\n~/chipseq/results/chip_qc/ChIPQC/ChIPQCreport\n)\n\n\n\n\nBefore you run this scriot you will \nneed to make sure each of your BAM files has an index (\n.bai\n file)\n. Since we are not running this script, it is not neccessary for us to create the indices. However, we will show you how to do this later in the \nvisualization lesson\n.\nYou can run the script interactively using the \nRscript\n command in the terminal. It will take 2-3 minutes to run to completion and you will see a bunch of text written to the screen as each line of code is run. \n\n\n$ Rscript run_chipQC.R 2\n ../logs/ChIPQC.Rout ##DO NOT RUN THIS\n\n\n\n\n\n\nNOTE:\n Sometimes the information printed to screen is useful log information. If you wanted to capture all of the verbosity into a file you can run the script using \n2\n and specify a file name. \n\n\n\n\nWhen the script is finished running you can check the \nresults/chip_qc/ChlPQC\n directory to make sure you have the \n.RData\n file and an additional folder which contains the HTML report and all associated images\n.\n\n\n\n\n\n\nInterested in running this script on your own?\n\n\nIf you wanted to run this script on your own time,\n we have some instructions on how to set yourself up. \n\n\n\n\nFirst, you will need X11 capabilities (contact HMSRC about this if you need help) \n\n\nCreate a copy of the script to run, and make sure the paths reflect your directory structure.\n\n\n\n\n$ cp  /n/groups/hbctraining/chip-seq/ChIPQC/run_chipQC.R ~/chipseq/scripts\n\n\n\n\nUse pre-existing R libraries. Since installing packages can sometimes be problematic on the cluster, you may just want to use the libraries we have created. To do so, follow the instructions below.\n\n\n\n\n```bash\n\n\ncheck if the variable is already set\n\n\n$ echo $R_LIBS_USER \n\n\nIf the above command returns nothing, then run the command below\n\n\n$ export R_LIBS_USER=\"/n/groups/hbctraining/R/library/\"\n\nbash\n\n\ncheck if the R/3.4.1 module is loaded\n\n\n$ module list\n\n\nif R/3.4.1 is not listed in the output of the above command, then load it\n\n\n$ module load gcc/6.2.0 R/3.4.1\n```\n4. Run the script from the command line.\n\n\n\n\n\n\n\nChIPQC\n report\n\n\nThe report generated using our toy dataset will not give us meaningful plots or metrics and so \nwe have generated a report using the full dataset for you to look at instead.\n Download \nthis zip archive\n. Decompress it and you should find an html file in the resulting directory. Double click it and it will open in your browser. At the top left you should see a button labeled 'Expand All', click on that to expand all sections.\n\n\nLet's start with the \nQC summary table\n:\n\n\n\n\nHere, we see the metrics mentioned above (SSD, RiP and RiBL). A higher SSD is more indicative of better enrichment. SSD scores are dependent on the degree of total genome wide signal pile-up and so are sensitive to regions of high signal found with Blacklisted regions as well as genuine ChIP enrichment. The RiBL percentages are not incredibly high (also shown in the plot in the next section) and RiP percentages are around 5% or higher, except for Pou5f1-rep2. \n\n\nAdditionally, we see other statistics related to the strand cross-correlation (FragLength and RelCC which is similar to the RSC values we discussed earlier). The RelCC values are larger than 1 for all ChIP samples which suggests good enrichment.\n\n\nThe next table contains \nthe mapping quality and duplication rate,\n however since we already filtered our BAM files these numbers do not give us any relevant information. Next is a plot showing the effect of blacklisting (not present here), which indicates that a large proportion of reads in our data do not overlap with blacklisted regions. \n\n\nThe final plot in this section uses the genomic annotation to show \nwhere reads map in terms of genomic features\n. This is represented as a heatmap showing the enrichment of reads compared to the background levels of the feature. We find that there is the most enrichment in promotor regions. This plot is useful when you expect enrichment of specific genomic regions.  \n\n\n\n\nThe next section, \nChIP Signal Distribution and Structure\n, looks at the inherent \"peakiness\" of the samples. The first plot is a \ncoverage histogram\n. The x-axis represents the read pileup height at a basepair position, and the y-axis represents how many positions have this pileup height. This is on a log scale. \nA ChIP sample with good enrichment should have a reasonable \"tail\", that is more positions (higher values on the y-axis) having higher sequencing depth\n. \nSamples with low enrichment (i.e input), consisting of mostly background reads, will have lower genome wide pile-up. In our dataset, the Nanog samples have quite heavy tails compared to Pou5f1, especially replicate 2. The SSD scores, however, are higher for Pou5f1. When SSD is high but coverage looks low it is possibly due to the presence of large regions of high depth and a flag for blacklisting of genomic regions. The cross-correlation plot which is displayed next is one we have already covered. \n\n\n\n\nThe final set of plots, \nPeak Profile and ChIP Enrichment\n, are based on the metric computed using the supplied peaks, if available. The first plot shows average peak profiles, centered on the summit (point of highest pileup) for each peak.\n\n\n\n\nThe \nshape of these profiles can vary depending on what type of mark is being studied\n \u2013 transcription factor, histone mark, or other DNA-binding protein such as a polymerase \u2013 but similar marks usually have a distinctive profile in successful ChIPs. \n\n\nNext we have two plots that summarize the number of \nReads in Peaks\n. ChIP samples with good enrichment will have a higher proportion of their reads overlapping called peaks. In our data we see that within each group there is one replicate which presents as having stronger signal (i.e. more reads in peaks). A direct comparison of the strongest replicate for Nanog and Pou5f1 in the barplot suggests that Nanog has better enrichment. The boxplot gives us a better overview of the distribution of values across all peaks, and we can see that for Pou5f1 the counts are low and it is the outlier peaks that are driving the percentages higher.\n\n\n\n\n\n\nFinally, the report also contains plots to show \nhow the samples are clustered\n. The \ncorrelation heatmap\n is based on correlation values for all the peak scores for each sample. The \nPCA plot\n showcases the first two principal component values for each sample. In our dataset, the replicates for Pou5f1 cluster together but we do not see the same for Nanog.\n\n\n\n\n\n\nIn general, this is not the best dataset. There is some discordance apparent between the Nanog replicates and there are plots that suggest that the signal is much stronger in Nanog than in Pou5f1. Keeping this in mind we can move forward, and revisit some of these issues when we get to data visualization.\n\n\nExperimental biases: sources of low quality ChIP-seq data\n\n\nOnce you have identified low quality samples, the next logical step is to troubleshoot what might be causing it.\n\n\n\n\nStrength/efficiency and specificity of the immunoprecipitation\n \n\n\n\n\nThe quality of a ChIP experiment is ultimately dictated by the specificity of the antibody and the degree of enrichment achieved in the affinity precipitation step \n[1]\n. Antibody deficiencies are of two main types:\n\n\n\n\nPoor reactivity against the intended target\n\n\nNon-specific antibody, causing cross-reactivity with other DNA-associated proteins\n\n\n\n\nAntibodies directed against transcription factors must be characterized using both a primary (i.e immunoblot, immunofluorescence) and secondary characterization (i.e knockout of target protein, IP with multiple antibodies).\n\n\n\n\nFragmentation/digestion\n\n\n\n\nThe way in which sonication is carried out can result in different fragment size distributions and, consequently, sample-specific chromatin configuration induced biases. As a result, it is not recommended to use a single input sample as a control for ChIP-seq peak calling if it is not sonicated together with the ChIP sample. \n\n\n\n\nBiases during library preparation:\n \n\n\n\n\nPCR amplification:\n Biases arise because DNA sequence content and length determine the kinetics of annealing and denaturing in each cycle of PCR. The combination of temperature profile, polymerase and buffer used during PCR can therefore lead to differential efficiencies in amplification between different sequences, which could be exacerbated with increasing PCR cycles. This is often manifest as a bias towards GC rich fragments \n[2]\n. \nLimited use of PCR amplification is recommended because bias increases with every PCR cycle.\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n\n\nDetails on ChIPQC plots was taken from the \nChIPQC vignette\n, which provides a walkthrough with examples and thorough explanations.", 
            "title": "Assessing Peak calls and ChIP quality using ChIPQC"
        }, 
        {
            "location": "/07_QC_quality_metrics/#learning-objectives", 
            "text": "Discuss other quality metrics for evaluating ChIP-Seq data  Understand the steps required to generate a QC report using the R Bioonductor package  ChIPQC  Interpretation of a report containing quality metrics and associated figures  Learn to run R scripts from the command-line  Identify sources of low quality data", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/07_QC_quality_metrics/#additional-quality-metrics-for-chip-seq-data", 
            "text": "The  ENCODE consortium  analyzes the quality of the data produced using a variety of metrics. We have already discussed metrics related to strand cross-correlation such as NSC and RSC. In this section, we will provide descriptions of additional metrics that  assess the distribution of signal within enriched regions, within/across expected annotations, across the whole genome, and within known artefact regions.   NOTE : For some of the metrics we give examples of what is considered a 'good measure' indicative of good quality data. Keep in mind that passing this threshold does not automatically mean that an experiment is successful and a values that fall below the threshold does not automatically mean failure!", 
            "title": "Additional Quality Metrics for ChIP-seq data"
        }, 
        {
            "location": "/07_QC_quality_metrics/#ssd", 
            "text": "The SSD score is a measure used to indicate evidence of enrichment. It provides a measure of pileup across the genome and is computed by looking at the  standard deviation of signal pile-up along the genome normalised to the total number of reads . An enriched sample typically has regions of significant pile-up so a higher SSD is more indicative of better enrichment. SSD scores are dependent on the degree of total genome wide signal pile-up and so are sensitive to regions of high signal found with Blacklisted regions as well as genuine ChIP enrichment.", 
            "title": "SSD"
        }, 
        {
            "location": "/07_QC_quality_metrics/#frip-fraction-of-reads-in-peaks", 
            "text": "This value reports the percentage of reads that overlap within called peaks.  This is another good indication of how \u201denriched\u201d the sample is, or the success of the immunoprecipitation. It can be considered a  \u201dsignal-to-noise\u201d measure of what proportion of the library consists of fragments from binding sites vs. background reads . FRiP values will vary depending on the protein of interest. A typical good quality TF with successful enrichment would exhibit a FRiP around 5% or higher. A good quality PolII would exhibit a FRiP of 30% or higher. There are also known examples of good data with FRiP   1% (i.e. RNAPIII).", 
            "title": "FRiP: Fraction of reads in peaks"
        }, 
        {
            "location": "/07_QC_quality_metrics/#relative-enrichment-of-genomic-intervals-regi", 
            "text": "Using the genomic regions identified as called peaks, we can obtain  genomic annotation to show where reads map in terms of various genomic features . We then evaluate the relative enrichment across these regions and make note of how this compares to what we expect for enrichment for our protein of interest.", 
            "title": "Relative Enrichment of Genomic Intervals (REGI)"
        }, 
        {
            "location": "/07_QC_quality_metrics/#ribl-reads-overlapping-in-blacklisted-regions", 
            "text": "It is important to keep track of and filter artifact regions that tend to show  artificially high signal  (excessive unstructured anomalous reads mapping). As such the DAC Blacklisted Regions track was generated for the ENCODE modENCODE consortia. The blacklisted regions  typically appear uniquely mappable so simple mappability filters do not remove them . These regions are often found at specific types of repeats such as centromeres, telomeres and satellite repeats.    These regions tend to have a very high ratio of multi-mapping to unique mapping reads and high variance in mappability.  The signal from blacklisted regions has been shown to contribute to confound peak callers and fragment length estimation.  The RiBL score then may act as a guide for the level of background signal in a ChIP or input and is found to be correlated with SSD in input samples and the read length cross coverage score in both input and ChIP samples. These regions represent around 0.5% of genome, yet can account for high proportion of total signal (  10%).   How were the 'blacklists compiled?  These blacklists were empirically derived from large compendia of data using a combination of automated heuristics and manual curation. Blacklists were generated for various species including and genome versions including human, mouse, worm and fly. The lists can be  downloaded here. . For human, they used 80 open chromatin tracks (DNase and FAIRE datasets) and 12 ChIP-seq input/control tracks spanning ~60 cell lines in total. These blacklists are applicable to functional genomic data based on short-read sequencing (20-100bp reads). These are not directly applicable to RNA-seq or any other transcriptome data types.", 
            "title": "RiBL: Reads overlapping in Blacklisted Regions"
        }, 
        {
            "location": "/07_QC_quality_metrics/#chipqc-quality-metrics-report", 
            "text": "ChIPQC  is a Bioconductor package that takes as input BAM files and peak calls to automatically  compute a number of quality metrics and generates a ChIPseq\nexperiment quality report .    NOTE:  In order to generate the report on  O2 you require the X11 system , which we are currently not setup to do with the training accounts. If you are interested in learning more about using X11 applications you can  find out more on the O2 wiki page .   We will not be running ChIPQC in class today, because of the limitation described above.  We will however, walk you through the script and at the end, we also provide detailed instructions on how to run it if you have our own personal account on O2 set up with X11 mode.   Before running  ChIPQC , you will need to  create a samplesheet . This is the most time consuming part, as it means collecting quite a bit of information about the samples and files and assembling that into a very specific format. We have created a samplesheet for you, let's copy it over and take a quick look at it:  $ mkdir ~/chipseq/results/chip_qc/ChIPQC\n\n$ cp /n/groups/hbctraining/chip-seq/ChIPQC/samplesheet.csv ~/chipseq/results/chip_qc/ChIPQC\n\n$ less ~/chipseq/results/chip_qc/ChIPQC/samplesheet.csv  The  sample sheet  contains metadata information for our dataset. Each row represents a peak set (which in most cases is every ChIP sample) and several columns of required information, which allows us to easily load the associated data in one single command.    NOTE: The column headers have specific names that are expected by ChIPQC!! .     SampleID : Identifier string for sample  Tissue, Factor, Condition : Identifier strings for up to three different factors (You will need to have all columns listed. If you don't have infomation, then set values to NA)  Replicate : Replicate number of sample  bamReads : file path for BAM file containing aligned reads for ChIP sample  ControlID : an identifier string for the control sample  bamControl : file path for bam file containing aligned reads for control sample  Peaks : path for file containing peaks for sample  PeakCaller : Identifier string for peak caller used. Possible values include \u201craw\u201d, \u201cbed\u201d, \u201cnarrow\u201d, \u201cmacs\u201d", 
            "title": "ChIPQC: Quality metrics report"
        }, 
        {
            "location": "/07_QC_quality_metrics/#r-script", 
            "text": "We are going to  walk through an R script which contains the lines of code required to generate the report.  There are very few lines of code and so we will briefly explain what each line is doing, so the script is not a complete black box.   Let's start with a  shebang line . Note that this is different from that which we used for our bash shell scripts.   #!/usr/bin/env Rscript  Next we will load the  ChIPQC  library so we have access to all the package functions and then load the samplesheet into R.   ## Load libraries\nlibrary(ChIPQC)\n\n## Load sample data\nsamples  - read.csv('~/chipseq/results/chip_qc/ChIPQC/samplesheet.csv')  ChIPQC  will use the samplesheet to read in the data for each sample (BAM files and narrowPeak files) and compute quality metrics. The results will be stored into an R object   called chipObj .   ## Create ChIPQC object\nchipObj  - ChIPQC(samples, annotation= hg19 )   The next line of code will export the  chipObj  from the R environment to an actual physical R data file. This file can be loaded into R at a later time (on your local computer or on the cluster) and you will have access to all of the metrics that were computed and stored in there.   ## Save the chipObj to file\nsave(chipObj, file= ~/chipseq/results/chip_qc/ChIPQC/chipObj.RData )  The final step is taking those quality metrics and summarizing the information into an HTML report with tables and figures.  ## Create ChIPQC report\nChIPQCreport(chipObj, reportName= ChIP QC report: Nanog and Pou5f1 , reportFolder= ~/chipseq/results/chip_qc/ChIPQC/ChIPQCreport )  Your final script will look like this:  #!/usr/bin/env Rscript\n\n## Load libraries\nlibrary(ChIPQC)\n\n## Load sample data\nsamples  - read.csv('~/chipseq/results/chip_qc/ChIPQC/samplesheet.csv')\n\n## Create ChIPQC object\nchipObj  - ChIPQC(samples, annotation= hg19 )\n\n## Save the chipObj to file\nsave(chipObj, file= ~/chipseq/results/chip_qc/ChIPQC/chipObj.RData )\n\n## Create ChIPQC report\nChIPQCreport(chipObj, reportName= Nanog_and_Pou5f1 , reportFolder= ~/chipseq/results/chip_qc/ChIPQC/ChIPQCreport )  Before you run this scriot you will  need to make sure each of your BAM files has an index ( .bai  file) . Since we are not running this script, it is not neccessary for us to create the indices. However, we will show you how to do this later in the  visualization lesson .\nYou can run the script interactively using the  Rscript  command in the terminal. It will take 2-3 minutes to run to completion and you will see a bunch of text written to the screen as each line of code is run.   $ Rscript run_chipQC.R 2  ../logs/ChIPQC.Rout ##DO NOT RUN THIS   NOTE:  Sometimes the information printed to screen is useful log information. If you wanted to capture all of the verbosity into a file you can run the script using  2  and specify a file name.    When the script is finished running you can check the  results/chip_qc/ChlPQC  directory to make sure you have the  .RData  file and an additional folder which contains the HTML report and all associated images .", 
            "title": "R script"
        }, 
        {
            "location": "/07_QC_quality_metrics/#interested-in-running-this-script-on-your-own", 
            "text": "If you wanted to run this script on your own time,  we have some instructions on how to set yourself up.    First, you will need X11 capabilities (contact HMSRC about this if you need help)   Create a copy of the script to run, and make sure the paths reflect your directory structure.   $ cp  /n/groups/hbctraining/chip-seq/ChIPQC/run_chipQC.R ~/chipseq/scripts   Use pre-existing R libraries. Since installing packages can sometimes be problematic on the cluster, you may just want to use the libraries we have created. To do so, follow the instructions below.   ```bash", 
            "title": "Interested in running this script on your own?"
        }, 
        {
            "location": "/07_QC_quality_metrics/#check-if-the-variable-is-already-set", 
            "text": "$ echo $R_LIBS_USER", 
            "title": "check if the variable is already set"
        }, 
        {
            "location": "/07_QC_quality_metrics/#if-the-above-command-returns-nothing-then-run-the-command-below", 
            "text": "$ export R_LIBS_USER=\"/n/groups/hbctraining/R/library/\" bash", 
            "title": "If the above command returns nothing, then run the command below"
        }, 
        {
            "location": "/07_QC_quality_metrics/#check-if-the-r341-module-is-loaded", 
            "text": "$ module list", 
            "title": "check if the R/3.4.1 module is loaded"
        }, 
        {
            "location": "/07_QC_quality_metrics/#if-r341-is-not-listed-in-the-output-of-the-above-command-then-load-it", 
            "text": "$ module load gcc/6.2.0 R/3.4.1\n```\n4. Run the script from the command line.", 
            "title": "if R/3.4.1 is not listed in the output of the above command, then load it"
        }, 
        {
            "location": "/07_QC_quality_metrics/#chipqc-report", 
            "text": "The report generated using our toy dataset will not give us meaningful plots or metrics and so  we have generated a report using the full dataset for you to look at instead.  Download  this zip archive . Decompress it and you should find an html file in the resulting directory. Double click it and it will open in your browser. At the top left you should see a button labeled 'Expand All', click on that to expand all sections.  Let's start with the  QC summary table :   Here, we see the metrics mentioned above (SSD, RiP and RiBL). A higher SSD is more indicative of better enrichment. SSD scores are dependent on the degree of total genome wide signal pile-up and so are sensitive to regions of high signal found with Blacklisted regions as well as genuine ChIP enrichment. The RiBL percentages are not incredibly high (also shown in the plot in the next section) and RiP percentages are around 5% or higher, except for Pou5f1-rep2.   Additionally, we see other statistics related to the strand cross-correlation (FragLength and RelCC which is similar to the RSC values we discussed earlier). The RelCC values are larger than 1 for all ChIP samples which suggests good enrichment.  The next table contains  the mapping quality and duplication rate,  however since we already filtered our BAM files these numbers do not give us any relevant information. Next is a plot showing the effect of blacklisting (not present here), which indicates that a large proportion of reads in our data do not overlap with blacklisted regions.   The final plot in this section uses the genomic annotation to show  where reads map in terms of genomic features . This is represented as a heatmap showing the enrichment of reads compared to the background levels of the feature. We find that there is the most enrichment in promotor regions. This plot is useful when you expect enrichment of specific genomic regions.     The next section,  ChIP Signal Distribution and Structure , looks at the inherent \"peakiness\" of the samples. The first plot is a  coverage histogram . The x-axis represents the read pileup height at a basepair position, and the y-axis represents how many positions have this pileup height. This is on a log scale.  A ChIP sample with good enrichment should have a reasonable \"tail\", that is more positions (higher values on the y-axis) having higher sequencing depth . \nSamples with low enrichment (i.e input), consisting of mostly background reads, will have lower genome wide pile-up. In our dataset, the Nanog samples have quite heavy tails compared to Pou5f1, especially replicate 2. The SSD scores, however, are higher for Pou5f1. When SSD is high but coverage looks low it is possibly due to the presence of large regions of high depth and a flag for blacklisting of genomic regions. The cross-correlation plot which is displayed next is one we have already covered.    The final set of plots,  Peak Profile and ChIP Enrichment , are based on the metric computed using the supplied peaks, if available. The first plot shows average peak profiles, centered on the summit (point of highest pileup) for each peak.   The  shape of these profiles can vary depending on what type of mark is being studied  \u2013 transcription factor, histone mark, or other DNA-binding protein such as a polymerase \u2013 but similar marks usually have a distinctive profile in successful ChIPs.   Next we have two plots that summarize the number of  Reads in Peaks . ChIP samples with good enrichment will have a higher proportion of their reads overlapping called peaks. In our data we see that within each group there is one replicate which presents as having stronger signal (i.e. more reads in peaks). A direct comparison of the strongest replicate for Nanog and Pou5f1 in the barplot suggests that Nanog has better enrichment. The boxplot gives us a better overview of the distribution of values across all peaks, and we can see that for Pou5f1 the counts are low and it is the outlier peaks that are driving the percentages higher.    Finally, the report also contains plots to show  how the samples are clustered . The  correlation heatmap  is based on correlation values for all the peak scores for each sample. The  PCA plot  showcases the first two principal component values for each sample. In our dataset, the replicates for Pou5f1 cluster together but we do not see the same for Nanog.    In general, this is not the best dataset. There is some discordance apparent between the Nanog replicates and there are plots that suggest that the signal is much stronger in Nanog than in Pou5f1. Keeping this in mind we can move forward, and revisit some of these issues when we get to data visualization.", 
            "title": "ChIPQC report"
        }, 
        {
            "location": "/07_QC_quality_metrics/#experimental-biases-sources-of-low-quality-chip-seq-data", 
            "text": "Once you have identified low quality samples, the next logical step is to troubleshoot what might be causing it.   Strength/efficiency and specificity of the immunoprecipitation     The quality of a ChIP experiment is ultimately dictated by the specificity of the antibody and the degree of enrichment achieved in the affinity precipitation step  [1] . Antibody deficiencies are of two main types:   Poor reactivity against the intended target  Non-specific antibody, causing cross-reactivity with other DNA-associated proteins   Antibodies directed against transcription factors must be characterized using both a primary (i.e immunoblot, immunofluorescence) and secondary characterization (i.e knockout of target protein, IP with multiple antibodies).   Fragmentation/digestion   The way in which sonication is carried out can result in different fragment size distributions and, consequently, sample-specific chromatin configuration induced biases. As a result, it is not recommended to use a single input sample as a control for ChIP-seq peak calling if it is not sonicated together with the ChIP sample.    Biases during library preparation:     PCR amplification:  Biases arise because DNA sequence content and length determine the kinetics of annealing and denaturing in each cycle of PCR. The combination of temperature profile, polymerase and buffer used during PCR can therefore lead to differential efficiencies in amplification between different sequences, which could be exacerbated with increasing PCR cycles. This is often manifest as a bias towards GC rich fragments  [2] .  Limited use of PCR amplification is recommended because bias increases with every PCR cycle.   This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.  Details on ChIPQC plots was taken from the  ChIPQC vignette , which provides a walkthrough with examples and thorough explanations.", 
            "title": "Experimental biases: sources of low quality ChIP-seq data"
        }, 
        {
            "location": "/08_handling-replicates/", 
            "text": "Contributors: Meeta Mistry, Radhika Khetani \n\n\nApproximate time: 75 minutes\n\n\nLearning Objectives\n\n\n\n\nCombining replicates using simple overlap with Bedtools\n\n\nCombining replicates to only get the highly reproducible peaks using the IDR method\n\n\n\n\nHandling replicates in ChIP-Seq\n\n\nAs with any high-throughput experiment, a single assay is often subject to a substantial amount of variability. Thus, it is highly recommended to setup your experimental design with a minimum of 2-3 biological replicates. Presumably, two replicates measuring the same underlying biology should have high consistency but that is not always the case. In order to evaluate consistency between replicates \nwe require metrics that objectively assess the reproducibility of high-throughput assays\n.\n\n\n\n\nSince we have 2 replicates in this example, we want to consider only those peaks that are present in both replicates before we compare the peaks from the two transcription factors to one another.\n\n\n \n\n\nCommon methods for handling replicates includes taking overlapping peak calls across replicates and then assessing differences in binding regions. Additionally, there are more complex methods that employ statistical testing and evaluate the reproducibility between replicates. In this lesson we will cover both methods.\n\n\n\n\nNOTE: A recent talk on \"Accessing and using ENCODE data\" \nlinked here\n where they talk about handling replicates and the similarities and differences when using an overlap versus IDR analysis.\n\n\n\n\nOverlapping peaks\n\n\nIn this section, our goal is to determine what peaks are in common between the the two replicates for each factor (Nanog and Pou5f1). To perform this task we are going to use a suite of tools called \nbedtools\n.\n\n\nbedtools\n\n\nThe idea is that genome coordinate information can be used to perform relatively simple arithmetic, like combining, subsetting, intersecting, etc., to obtain all sorts of information. \nbedtools\n from \nAaron Quinlan's group\n at University of Utah is easy to use, and an extremely versatile tool that performs tasks of this nature. \n\n\n\n\nAs the name implies, this suite of tools works with \nBed\n files, but it also works with other file formats that have genome coordinate information. \n\n\n\n\n\n\nNOTE:\n When working with multiple files to perform arithmetic on genomic coordinates, it is essential that all files have coordinate information for the same exact version of the genome and the same coordinate system (0-based or 1-based)!\n\n\n\n\nSetting up\n\n\nLet's start an interactive session and change directories and set up a space for the resulting overlaps. \n\n\n$ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash    \n\n$ cd ~/chipseq/results/\n\n$ mkdir bedtools\n\n\n\n\nLoad the modules for \nbedtools\n and \nsamtools\n:\n\n\n\n$ module load gcc/6.2.0 bedtools/2.26.0 samtools/1.3.1\n\n\n\n\nFinding overlapping peaks between replicates\n\n\nThe \nbedtools intersect\n command only reports back the peaks that are overlapping with respect to the file defined as \na\n in the command.\n\n\n\n\nTo find out more information on the parameters available when intersecting, use the help flag:\n\n\n$ bedtools intersect -h\n\n\n\n\nThe intersect tool evaluates A (file 1) and finds regions that overlap in B (file 2). We will add the \n-wo\n which indicates to write the original A (file 1) and B (file 2) entries plus the number of base pairs of overlap between the two features.\n\n\nLet's start with the Nanog replicates: \n\n\n$ bedtools intersect \\\n-a macs2/Nanog-rep1_peaks.narrowPeak \\\n-b macs2/Nanog-rep2_peaks.narrowPeak \\\n-wo \n bedtools/Nanog-overlaps.bed\n\n\n\n\nHow many overlapping peaks did we get?\n\n\nWe'll do the same for the Pou5f1 replicates:\n\n\n$ bedtools intersect \\\n-a macs2/Pou5f1-rep1_peaks.narrowPeak \\\n-b macs2/Pou5f1-rep2_peaks.narrowPeak \\\n-wo \n bedtools/Pou5f1-overlaps.bed\n\n\n\n\nNote that we are working with subsetted data and so our list of peaks for each replicate is small. Thus, the overlapping peak set will be small as we found with both Nanog and Pou5f1. What is interesting though, is that even though the individual peak lists are smaller for Pou5f1 samples, the overlapping replicates represent a higher proportion of overlap with respect to each replicate.\n\n\n\n\nHistorical Note\n:\n A simpler heuristic for establishing reproducibility was previously used as a standard for depositing ENCODE data and was in effect when much of the currently available data was submitted. According to this standard, either 80% of the top 40% of the peaks identified from one replicate using an acceptable scoring method should overlap the list of peaks from the other replicate, OR peak lists scored using all available reads from each replicate should share more than 75% of regions in common. As with the current standards, this was developed based on experience with accumulated ENCODE ChIP-seq data, albeit with a much smaller sample size.\n\n\n\n\nIrreproducibility Discovery Rate (IDR)\n\n\nIDR\n is a framework developed by Qunhua Li and Peter Bickel's group that \ncompares a pair of ranked lists of regions/peaks and assigns values that reflect its reproducibility.\n \n\n\n \n\n\n\"The basic idea is that if two replicates measure the same underlying biology, the most significant peaks, which are likely to be genuine signals, are expected to have high consistency between replicates, whereas peaks with low significance, which are more likely to be noise, are expected to have low consistency. If the consistency between a pair of rank lists (peaks) that contains both significant and insignificant findings is plotted, a transition in consistency is expected (Fig. 1C). This consistency transition provides an internal indicator of the change from signal to noise and suggests how many peaks have been reliably detected.\"\n \nhttps://ccg.vital-it.ch/var/sib_april15/cases/landt12/idr.html\n\n\nIt is extensively used by the ENCODE and modENCODE projects and is part of their \nChIP-seq guidelines and standards\n. It has been established for submission of ChIP-seq data sets and have been constructed based on the historical experiences of ENCODE ChIP-seq data production groups.\n\n\nWhy IDR?\n\n\n\n\nIDR avoids choices of initial cutoffs, which are not comparable for different callers \n\n\nIDR does not depend on arbitrary thresholds and so all regions/peaks are considered. \n\n\nIt is based on ranks, so does not require the input signals to be calibrated or with a specific fixed scale (only order matters).\n\n\n\n\nComponents of IDR\n\n\nThe IDR approach creates a curve, from which it then quantitatively assesses when the \ufb01ndings are no longer consistent across replicates. There are three main components: \n\n\n1) A \ncorrespondence curve\n: a graphical representation of matched peaks as you go down the ranked list. Qualitative, not adequate for selecting signals.\n\n\n \n\n\n2) An \ninference procedure\n: summarizes the proportion of reproducible and irreproducible signals. Quantitative, using a copula mixture model.\n\n\n\n\nWhat proportion of identifications have a poor correspondence, i.e. falling into \u201dnoise\u201d?\nHow consistent are the identifications before reaching breakdown?\n\n\n\n\n3) \nIrreproducible Discovery Rate (IDR)\n: Derive a significance value from the inference procedure (#2) in a fashion similar to FDR, and can be used to control the level of irreproducibility rate when selecting signals.\ni.e. 0.05 IDR means that peak has a 5% chance of being an irreproducible discovery\n\n\nThe IDR pipeline\n\n\nThere are three main steps to the IDR pipeline:\n\n\n\n\nEvaluate peak consistency between \ntrue replicates\n\n\nEvaluate peak consistency between \npooled pseudo-replicates\n\n\nEvaluate \nself-consistency\n for each individual replicate\n\n\n\n\n \n\n\n\n\nThis figure is taken from the \nENCODE ChIP-Seq Guidelines\n.\n\n\n\n\nWe will only be running Step 1 in this lesson, but will discuss steps 2 and 3 in a bit more detail.\n\n\nRunning IDR\n\n\nTo run IDR \nwe should be using the full dataset\n, and if you are interested the full BAM files can be downloaded from ENCODE, however for timeliness and consistency we will continue with our subsetted data for this lesson.\n\n\nTo run IDR the recommendation is to \nrun MACS2 less stringently\n to allow a larger set of peaks to be identified for each replicate. In addition the narrowPeak files have to be \nsorted by the \n-log10(p-value)\n column\n. We have already performed these 2 steps for the samples from both groups, and the commands we used for it as listed below. To reiterate, \nyou do NOT NEED TO RUN the following lines of code, we have already generated narrowPeak files for you!\n\n\n###DO NOT RUN THIS###\n\n# Call peaks using a liberal p-value cutoff\nmacs2 callpeak -t treatFile.bam -c inputFile.bam -f BAM -g 1.3e+8 -n macs/NAME_FOR_OUPUT -B -p 1e-3  2\n macs/NAME_FOR_OUTPUT_macs2.log\n\n#Sort peak by -log10(p-value)\nsort -k8,8nr NAME_OF_INPUT_peaks.narrowPeak \n macs/NAME_FOR_OUPUT_peaks.narrowPeak \n\n\n\n\n\n\nIDR will work with many different Peak callers, the following have been tested:\n\n\n\n\nSPP - Works out of the box\n\n\nMACS1.4 - DO NOT use with IDR\n\n\nMACS2 - Works well with IDR with occasional problems of too many ties in ranks for low quality ChIP-seq data.\n\n\nHOMER - developers have a detailed pipeline and code (in beta) for IDR analysis with HOMER at https://github.com/karmel/homer-idr \n\n\nPeakSeq - Run with modified PeakSeq parameters to obtain large number of peaks\n\n\nHotSpot, MOSAiCS, GPS/GEM, \u2026\n\n\n\n\n\n\nSetting up\n\n\nIDR is an open source tool available on \nGitHub\n. It is a Python program that has already been installed on O2. The first thing we need to do is load the module (and all dependency modules) to run IDR:\n\n\n$ module load gcc/6.2.0  python/3.6.0\n$ module load idr/2.0.2\n\n\n\n\nNow let's move into the \nchipseq/results\n directory and create a new directory for the results of our IDR analysis.\n\n\n$ cd ~/chipseq/results\n$ mkdir IDR\n\n\n\n\nCopy over the sorted narrowPeak files for each replicate for Nanog and Pou5f1:\n\n\n$ cp /n/groups/hbctraining/chip-seq/idr/macs2/*sorted* IDR/\n\n\n\n\nPeak consistency between true replicates\n\n\nThe first step is taking our replicates and evaluating how consistent they are with one another.\n\n\n\n\nTo run IDR we use the \nidr\n command followed by any necessary parameters. To see what parameters we have available to us, we can use:\n\n\n$ idr -h\n\n\n\n\nFor our run we will change only parameters associated with input and output files. We will also change the field on which we want to create ranks since the defaults are set for SPP peak calls. Parameters that pertain to the inference procedure are left as is, since the chosen defaults are what the developers believe are reasonable in the vast majority of cases. \n\n\nMove into the IDR directory:\n\n\n$ cd IDR\n\n\n\n\nLet's start with the Nanog replicates:\n\n\n$ idr --samples Nanog_Rep1_sorted_peaks.narrowPeak Nanog_Rep2_sorted_peaks.narrowPeak \\\n--input-file-type narrowPeak \\\n--rank p.value \\\n--output-file Nanog-idr \\\n--plot \\\n--log-output-file nanog.idr.log\n\n\n\n\nAnd now with the Pou5f1 replicates:\n\n\n$ idr --samples Pou5f1_Rep1_sorted_peaks.narrowPeak Pou5f1_Rep2_sorted_peaks.narrowPeak \\\n--input-file-type narrowPeak \\\n--rank p.value \\\n--output-file Pou5f1-idr \\\n--plot \\\n--log-output-file pou5f1.idr.log\n\n\n\n\nOutput files\n\n\nThe output file format mimics the input file type, with some additional fields. Note that the \nfirst 10 columns are a standard narrowPeak file\n, pertaining to the merged peak across the two replicates. \n\n\nColumn 5 contains the scaled IDR value, \nmin(int(log2(-125IDR), 1000)\n For example, peaks with an IDR of 0 have a score of 1000, peaks with an IDR of 0.05 have a score of int(-125log2(0.05)) = 540, and IDR of 1.0 has a score of 0.\n\n\nColumns 11 and 12 correspond to the local and global IDR value, respectively.\n \n\n The \nglobal IDR\n is the value used to calculate the scaled IDR number in column 5, it \nis analogous to a multiple hypothesis correction on a p-value to compute an FDR\n. \n\n The \nlocal IDR\n is akin to the posterior probability of a peak belonging to the irreproducible noise component. You can read \nthis paper\n for more details. \n\n\nColumns 13 through 16 correspond to Replicate 1 peak data\n and \nColumns 17 through 20 correspond to Replicate 2 peak data.\n\n\nMore detail on the output can be \nfound in the user manual\n. Also, if you have any unanswered questions check out posts in the \nGoogle groups forum\n. \n\n\nLet's take a look at our output files. \nHow many common peaks are considered for each TF?\n\n\n$ wc -l *-idr\n\n\n\n\nTo find out how may of those shared regions have an IDR \n 0.05, we can take a look at the log files. Alternatively, since we requested all peaks and their IDR value as output we can also filter the file using an \nawk\n command.\n\n\n$ awk '{if($5 \n= 540) print $0}' Nanog-idr | wc -l\n$ awk '{if($5 \n= 540) print $0}' Pou5f1-idr | wc -l\n\n\n\n\nWhich of the two TFs show better reproducibility between replicates? How does this compare to the \nbedtools\n overlaps?\n\n\nOutput plots\n\n\nThere is a single image file output for each IDR analyses (\n.png\n files). Within each image you should see four plots. \nSince we are working with such a small subset of data, the plots are not as meaningful. Below we have provided the images generated for the full dataset for Pou5f1.\n\n\n \n\n\nThe plot for each quadrant is described below:\n\n\nUpper Left\n: Replicate 1 peak ranks versus Replicate 2 peak ranks - peaks that do not pass the specified idr threshold are colored red.\n\n\nUpper Right\n: Replicate 1 log10 peak scores versus Replicate 2 log10 peak scores - peaks that do not pass the specified idr threshold are colored red.\n\n\nBottom Row\n: Peak rank versus IDR scores are plotted in black. The overlayed boxplots display the distribution of idr values in each 5% quantile. The IDR values are thresholded at the optimization precision - 1e-6 by default.\n\n\nPeak consistency between pooled pseudoreplicates\n\n\nOnce you have IDR values for true replicates, you want to see how this compares to pooled replicates. This is a bit more involved, as it requires you to go back to the BAM files, merge the reads and randomly split them into two pseudo-replicates. If the original replicates are highly concordant, then shuffling and splitting them should result in pseudo-replicates that the reflect the originals. \nTherefore, if IDR analysis on the pooled pseudo-replicates results in a number of peaks that are similar (within a factor of 2) these are truly good replicates.\n\n\n \n\n\nWe will not run this analysis, but have provided a bash script below if you wanted to take a stab at it.\n To run this script you will need to:\n\n\n\n\nProvide BAM files and run it for each TF separately. These are located at \n/groups/hbctraining/ngs-data-analysis-longcourse/chipseq/bowtie2\n. Or you can point to the BAM files generated from Bowtie2 in the home directory.\n\n\nBe sure to also ask for enough memory in your \nsbatch\n command.\n\n\nChange the paths for output to the directories that are relevant to you\n\n\n\n\n\n\nNOTE 1: For the paths and directories we are using \n/n/scratch2\n. This script generates fairly large intermediate files which can quickly fill up your home directory. To avoid this please make use of the scratch space and once the analysis is complete move over only the relevant files.\n\n\nNOTE 2: To run the script below you will have to replace every instance of \nmm573\n with your account name.\n\n\n\n\n#!/bin/sh\n\n# USAGE: sh pseudorep_idr.sh \ninput BAM rep1\n \nchip BAM rep1\n \ninput BAM rep2\n \nchip BAM rep2\n \nNAME for IDR output\n\n\n# This script will take the BAM files and perform the following steps: \n    ## Merge BAMs for ChiP files,\n    ## Shuffle reads and split into two new BAM files (pseudo-replicates), \n    ## Merge BAMs for Input files,\n    ## Shuffle reads and split into two new BAM files (pseudo-replicates), \n    ## Call peaks on pseudo-replicates with MACS2 , \n    ## Sort peaks called on pseudo-replicates,\n    ## IDR analysis using pseudo-replicate peak calls\n\n# Please use the following SLURM directives:\n    ## -t 0-12:00\n    ## -p short\n    ## --mem=40G\n\ndate \n\ninputFile1=`basename $1`\ntreatFile1=`basename $2`\ninputFile2=`basename $3`\ntreatFile2=`basename $4`\nEXPT=$5\n\nNAME1=`basename $treatFile1 _full.bam`\nNAME2=`basename $treatFile2 _full.bam`\n\n# Make Directories\nmkdir -p /n/scratch2/mm573/idr_chipseq/macs\nmkdir -p /n/scratch2/mm573/idr_chipseq/pooled_pseudoreps\nmkdir -p /n/scratch2/mm573/idr_chipseq/tmp\n\n# Set paths\nbaseDir=/n/groups/hbctraining/ngs-data-analysis-longcourse/chipseq/bowtie2\nmacsDir=/n/scratch2/mm573/idr_chipseq/macs\noutputDir=/n/scratch2/mm573/idr_chipseq/pooled_pseudoreps\ntmpDir=/n/scratch2/mm573/idr_chipseq/tmp\n\n#Merge treatment BAMS\necho \nMerging BAM files for pseudoreplicates...\n\nsamtools merge -u ${tmpDir}/${NAME1}_${NAME2}_merged.bam $baseDir/${treatFile1} $baseDir/${treatFile2}\nsamtools view -H ${tmpDir}/${NAME1}_${NAME2}_merged.bam \n ${tmpDir}/${EXPT}_header.sam\n\n#Split merged treatments\nnlines=$(samtools view ${tmpDir}/${NAME1}_${NAME2}_merged.bam | wc -l ) # Number of reads in the BAM file\nnlines=$(( (nlines + 1) / 2 )) # half that number\nsamtools view ${tmpDir}/${NAME1}_${NAME2}_merged.bam | shuf - | split -d -l ${nlines} - \n${tmpDir}/${EXPT}\n # This will shuffle the lines in the file and split it\n into two SAM files\ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}00 | samtools view -bS - \n ${outputDir}/${EXPT}00.bam\ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}01 | samtools view -bS - \n ${outputDir}/${EXPT}01.bam\n\n#Merge input BAMS\necho \nMerging input BAM files for pseudoreplicates...\n\nsamtools merge -u ${tmpDir}/${NAME1}input_${NAME2}input_merged.bam $baseDir/${inputFile1} $baseDir/${inputFile2}\n\n#Split merged treatment BAM\nnlines=$(samtools view ${tmpDir}/${NAME1}input_${NAME2}input_merged.bam | wc -l ) # Number of reads in the BAM file\nnlines=$(( (nlines + 1) / 2 )) # half that number\nsamtools view ${tmpDir}/${NAME1}input_${NAME2}input_merged.bam | shuf - | split -d -l ${nlines} - \n${tmpDir}/${EXPT}_input\n # This will shuffle the lines in the file and split in two \ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}_input00 | samtools view -bS - \n ${outputDir}/${EXPT}_input00.bam\ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}_input01 | samtools view -bS - \n ${outputDir}/${EXPT}_input01.bam\n\n\n#Peak calling on pseudoreplicates\necho \nCalling peaks for pseudoreplicate1 \n\nmacs2 callpeak -t ${outputDir}/${EXPT}00.bam -c ${outputDir}/${EXPT}_input00.bam -f BAM -g hs -n $macsDir/${NAME1}_pr -B -p 1e-3  2\n $macsDir/${NAME1}_pr_macs2.log\n\necho \nCalling peaks for pseudoreplicate2\n\nmacs2 callpeak -t ${outputDir}/${EXPT}01.bam -c ${outputDir}/${EXPT}_input01.bam -f BAM -g hs -n $macsDir/${NAME2}_pr -B -p 1e-3  2\n $macsDir/${NAME2}_pr_macs2.log\n\n#Sort peak by -log10(p-value)\necho \nSorting peaks...\n\nsort -k8,8nr $macsDir/${NAME1}_pr_peaks.narrowPeak | head -n 100000 \n $macsDir/${NAME1}_pr_sorted.narrowPeak\nsort -k8,8nr $macsDir/${NAME2}_pr_peaks.narrowPeak | head -n 100000 \n $macsDir/${NAME2}_pr_sorted.narrowPeak\n\n#Independent replicate IDR\necho \nRunning IDR on pseudoreplicates...\n\nidr --samples $macsDir/${NAME1}_pr_sorted.narrowPeak $macsDir/${NAME2}_pr_sorted.narrowPeak --input-file-type narrowPeak --output-file ${EXPT}_pseudorep-idr --rank p.value --plot\n\n\n# Remove the tmp directory\nrm -r $tmpDir\n\n\n\n\nSelf-consistency analysis\n\n\nAn \noptional step\n is to create pseudo-replicates for each replicate by randomly splitting the reads and running them through the same workflow. Again, \nif IDR analysis on the self-replicates for Replicate 1 results in a number of peaks that are similar (within a factor of 2) to self-replicates for Replicate 2 these are truly good replicates.\n\n\n\n\nThreshold guidelines\n\n\nThe user manual provides \nguidelines on IDR thresholds\n which are recommended for the different types of IDR analyses. Depending on the organism you are studying and the total number of peaks you are starting with you will want to modify the thresholds accordingly.\n\n\nAn example for our analysis is described below:\n\n\n\n\nIf starting with \n 100K pre-IDR peaks for large genomes (human/mouse):\nFor true replicates and self-consistency replicates an IDR threshold of 0.05 is more appropriate \n\n\nUse a tighter threshold for pooled-consistency since pooling and subsampling equalizes the pseudo-replicates in terms of data quality. Err on the side of caution and use more stringent IDR threshold of 0.01\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Handling Replicates with Bedtools or IDR"
        }, 
        {
            "location": "/08_handling-replicates/#learning-objectives", 
            "text": "Combining replicates using simple overlap with Bedtools  Combining replicates to only get the highly reproducible peaks using the IDR method", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/08_handling-replicates/#handling-replicates-in-chip-seq", 
            "text": "As with any high-throughput experiment, a single assay is often subject to a substantial amount of variability. Thus, it is highly recommended to setup your experimental design with a minimum of 2-3 biological replicates. Presumably, two replicates measuring the same underlying biology should have high consistency but that is not always the case. In order to evaluate consistency between replicates  we require metrics that objectively assess the reproducibility of high-throughput assays .   Since we have 2 replicates in this example, we want to consider only those peaks that are present in both replicates before we compare the peaks from the two transcription factors to one another.     Common methods for handling replicates includes taking overlapping peak calls across replicates and then assessing differences in binding regions. Additionally, there are more complex methods that employ statistical testing and evaluate the reproducibility between replicates. In this lesson we will cover both methods.   NOTE: A recent talk on \"Accessing and using ENCODE data\"  linked here  where they talk about handling replicates and the similarities and differences when using an overlap versus IDR analysis.", 
            "title": "Handling replicates in ChIP-Seq"
        }, 
        {
            "location": "/08_handling-replicates/#overlapping-peaks", 
            "text": "In this section, our goal is to determine what peaks are in common between the the two replicates for each factor (Nanog and Pou5f1). To perform this task we are going to use a suite of tools called  bedtools .", 
            "title": "Overlapping peaks"
        }, 
        {
            "location": "/08_handling-replicates/#bedtools", 
            "text": "The idea is that genome coordinate information can be used to perform relatively simple arithmetic, like combining, subsetting, intersecting, etc., to obtain all sorts of information.  bedtools  from  Aaron Quinlan's group  at University of Utah is easy to use, and an extremely versatile tool that performs tasks of this nature.    As the name implies, this suite of tools works with  Bed  files, but it also works with other file formats that have genome coordinate information.     NOTE:  When working with multiple files to perform arithmetic on genomic coordinates, it is essential that all files have coordinate information for the same exact version of the genome and the same coordinate system (0-based or 1-based)!", 
            "title": "bedtools"
        }, 
        {
            "location": "/08_handling-replicates/#setting-up", 
            "text": "Let's start an interactive session and change directories and set up a space for the resulting overlaps.   $ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash    \n\n$ cd ~/chipseq/results/\n\n$ mkdir bedtools  Load the modules for  bedtools  and  samtools :  \n$ module load gcc/6.2.0 bedtools/2.26.0 samtools/1.3.1", 
            "title": "Setting up"
        }, 
        {
            "location": "/08_handling-replicates/#finding-overlapping-peaks-between-replicates", 
            "text": "The  bedtools intersect  command only reports back the peaks that are overlapping with respect to the file defined as  a  in the command.   To find out more information on the parameters available when intersecting, use the help flag:  $ bedtools intersect -h  The intersect tool evaluates A (file 1) and finds regions that overlap in B (file 2). We will add the  -wo  which indicates to write the original A (file 1) and B (file 2) entries plus the number of base pairs of overlap between the two features.  Let's start with the Nanog replicates:   $ bedtools intersect \\\n-a macs2/Nanog-rep1_peaks.narrowPeak \\\n-b macs2/Nanog-rep2_peaks.narrowPeak \\\n-wo   bedtools/Nanog-overlaps.bed  How many overlapping peaks did we get?  We'll do the same for the Pou5f1 replicates:  $ bedtools intersect \\\n-a macs2/Pou5f1-rep1_peaks.narrowPeak \\\n-b macs2/Pou5f1-rep2_peaks.narrowPeak \\\n-wo   bedtools/Pou5f1-overlaps.bed  Note that we are working with subsetted data and so our list of peaks for each replicate is small. Thus, the overlapping peak set will be small as we found with both Nanog and Pou5f1. What is interesting though, is that even though the individual peak lists are smaller for Pou5f1 samples, the overlapping replicates represent a higher proportion of overlap with respect to each replicate.   Historical Note :  A simpler heuristic for establishing reproducibility was previously used as a standard for depositing ENCODE data and was in effect when much of the currently available data was submitted. According to this standard, either 80% of the top 40% of the peaks identified from one replicate using an acceptable scoring method should overlap the list of peaks from the other replicate, OR peak lists scored using all available reads from each replicate should share more than 75% of regions in common. As with the current standards, this was developed based on experience with accumulated ENCODE ChIP-seq data, albeit with a much smaller sample size.", 
            "title": "Finding overlapping peaks between replicates"
        }, 
        {
            "location": "/08_handling-replicates/#irreproducibility-discovery-rate-idr", 
            "text": "IDR  is a framework developed by Qunhua Li and Peter Bickel's group that  compares a pair of ranked lists of regions/peaks and assigns values that reflect its reproducibility.       \"The basic idea is that if two replicates measure the same underlying biology, the most significant peaks, which are likely to be genuine signals, are expected to have high consistency between replicates, whereas peaks with low significance, which are more likely to be noise, are expected to have low consistency. If the consistency between a pair of rank lists (peaks) that contains both significant and insignificant findings is plotted, a transition in consistency is expected (Fig. 1C). This consistency transition provides an internal indicator of the change from signal to noise and suggests how many peaks have been reliably detected.\"   https://ccg.vital-it.ch/var/sib_april15/cases/landt12/idr.html  It is extensively used by the ENCODE and modENCODE projects and is part of their  ChIP-seq guidelines and standards . It has been established for submission of ChIP-seq data sets and have been constructed based on the historical experiences of ENCODE ChIP-seq data production groups.", 
            "title": "Irreproducibility Discovery Rate (IDR)"
        }, 
        {
            "location": "/08_handling-replicates/#why-idr", 
            "text": "IDR avoids choices of initial cutoffs, which are not comparable for different callers   IDR does not depend on arbitrary thresholds and so all regions/peaks are considered.   It is based on ranks, so does not require the input signals to be calibrated or with a specific fixed scale (only order matters).", 
            "title": "Why IDR?"
        }, 
        {
            "location": "/08_handling-replicates/#components-of-idr", 
            "text": "The IDR approach creates a curve, from which it then quantitatively assesses when the \ufb01ndings are no longer consistent across replicates. There are three main components:   1) A  correspondence curve : a graphical representation of matched peaks as you go down the ranked list. Qualitative, not adequate for selecting signals.     2) An  inference procedure : summarizes the proportion of reproducible and irreproducible signals. Quantitative, using a copula mixture model.   What proportion of identifications have a poor correspondence, i.e. falling into \u201dnoise\u201d?\nHow consistent are the identifications before reaching breakdown?   3)  Irreproducible Discovery Rate (IDR) : Derive a significance value from the inference procedure (#2) in a fashion similar to FDR, and can be used to control the level of irreproducibility rate when selecting signals.\ni.e. 0.05 IDR means that peak has a 5% chance of being an irreproducible discovery", 
            "title": "Components of IDR"
        }, 
        {
            "location": "/08_handling-replicates/#the-idr-pipeline", 
            "text": "There are three main steps to the IDR pipeline:   Evaluate peak consistency between  true replicates  Evaluate peak consistency between  pooled pseudo-replicates  Evaluate  self-consistency  for each individual replicate       This figure is taken from the  ENCODE ChIP-Seq Guidelines .   We will only be running Step 1 in this lesson, but will discuss steps 2 and 3 in a bit more detail.", 
            "title": "The IDR pipeline"
        }, 
        {
            "location": "/08_handling-replicates/#running-idr", 
            "text": "To run IDR  we should be using the full dataset , and if you are interested the full BAM files can be downloaded from ENCODE, however for timeliness and consistency we will continue with our subsetted data for this lesson.  To run IDR the recommendation is to  run MACS2 less stringently  to allow a larger set of peaks to be identified for each replicate. In addition the narrowPeak files have to be  sorted by the  -log10(p-value)  column . We have already performed these 2 steps for the samples from both groups, and the commands we used for it as listed below. To reiterate,  you do NOT NEED TO RUN the following lines of code, we have already generated narrowPeak files for you!  ###DO NOT RUN THIS###\n\n# Call peaks using a liberal p-value cutoff\nmacs2 callpeak -t treatFile.bam -c inputFile.bam -f BAM -g 1.3e+8 -n macs/NAME_FOR_OUPUT -B -p 1e-3  2  macs/NAME_FOR_OUTPUT_macs2.log\n\n#Sort peak by -log10(p-value)\nsort -k8,8nr NAME_OF_INPUT_peaks.narrowPeak   macs/NAME_FOR_OUPUT_peaks.narrowPeak    IDR will work with many different Peak callers, the following have been tested:   SPP - Works out of the box  MACS1.4 - DO NOT use with IDR  MACS2 - Works well with IDR with occasional problems of too many ties in ranks for low quality ChIP-seq data.  HOMER - developers have a detailed pipeline and code (in beta) for IDR analysis with HOMER at https://github.com/karmel/homer-idr   PeakSeq - Run with modified PeakSeq parameters to obtain large number of peaks  HotSpot, MOSAiCS, GPS/GEM, \u2026", 
            "title": "Running IDR"
        }, 
        {
            "location": "/08_handling-replicates/#setting-up_1", 
            "text": "IDR is an open source tool available on  GitHub . It is a Python program that has already been installed on O2. The first thing we need to do is load the module (and all dependency modules) to run IDR:  $ module load gcc/6.2.0  python/3.6.0\n$ module load idr/2.0.2  Now let's move into the  chipseq/results  directory and create a new directory for the results of our IDR analysis.  $ cd ~/chipseq/results\n$ mkdir IDR  Copy over the sorted narrowPeak files for each replicate for Nanog and Pou5f1:  $ cp /n/groups/hbctraining/chip-seq/idr/macs2/*sorted* IDR/", 
            "title": "Setting up"
        }, 
        {
            "location": "/08_handling-replicates/#peak-consistency-between-true-replicates", 
            "text": "The first step is taking our replicates and evaluating how consistent they are with one another.   To run IDR we use the  idr  command followed by any necessary parameters. To see what parameters we have available to us, we can use:  $ idr -h  For our run we will change only parameters associated with input and output files. We will also change the field on which we want to create ranks since the defaults are set for SPP peak calls. Parameters that pertain to the inference procedure are left as is, since the chosen defaults are what the developers believe are reasonable in the vast majority of cases.   Move into the IDR directory:  $ cd IDR  Let's start with the Nanog replicates:  $ idr --samples Nanog_Rep1_sorted_peaks.narrowPeak Nanog_Rep2_sorted_peaks.narrowPeak \\\n--input-file-type narrowPeak \\\n--rank p.value \\\n--output-file Nanog-idr \\\n--plot \\\n--log-output-file nanog.idr.log  And now with the Pou5f1 replicates:  $ idr --samples Pou5f1_Rep1_sorted_peaks.narrowPeak Pou5f1_Rep2_sorted_peaks.narrowPeak \\\n--input-file-type narrowPeak \\\n--rank p.value \\\n--output-file Pou5f1-idr \\\n--plot \\\n--log-output-file pou5f1.idr.log", 
            "title": "Peak consistency between true replicates"
        }, 
        {
            "location": "/08_handling-replicates/#output-files", 
            "text": "The output file format mimics the input file type, with some additional fields. Note that the  first 10 columns are a standard narrowPeak file , pertaining to the merged peak across the two replicates.   Column 5 contains the scaled IDR value,  min(int(log2(-125IDR), 1000)  For example, peaks with an IDR of 0 have a score of 1000, peaks with an IDR of 0.05 have a score of int(-125log2(0.05)) = 540, and IDR of 1.0 has a score of 0.  Columns 11 and 12 correspond to the local and global IDR value, respectively.    The  global IDR  is the value used to calculate the scaled IDR number in column 5, it  is analogous to a multiple hypothesis correction on a p-value to compute an FDR .   The  local IDR  is akin to the posterior probability of a peak belonging to the irreproducible noise component. You can read  this paper  for more details.   Columns 13 through 16 correspond to Replicate 1 peak data  and  Columns 17 through 20 correspond to Replicate 2 peak data.  More detail on the output can be  found in the user manual . Also, if you have any unanswered questions check out posts in the  Google groups forum .   Let's take a look at our output files.  How many common peaks are considered for each TF?  $ wc -l *-idr  To find out how may of those shared regions have an IDR   0.05, we can take a look at the log files. Alternatively, since we requested all peaks and their IDR value as output we can also filter the file using an  awk  command.  $ awk '{if($5  = 540) print $0}' Nanog-idr | wc -l\n$ awk '{if($5  = 540) print $0}' Pou5f1-idr | wc -l  Which of the two TFs show better reproducibility between replicates? How does this compare to the  bedtools  overlaps?", 
            "title": "Output files"
        }, 
        {
            "location": "/08_handling-replicates/#output-plots", 
            "text": "There is a single image file output for each IDR analyses ( .png  files). Within each image you should see four plots.  Since we are working with such a small subset of data, the plots are not as meaningful. Below we have provided the images generated for the full dataset for Pou5f1.     The plot for each quadrant is described below:  Upper Left : Replicate 1 peak ranks versus Replicate 2 peak ranks - peaks that do not pass the specified idr threshold are colored red.  Upper Right : Replicate 1 log10 peak scores versus Replicate 2 log10 peak scores - peaks that do not pass the specified idr threshold are colored red.  Bottom Row : Peak rank versus IDR scores are plotted in black. The overlayed boxplots display the distribution of idr values in each 5% quantile. The IDR values are thresholded at the optimization precision - 1e-6 by default.", 
            "title": "Output plots"
        }, 
        {
            "location": "/08_handling-replicates/#peak-consistency-between-pooled-pseudoreplicates", 
            "text": "Once you have IDR values for true replicates, you want to see how this compares to pooled replicates. This is a bit more involved, as it requires you to go back to the BAM files, merge the reads and randomly split them into two pseudo-replicates. If the original replicates are highly concordant, then shuffling and splitting them should result in pseudo-replicates that the reflect the originals.  Therefore, if IDR analysis on the pooled pseudo-replicates results in a number of peaks that are similar (within a factor of 2) these are truly good replicates.     We will not run this analysis, but have provided a bash script below if you wanted to take a stab at it.  To run this script you will need to:   Provide BAM files and run it for each TF separately. These are located at  /groups/hbctraining/ngs-data-analysis-longcourse/chipseq/bowtie2 . Or you can point to the BAM files generated from Bowtie2 in the home directory.  Be sure to also ask for enough memory in your  sbatch  command.  Change the paths for output to the directories that are relevant to you    NOTE 1: For the paths and directories we are using  /n/scratch2 . This script generates fairly large intermediate files which can quickly fill up your home directory. To avoid this please make use of the scratch space and once the analysis is complete move over only the relevant files.  NOTE 2: To run the script below you will have to replace every instance of  mm573  with your account name.   #!/bin/sh\n\n# USAGE: sh pseudorep_idr.sh  input BAM rep1   chip BAM rep1   input BAM rep2   chip BAM rep2   NAME for IDR output \n\n# This script will take the BAM files and perform the following steps: \n    ## Merge BAMs for ChiP files,\n    ## Shuffle reads and split into two new BAM files (pseudo-replicates), \n    ## Merge BAMs for Input files,\n    ## Shuffle reads and split into two new BAM files (pseudo-replicates), \n    ## Call peaks on pseudo-replicates with MACS2 , \n    ## Sort peaks called on pseudo-replicates,\n    ## IDR analysis using pseudo-replicate peak calls\n\n# Please use the following SLURM directives:\n    ## -t 0-12:00\n    ## -p short\n    ## --mem=40G\n\ndate \n\ninputFile1=`basename $1`\ntreatFile1=`basename $2`\ninputFile2=`basename $3`\ntreatFile2=`basename $4`\nEXPT=$5\n\nNAME1=`basename $treatFile1 _full.bam`\nNAME2=`basename $treatFile2 _full.bam`\n\n# Make Directories\nmkdir -p /n/scratch2/mm573/idr_chipseq/macs\nmkdir -p /n/scratch2/mm573/idr_chipseq/pooled_pseudoreps\nmkdir -p /n/scratch2/mm573/idr_chipseq/tmp\n\n# Set paths\nbaseDir=/n/groups/hbctraining/ngs-data-analysis-longcourse/chipseq/bowtie2\nmacsDir=/n/scratch2/mm573/idr_chipseq/macs\noutputDir=/n/scratch2/mm573/idr_chipseq/pooled_pseudoreps\ntmpDir=/n/scratch2/mm573/idr_chipseq/tmp\n\n#Merge treatment BAMS\necho  Merging BAM files for pseudoreplicates... \nsamtools merge -u ${tmpDir}/${NAME1}_${NAME2}_merged.bam $baseDir/${treatFile1} $baseDir/${treatFile2}\nsamtools view -H ${tmpDir}/${NAME1}_${NAME2}_merged.bam   ${tmpDir}/${EXPT}_header.sam\n\n#Split merged treatments\nnlines=$(samtools view ${tmpDir}/${NAME1}_${NAME2}_merged.bam | wc -l ) # Number of reads in the BAM file\nnlines=$(( (nlines + 1) / 2 )) # half that number\nsamtools view ${tmpDir}/${NAME1}_${NAME2}_merged.bam | shuf - | split -d -l ${nlines} -  ${tmpDir}/${EXPT}  # This will shuffle the lines in the file and split it\n into two SAM files\ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}00 | samtools view -bS -   ${outputDir}/${EXPT}00.bam\ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}01 | samtools view -bS -   ${outputDir}/${EXPT}01.bam\n\n#Merge input BAMS\necho  Merging input BAM files for pseudoreplicates... \nsamtools merge -u ${tmpDir}/${NAME1}input_${NAME2}input_merged.bam $baseDir/${inputFile1} $baseDir/${inputFile2}\n\n#Split merged treatment BAM\nnlines=$(samtools view ${tmpDir}/${NAME1}input_${NAME2}input_merged.bam | wc -l ) # Number of reads in the BAM file\nnlines=$(( (nlines + 1) / 2 )) # half that number\nsamtools view ${tmpDir}/${NAME1}input_${NAME2}input_merged.bam | shuf - | split -d -l ${nlines} -  ${tmpDir}/${EXPT}_input  # This will shuffle the lines in the file and split in two \ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}_input00 | samtools view -bS -   ${outputDir}/${EXPT}_input00.bam\ncat ${tmpDir}/${EXPT}_header.sam ${tmpDir}/${EXPT}_input01 | samtools view -bS -   ${outputDir}/${EXPT}_input01.bam\n\n\n#Peak calling on pseudoreplicates\necho  Calling peaks for pseudoreplicate1  \nmacs2 callpeak -t ${outputDir}/${EXPT}00.bam -c ${outputDir}/${EXPT}_input00.bam -f BAM -g hs -n $macsDir/${NAME1}_pr -B -p 1e-3  2  $macsDir/${NAME1}_pr_macs2.log\n\necho  Calling peaks for pseudoreplicate2 \nmacs2 callpeak -t ${outputDir}/${EXPT}01.bam -c ${outputDir}/${EXPT}_input01.bam -f BAM -g hs -n $macsDir/${NAME2}_pr -B -p 1e-3  2  $macsDir/${NAME2}_pr_macs2.log\n\n#Sort peak by -log10(p-value)\necho  Sorting peaks... \nsort -k8,8nr $macsDir/${NAME1}_pr_peaks.narrowPeak | head -n 100000   $macsDir/${NAME1}_pr_sorted.narrowPeak\nsort -k8,8nr $macsDir/${NAME2}_pr_peaks.narrowPeak | head -n 100000   $macsDir/${NAME2}_pr_sorted.narrowPeak\n\n#Independent replicate IDR\necho  Running IDR on pseudoreplicates... \nidr --samples $macsDir/${NAME1}_pr_sorted.narrowPeak $macsDir/${NAME2}_pr_sorted.narrowPeak --input-file-type narrowPeak --output-file ${EXPT}_pseudorep-idr --rank p.value --plot\n\n\n# Remove the tmp directory\nrm -r $tmpDir", 
            "title": "Peak consistency between pooled pseudoreplicates"
        }, 
        {
            "location": "/08_handling-replicates/#self-consistency-analysis", 
            "text": "An  optional step  is to create pseudo-replicates for each replicate by randomly splitting the reads and running them through the same workflow. Again,  if IDR analysis on the self-replicates for Replicate 1 results in a number of peaks that are similar (within a factor of 2) to self-replicates for Replicate 2 these are truly good replicates.", 
            "title": "Self-consistency analysis"
        }, 
        {
            "location": "/08_handling-replicates/#threshold-guidelines", 
            "text": "The user manual provides  guidelines on IDR thresholds  which are recommended for the different types of IDR analyses. Depending on the organism you are studying and the total number of peaks you are starting with you will want to modify the thresholds accordingly.  An example for our analysis is described below:   If starting with   100K pre-IDR peaks for large genomes (human/mouse):\nFor true replicates and self-consistency replicates an IDR threshold of 0.05 is more appropriate   Use a tighter threshold for pooled-consistency since pooling and subsampling equalizes the pseudo-replicates in terms of data quality. Err on the side of caution and use more stringent IDR threshold of 0.01    This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Threshold guidelines"
        }, 
        {
            "location": "/09_data_visualization/", 
            "text": "Approximate time: 80 minutes\n\n\nLearning Objectives\n\n\n\n\nGenerate bigWig files\n\n\nVisualizing enrichment patterns at particular locations in the genome\n\n\nEvaluating regions of differential enrichment using \nbedtools\n\n\n\n\nVisualization of ChIP-seq data\n\n\nThe first part of ChIP-sequencing analysis uses common processing pipelines, which involves the alignment of raw reads to the genome, data filtering, and identification of enriched signal regions (peak calling). In the second stage, individual programs allow detailed analysis of those peaks, biological interpretation, and visualization of ChIP-seq results.\n\n\nThere are various strategies for visualizing enrichment patterns and we will explore a few of them. To start, we will create bigWig files for our samples, a standard file format commonly used for ChIP-seq data visualization.\n\n\nCreating bigWig files\n\n\nThe first thing we want to do is take our alignment files (BAM) and convert them into bigWig files. The bigWig format is an indexed binary format useful for dense, continuous data that will be displayed in a genome browser as a graph/track, but also is used as input for some of the visualization commands we will be running in \ndeepTools\n. \n\n\ndeepTools\n, is a suite of Python tools developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq. \ndeepTools\n has a wide variety of commands that go beyond those that are covered in this lesson. We encourage you to look through the docuementation and explore on your own time. \n\n\n\n\nImage acquired from \ndeepTools documentation\n pages\n\n\nStart an interactive session with 6 cores. \nIf you are already logged on to a compute node you will want to exit and start a new session\n.\n\n\n$ srun --pty -p short -t 0-12:00 --mem 8G -n 6 --reservation=HBC bash\n\n\n\n\nWe will begin by creating a directory for the visualization output and loading the required modules to run \ndeepTools\n.\n\n\n$ cd ~/chipseq/results/\n$ mkdir -p visualization/bigWig visualization/figures\n\n\n\n\n$ module load gcc/6.2.0  python/2.7.12\n$ module load deeptools/2.5.3 \n\n\n\n\nOne last thing we need to do is \ncreate an index file for each one of our BAM files\n. To perform some functions on the BAM file, many tools require an index. Think of an index located at the back of a textbook. When you are interested in a particular subject area you look for the keyword in the index and identify the pages that contain the relevant information. Similarily, indexing the BAM file aims to achieve fast retrieval of alignments overlapping a specified region without going through the whole alignment file. \n\n\nIn order to index a BAM file, we will use \nSAMtools\n, a tool suite that provides alot of functionality in dealing with alignment files. There is a command called \nsamtools index\n, which is what we will use. Since we need an index for each of our BAM files, we will put this in a \nfor\n loop to avoid having to run the same command multiple times.\n\n\nFirst, let's load the module:\n\n\n$ module load samtools/1.3.1\n\n\n\n\nNow, at the command prompt start the \nfor\n loop\n:\n\n\n\nfor file in ~/chipseq/results/bowtie2/*aln.bam\ndo\nsamtools index $file\ndone\n\n\n\n\n\n\nNOTE:\n The above is assuming that you are pressing return after each line of code. If you wanted you could also run this command as a single line:\n\n\n$ for file in ~/chipseq/results/bowtie2/*aln.bam; do samtools index $file; done\n\n\n\n\n\n\nNow, to create our bigWig files there are two tools that can be useful: \nbamCoverage\n and \nbamCompare\n. The former will take in a single BAM file and return to you a bigWig file. The latter allows you to normalize two files to each other (i.e. ChIP sample relative to input) and will return a single bigWig file.\n\n\nLet's \ncreate a bigWig file for Nanog replicate 2\n using the \nbamCoverage\n command. In addition to the input and output files, there are a few additional parameters we have added. \n\n\n\n\nnormalizeTo1x\n: Report read coverage normalized to 1x sequencing depth (also known as Reads Per Genomic Content (RPGC)). Sequencing depth is defined as: (total number of mapped reads * fragment length) / effective genome size). So \nthe number provided here represents the effective genome size\n. Some examples of values for commonly used organisms can be \nfound here\n.\n\n\nbinSize\n: size of bins in bases\n\n\nsmoothLength\n: defines a window, larger than the \nbinSize\n, to average the number of reads over. This helps produce a more continuous plot.\n\n\ncenterReads\n: reads are centered with respect to the fragment length as specified by \nextendReads\n. This option is useful to get a sharper signal around enriched regions.\n\n\n\n\n$ bamCoverage -b bowtie2/H1hesc_Nanog_Rep2_aln.bam \\\n-o visualization/bigWig/H1hesc_Nanog_Rep2.bw \\\n--binSize 20 \\\n--normalizeTo1x 130000000 \\\n--smoothLength 60 \\\n--extendReads 150 \\\n--centerReads \\\n-p 6 2\n ../logs/Nanog_rep2_bamCoverage.log\n\n\n\n\nWe can do the same for the \nPou5f1 replicate 1\n:\n\n\n$ bamCoverage -b bowtie2/H1hesc_Pou5f1_Rep1_aln.bam \\\n-o visualization/bigWig/H1hesc_Pou5f1_Rep1.bw \\\n--binSize 20 \\\n--normalizeTo1x 130000000 \\\n--smoothLength 60 \\\n--extendReads 150 \\\n--centerReads \\\n-p 6 2\n ../logs/Pou5f1_rep1_bamCoverage.log\n\n\n\n\n\n\nNOTE:\n There is a reason we chose the specific replicates for the above commands, and it will become more obvious as we get to the end of this lesson!\n\n\n\n\nNow, if we wanted to \ncreate a bigWig file in which we normalize the ChIP against the input\n we would use \nbamCompare\n. The command is quite similar to \nbamCoverage\n, the only difference being you require two files as input (\nb1\n and \nb2\n).\n\n\n## DO NOT RUN THIS\n\n$ bamCompare -b1 bowtie2/H1hesc_Pou5f1_Rep1_aln.bam \\\n-b2 bowtie2/H1hesc_Input_Rep1_chr12_aln.bam \\\n-o visualization/bigWig/H1hesc_Pou5f1_Rep1_bgNorm.bw \\\n--binSize 20 \\\n--normalizeTo1x 130000000 \\\n--smoothLength 60 \\\n--extendReads 150 \\\n--centerReads \\\n-p 6 2\n ../logs/Pou5f1_rep1_bamCompare.log\n\n\n\n\n\n\nNOTE:\n When you are creating bigWig files for your full dataset, this will take considerably longer and you will not want to run this interactively (except for testing purposes). Instead, you might want to consider writing a job submission script with a loop that runs this command over all of your BAM files.\n\n\n\n\nSince we are using a toy dataset which contains only a subset of the data, using these bigWigs for visualization would not give us meaningful results. As such, \nwe have created bigWig files from the full dataset that you can use for the rest of this lesson.\n\n\nProfile plots and heatmaps\n\n\nOnce you have bigWig files you can use them to get a global look at enrichment patterns in your data at specified regions. In our example, we will assess enrichment around the TSS and plot this separately for the Nanog and Pou5f1 samples (two replicates in each plot). \n\n\nRather than looking at the TSS for all known genes, we will only look be looking at genes on chromosome 12 in the interest of time. Copy over the BED file which contains the coordinates for all genes on chromosome 12 to the visualization folder.\n\n\n$ cp /n/groups/hbctraining/chip-seq/deepTools/chr12_genes.bed ~/chipseq/results/visualization/\n\n\n\n\nBefore we start plotting our data, we first need to prepare an intermediate file that can be used with the \nplotHeatmap\n and \nplotProfile\n commands.\n\n\n\n\nThe \ncomputeMatrix\n command accepts multiple bigWig files and multiple region files (BED format) to create a count matrix which is the intermediate file. It can also be used to filter and sort regions according to their score. Our region file will be the BED file we just copied over and our bigWog files will be those generated from the full dataset that we have provided for you. Additionally, We will specify a window of +/- 1000bp around the TSS of genes (\n-b\n and \n-a\n). For each window, \ncomputeMatrix\n will calculate scores based on the read density values in the bigWig files.\n\n\nFirst, let's create a matrix one for the Nanog replicates:\n\n\n\n$ computeMatrix reference-point --referencePoint TSS \\\n-b 1000 -a 1000 \\\n-R ~/chipseq/results/visualization/chr12_genes.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Nanog*.bw \\\n--skipZeros \\\n-o ~/chipseq/results/visualization/matrixNanog_TSS_chr12.gz \\\n--outFileSortedRegions ~/chipseq/results/visualization/regions_TSS_chr12.bed\n\n\n\n\n\n\n\nNOTE:\n Typically, the genome regions are genes, and can be obtained from the \nUCSC table browser\n. Alternatively, you could look at other regions of interest that are not genomic feature related (i.e. binding regions from another protein of interest).\n\n\n\n\nNow, let's create another matrix for the Pou5f1 replicates:\n\n\n\n$ computeMatrix reference-point --referencePoint TSS \\\n-b 1000 -a 1000 \\\n-R ~/chipseq/results/visualization/chr12_genes.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Pou5f1*.bw \\\n--skipZeros -o ~/chipseq/results/visualization/matrixPou5f1_TSS_chr12.gz \\\n--outFileSortedRegions ~/chipseq/results/visualization/regionsPou5f1_TSS_chr12.bed\n\n\n\n\n\nUsing that matrix we can create a \nprofile plot\n which is essentially a density plot that evaluates read density across all transcription start sites. For Nanog, we can see that \nReplicate 2 has a particularly higher amount of signal at the TSS compared to Replicate 1\n. \n\n\n$ plotProfile -m visualization/matrixNanog_TSS_chr12.gz \\\n-out visualization/figures/TSS_Nanog_profile.png \\\n--perGroup \\\n--colors green purple \\\n--plotTitle \n --samplesLabel \nRep1\n \nRep2\n \\\n--refPointLabel \nTSS\n \\\n-T \nNanog read density\n \\\n-z \n\n\n\n\n\n\n\n\nAlternatively, we could use a \nheatmap\n to evaluate the same matrix of information:\n\n\n$ plotHeatmap -m visualization/matrixNanog_TSS_chr12.gz \\\n-out visualization/figures/TSS_Nanog_heatmap.png \\\n--colorMap RdBu \\\n--whatToShow 'heatmap and colorbar' \\\n--zMin -4 --zMax 4  \n\n\n\n\n\n\nSimilarly we can do the same for \nPou5f1. Here, we find that Replicate 1 exhibits stronger signal\n.\n\n\n$ plotProfile -m visualization/matrixPou5f1_TSS_chr12.gz \\\n-out visualization/figures/TSS_Pou5f1_profile.png \\\n--perGroup --colors green purple \\\n--plotTitle \n --samplesLabel \nRep1\n \nRep2\n \\\n--refPointLabel \nTSS\n -T \nPou5f1 read density\n -z \n\n\n\n\n\n\n\n$ plotHeatmap -m visualization/matrixPou5f1_TSS_chr12.gz \\\n-out visualization/figures/TSS_Pou5f1_heatmap.png \\\n--colorMap RdBu \\\n--whatToShow 'heatmap and colorbar' \\\n--zMin -2 --zMax 2  \n\n\n\n\n\n\nIf we wanted \nboth images in one single plot\n, we can do that with \nplotHeatmap\n and just removing the \n--whatToShow\n parameter.\n\n\n$ plotHeatmap -m visualization/matrixPou5f1_TSS_chr12.gz \\\n-out visualization/figures/TSS_Pou5f1_heatmap.png \\\n--colorMap RdBu \\\n--zMin -2 --zMax 2  \n\n\n\n\n\n\n\n\nNOTE:\n Both \nplotProfile\n and \nplotHeatmap\n have many options, including the ability to change the type of lines plotted and to plot by group rather than sample. Explore the documentation to find out more detail.\n\n\n\n\nDifferential enrichment\n\n\n\n\nNOTE:\n Identifying differential binding sites across multiple conditions has become of practical importance in biological and medical research and more tools have become available for this type of analysis.  For each group \nwe have two replicates, and it would be best to use tools that make use of these replicates (i.e \nDiffBind\n, \nChIPComp\n) to compute statistics reflecting how significant the changes are. If you are interested in learning more, we have a \nlesson on DiffBind\n analysis using this same dataset.\n\n\n\n\n\nTo provide a more complex picture of biological processes in a cell, many studies aim to compare different datasets obtained by ChIP-seq. In our dataset, we have peak calls from two different transcription factors: Nanog and Pou5f1. To look at the differences in binding between the two we will use \nbedtools\n.\n\n\nYou may already have the module loaded, but in case you don't you will need to load it:\n\n\n$ module list # check which modules you have listed\n\n$ module load gcc/6.2.0 bedtools/2.26.0 \n\n\n\n\nBefore using bedtools to obtain the overlap, we need to combine the information from both replicates. We will do this by concatenating (\ncat\n) the peak calls into a single file.\n\n\nCombining the replicates\n\n\n\n\n$ cat macs2/Nanog-rep1_peaks.narrowPeak macs2/Nanog-rep2_peaks.narrowPeak \n macs2/Nanog_combined.narrowPeak\n\n$ cat macs2/Pou5f1-rep1_peaks.narrowPeak macs2/Pou5f1-rep2_peaks.narrowPeak \n macs2/Pou5f1_combined.narrowPeak  \n\n\n\n\n\nMerge peaks within a file\n\n\nNow for each for each of those combined peak files we need to merge regions that are overlapping. However, \nbedtools merge\n \nrequires a sorted file\n as input as specified in the help documentation. \n\n\n\n\n\n$ bedtools merge -h\n\n\n\n\n\nSort peaks based on coordinates\n\n\nThe \nsort\n command allows you to sort lines of text in a file. However, \nwhen you have multiple columns that you are sorting on (and not on the entire line) you need to specify where sort keys start and where they end\n. In our case we have two columns of numeric values that we want to sort on, the chromosome and the start coordinates (column 1 and 2). The \n-k1,1\n indicates first sort on the first column, and the \n-k2,2n\n is to specify sort next on the second column and that this column is numeric (\nn\n).\n\n\nWe will sort the file and pipe (\n|\n) the output to \nless\n to take a quick peek at the sorted file.\n\n\n\n$ sort -k1,1 -k2,2n macs2/Nanog_combined.narrowPeak | less \n\n\n\n\n\nSort peaks and then merge\n\n\nLet's  start with the Nanog replicates:\n\n\n$ sort -k1,1 -k2,2n macs2/Nanog_combined.narrowPeak | bedtools merge -i - \n bedtools/Nanog_merged.bed \n\n\n\n\n\n\nNOTE:\n this command modifies your \nnarrowPeak\n file into a simple, 3-column \nbed\n file.\n\n\n\n\nNow, we'll do the same for Pou5f1:\n\n\n$ sort -k1,1 -k2,2n macs2/Pou5f1_combined.narrowPeak | bedtools merge -i - \n bedtools/Pou5f1_merged.bed \n\n\n\n\n\n\n\nNOTE:\n You could also use the IDR-optimized set of peaks we generated, instead of combining and merging. In our case, because we are looking at a small subset of the data the number of IDR peaks is very low and so this will give us more peaks as a starting point to evaluate the differences.\n\n\n\n\nLooking for differences in enrichment between Nanog and Pou5f1\n\n\nThe \nbedtools intersect\n will report back the peaks that are overlapping in the file defined in \nb\n with respect to the file defined as \na\n in the command. However, if we add the modifier \n-v\n, this will report only those entries in A that have \nno overlaps\n with B. In this first example, we will obtain peaks that are only present in Nanog samples.\n\n\n\n\n$ bedtools intersect -h\n\n$ bedtools intersect -a bedtools/Nanog_merged.bed -b bedtools/Pou5f1_merged.bed -v \n bedtools/Nanog_only_peaks.bed\n\n\n\nIf we reverse the files listed for \n-a\n and \n-b\n, this will now give us peaks that are only present in Pou5f1:\n\n\n$ bedtools intersect -a bedtools/Pou5f1_merged.bed -b bedtools/Nanog_merged.bed -v \n bedtools/Pou5f1_only_peaks.bed\n\n\n\nHow many peaks are unique to Nanog?\n\n\n$ wc -l bedtools/Nanog_only_peaks.bed\n\n\n\nHow many peaks are unique to Pou5f1?\n\n\n$ wc -l bedtools/Pou5f1_only_peaks.bed\n\n\n\nWe could \nvisualize read density at these sites by using some of the \ndeepTools\n commands we had explored previously.\n\n\n\n $ computeMatrix scale-regions \\\n-R ~/chipseq/results/bedtools/Nanog_only_peaks.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Pou5f1*.bw /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Nanog*.bw \\\n--skipZeros -p 6 \\\n-a 500 -b 500 \\\n-o ~/chipseq/results/visualization/matrixAll_Nanog_binding_sites.gz\n\n\n$ plotProfile -m visualization/matrixAll_Nanog_binding_sites.gz \\\n-out visualization/figures/Allsamples_NanogSites_profile.png \\\n--perGroup  --plotTitle \n \\\n--samplesLabel \nPou5f1-Rep1\n \nPou5f1-Rep2\n \nNanog-Rep1\n \nNanog-Rep2\n \\\n-T \nNanog only binding sites\n  -z \n \\\n--startLabel \n --endLabel \n \\\n--colors red red darkblue darkblue\n\n\n\n\n\n\n\n $ computeMatrix scale-regions \\\n-R ~/chipseq/results/bedtools/Pou5f1_only_peaks.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Pou5f1*.bw /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Nanog*.bw \\\n--skipZeros -p 6 \\\n-a 500 -b 500 \\\n-o ~/chipseq/results/visualization/matrixAll_Pou5f1_binding_sites.gz \n\n\n$ plotProfile -m visualization/matrixAll_Pou5f1_binding_sites.gz \\\n-out visualization/figures/Allsamples_Pou5f1Sites_profile.png \\\n--perGroup  --plotTitle \n \\\n--samplesLabel \nPou5f1-Rep1\n \nPou5f1-Rep2\n \nNanog-Rep1\n \nNanog-Rep2\n \\\n-T \nPou5f1 only binding sites\n  -z \n \\\n--startLabel \n --endLabel \n \\\n--colors red red darkblue darkblue\n\n\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Visualization and exploration of ChIP-seq data"
        }, 
        {
            "location": "/09_data_visualization/#learning-objectives", 
            "text": "Generate bigWig files  Visualizing enrichment patterns at particular locations in the genome  Evaluating regions of differential enrichment using  bedtools", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/09_data_visualization/#visualization-of-chip-seq-data", 
            "text": "The first part of ChIP-sequencing analysis uses common processing pipelines, which involves the alignment of raw reads to the genome, data filtering, and identification of enriched signal regions (peak calling). In the second stage, individual programs allow detailed analysis of those peaks, biological interpretation, and visualization of ChIP-seq results.  There are various strategies for visualizing enrichment patterns and we will explore a few of them. To start, we will create bigWig files for our samples, a standard file format commonly used for ChIP-seq data visualization.", 
            "title": "Visualization of ChIP-seq data"
        }, 
        {
            "location": "/09_data_visualization/#creating-bigwig-files", 
            "text": "The first thing we want to do is take our alignment files (BAM) and convert them into bigWig files. The bigWig format is an indexed binary format useful for dense, continuous data that will be displayed in a genome browser as a graph/track, but also is used as input for some of the visualization commands we will be running in  deepTools .   deepTools , is a suite of Python tools developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq.  deepTools  has a wide variety of commands that go beyond those that are covered in this lesson. We encourage you to look through the docuementation and explore on your own time.    Image acquired from  deepTools documentation  pages  Start an interactive session with 6 cores.  If you are already logged on to a compute node you will want to exit and start a new session .  $ srun --pty -p short -t 0-12:00 --mem 8G -n 6 --reservation=HBC bash  We will begin by creating a directory for the visualization output and loading the required modules to run  deepTools .  $ cd ~/chipseq/results/\n$ mkdir -p visualization/bigWig visualization/figures  $ module load gcc/6.2.0  python/2.7.12\n$ module load deeptools/2.5.3   One last thing we need to do is  create an index file for each one of our BAM files . To perform some functions on the BAM file, many tools require an index. Think of an index located at the back of a textbook. When you are interested in a particular subject area you look for the keyword in the index and identify the pages that contain the relevant information. Similarily, indexing the BAM file aims to achieve fast retrieval of alignments overlapping a specified region without going through the whole alignment file.   In order to index a BAM file, we will use  SAMtools , a tool suite that provides alot of functionality in dealing with alignment files. There is a command called  samtools index , which is what we will use. Since we need an index for each of our BAM files, we will put this in a  for  loop to avoid having to run the same command multiple times.  First, let's load the module:  $ module load samtools/1.3.1  Now, at the command prompt start the  for  loop :  \nfor file in ~/chipseq/results/bowtie2/*aln.bam\ndo\nsamtools index $file\ndone   NOTE:  The above is assuming that you are pressing return after each line of code. If you wanted you could also run this command as a single line:  $ for file in ~/chipseq/results/bowtie2/*aln.bam; do samtools index $file; done    Now, to create our bigWig files there are two tools that can be useful:  bamCoverage  and  bamCompare . The former will take in a single BAM file and return to you a bigWig file. The latter allows you to normalize two files to each other (i.e. ChIP sample relative to input) and will return a single bigWig file.  Let's  create a bigWig file for Nanog replicate 2  using the  bamCoverage  command. In addition to the input and output files, there are a few additional parameters we have added.    normalizeTo1x : Report read coverage normalized to 1x sequencing depth (also known as Reads Per Genomic Content (RPGC)). Sequencing depth is defined as: (total number of mapped reads * fragment length) / effective genome size). So  the number provided here represents the effective genome size . Some examples of values for commonly used organisms can be  found here .  binSize : size of bins in bases  smoothLength : defines a window, larger than the  binSize , to average the number of reads over. This helps produce a more continuous plot.  centerReads : reads are centered with respect to the fragment length as specified by  extendReads . This option is useful to get a sharper signal around enriched regions.   $ bamCoverage -b bowtie2/H1hesc_Nanog_Rep2_aln.bam \\\n-o visualization/bigWig/H1hesc_Nanog_Rep2.bw \\\n--binSize 20 \\\n--normalizeTo1x 130000000 \\\n--smoothLength 60 \\\n--extendReads 150 \\\n--centerReads \\\n-p 6 2  ../logs/Nanog_rep2_bamCoverage.log  We can do the same for the  Pou5f1 replicate 1 :  $ bamCoverage -b bowtie2/H1hesc_Pou5f1_Rep1_aln.bam \\\n-o visualization/bigWig/H1hesc_Pou5f1_Rep1.bw \\\n--binSize 20 \\\n--normalizeTo1x 130000000 \\\n--smoothLength 60 \\\n--extendReads 150 \\\n--centerReads \\\n-p 6 2  ../logs/Pou5f1_rep1_bamCoverage.log   NOTE:  There is a reason we chose the specific replicates for the above commands, and it will become more obvious as we get to the end of this lesson!   Now, if we wanted to  create a bigWig file in which we normalize the ChIP against the input  we would use  bamCompare . The command is quite similar to  bamCoverage , the only difference being you require two files as input ( b1  and  b2 ).  ## DO NOT RUN THIS\n\n$ bamCompare -b1 bowtie2/H1hesc_Pou5f1_Rep1_aln.bam \\\n-b2 bowtie2/H1hesc_Input_Rep1_chr12_aln.bam \\\n-o visualization/bigWig/H1hesc_Pou5f1_Rep1_bgNorm.bw \\\n--binSize 20 \\\n--normalizeTo1x 130000000 \\\n--smoothLength 60 \\\n--extendReads 150 \\\n--centerReads \\\n-p 6 2  ../logs/Pou5f1_rep1_bamCompare.log   NOTE:  When you are creating bigWig files for your full dataset, this will take considerably longer and you will not want to run this interactively (except for testing purposes). Instead, you might want to consider writing a job submission script with a loop that runs this command over all of your BAM files.   Since we are using a toy dataset which contains only a subset of the data, using these bigWigs for visualization would not give us meaningful results. As such,  we have created bigWig files from the full dataset that you can use for the rest of this lesson.", 
            "title": "Creating bigWig files"
        }, 
        {
            "location": "/09_data_visualization/#profile-plots-and-heatmaps", 
            "text": "Once you have bigWig files you can use them to get a global look at enrichment patterns in your data at specified regions. In our example, we will assess enrichment around the TSS and plot this separately for the Nanog and Pou5f1 samples (two replicates in each plot).   Rather than looking at the TSS for all known genes, we will only look be looking at genes on chromosome 12 in the interest of time. Copy over the BED file which contains the coordinates for all genes on chromosome 12 to the visualization folder.  $ cp /n/groups/hbctraining/chip-seq/deepTools/chr12_genes.bed ~/chipseq/results/visualization/  Before we start plotting our data, we first need to prepare an intermediate file that can be used with the  plotHeatmap  and  plotProfile  commands.   The  computeMatrix  command accepts multiple bigWig files and multiple region files (BED format) to create a count matrix which is the intermediate file. It can also be used to filter and sort regions according to their score. Our region file will be the BED file we just copied over and our bigWog files will be those generated from the full dataset that we have provided for you. Additionally, We will specify a window of +/- 1000bp around the TSS of genes ( -b  and  -a ). For each window,  computeMatrix  will calculate scores based on the read density values in the bigWig files.  First, let's create a matrix one for the Nanog replicates:  \n$ computeMatrix reference-point --referencePoint TSS \\\n-b 1000 -a 1000 \\\n-R ~/chipseq/results/visualization/chr12_genes.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Nanog*.bw \\\n--skipZeros \\\n-o ~/chipseq/results/visualization/matrixNanog_TSS_chr12.gz \\\n--outFileSortedRegions ~/chipseq/results/visualization/regions_TSS_chr12.bed   NOTE:  Typically, the genome regions are genes, and can be obtained from the  UCSC table browser . Alternatively, you could look at other regions of interest that are not genomic feature related (i.e. binding regions from another protein of interest).   Now, let's create another matrix for the Pou5f1 replicates:  \n$ computeMatrix reference-point --referencePoint TSS \\\n-b 1000 -a 1000 \\\n-R ~/chipseq/results/visualization/chr12_genes.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Pou5f1*.bw \\\n--skipZeros -o ~/chipseq/results/visualization/matrixPou5f1_TSS_chr12.gz \\\n--outFileSortedRegions ~/chipseq/results/visualization/regionsPou5f1_TSS_chr12.bed  Using that matrix we can create a  profile plot  which is essentially a density plot that evaluates read density across all transcription start sites. For Nanog, we can see that  Replicate 2 has a particularly higher amount of signal at the TSS compared to Replicate 1 .   $ plotProfile -m visualization/matrixNanog_TSS_chr12.gz \\\n-out visualization/figures/TSS_Nanog_profile.png \\\n--perGroup \\\n--colors green purple \\\n--plotTitle   --samplesLabel  Rep1   Rep2  \\\n--refPointLabel  TSS  \\\n-T  Nanog read density  \\\n-z     Alternatively, we could use a  heatmap  to evaluate the same matrix of information:  $ plotHeatmap -m visualization/matrixNanog_TSS_chr12.gz \\\n-out visualization/figures/TSS_Nanog_heatmap.png \\\n--colorMap RdBu \\\n--whatToShow 'heatmap and colorbar' \\\n--zMin -4 --zMax 4     Similarly we can do the same for  Pou5f1. Here, we find that Replicate 1 exhibits stronger signal .  $ plotProfile -m visualization/matrixPou5f1_TSS_chr12.gz \\\n-out visualization/figures/TSS_Pou5f1_profile.png \\\n--perGroup --colors green purple \\\n--plotTitle   --samplesLabel  Rep1   Rep2  \\\n--refPointLabel  TSS  -T  Pou5f1 read density  -z     $ plotHeatmap -m visualization/matrixPou5f1_TSS_chr12.gz \\\n-out visualization/figures/TSS_Pou5f1_heatmap.png \\\n--colorMap RdBu \\\n--whatToShow 'heatmap and colorbar' \\\n--zMin -2 --zMax 2     If we wanted  both images in one single plot , we can do that with  plotHeatmap  and just removing the  --whatToShow  parameter.  $ plotHeatmap -m visualization/matrixPou5f1_TSS_chr12.gz \\\n-out visualization/figures/TSS_Pou5f1_heatmap.png \\\n--colorMap RdBu \\\n--zMin -2 --zMax 2      NOTE:  Both  plotProfile  and  plotHeatmap  have many options, including the ability to change the type of lines plotted and to plot by group rather than sample. Explore the documentation to find out more detail.", 
            "title": "Profile plots and heatmaps"
        }, 
        {
            "location": "/09_data_visualization/#differential-enrichment", 
            "text": "NOTE:  Identifying differential binding sites across multiple conditions has become of practical importance in biological and medical research and more tools have become available for this type of analysis.  For each group  we have two replicates, and it would be best to use tools that make use of these replicates (i.e  DiffBind ,  ChIPComp ) to compute statistics reflecting how significant the changes are. If you are interested in learning more, we have a  lesson on DiffBind  analysis using this same dataset.   To provide a more complex picture of biological processes in a cell, many studies aim to compare different datasets obtained by ChIP-seq. In our dataset, we have peak calls from two different transcription factors: Nanog and Pou5f1. To look at the differences in binding between the two we will use  bedtools .  You may already have the module loaded, but in case you don't you will need to load it:  $ module list # check which modules you have listed\n\n$ module load gcc/6.2.0 bedtools/2.26.0   Before using bedtools to obtain the overlap, we need to combine the information from both replicates. We will do this by concatenating ( cat ) the peak calls into a single file.", 
            "title": "Differential enrichment"
        }, 
        {
            "location": "/09_data_visualization/#combining-the-replicates", 
            "text": "$ cat macs2/Nanog-rep1_peaks.narrowPeak macs2/Nanog-rep2_peaks.narrowPeak   macs2/Nanog_combined.narrowPeak\n\n$ cat macs2/Pou5f1-rep1_peaks.narrowPeak macs2/Pou5f1-rep2_peaks.narrowPeak   macs2/Pou5f1_combined.narrowPeak", 
            "title": "Combining the replicates"
        }, 
        {
            "location": "/09_data_visualization/#merge-peaks-within-a-file", 
            "text": "Now for each for each of those combined peak files we need to merge regions that are overlapping. However,  bedtools merge   requires a sorted file  as input as specified in the help documentation.    \n$ bedtools merge -h", 
            "title": "Merge peaks within a file"
        }, 
        {
            "location": "/09_data_visualization/#sort-peaks-based-on-coordinates", 
            "text": "The  sort  command allows you to sort lines of text in a file. However,  when you have multiple columns that you are sorting on (and not on the entire line) you need to specify where sort keys start and where they end . In our case we have two columns of numeric values that we want to sort on, the chromosome and the start coordinates (column 1 and 2). The  -k1,1  indicates first sort on the first column, and the  -k2,2n  is to specify sort next on the second column and that this column is numeric ( n ).  We will sort the file and pipe ( | ) the output to  less  to take a quick peek at the sorted file.  \n$ sort -k1,1 -k2,2n macs2/Nanog_combined.narrowPeak | less", 
            "title": "Sort peaks based on coordinates"
        }, 
        {
            "location": "/09_data_visualization/#sort-peaks-and-then-merge", 
            "text": "Let's  start with the Nanog replicates:  $ sort -k1,1 -k2,2n macs2/Nanog_combined.narrowPeak | bedtools merge -i -   bedtools/Nanog_merged.bed    NOTE:  this command modifies your  narrowPeak  file into a simple, 3-column  bed  file.   Now, we'll do the same for Pou5f1:  $ sort -k1,1 -k2,2n macs2/Pou5f1_combined.narrowPeak | bedtools merge -i -   bedtools/Pou5f1_merged.bed    NOTE:  You could also use the IDR-optimized set of peaks we generated, instead of combining and merging. In our case, because we are looking at a small subset of the data the number of IDR peaks is very low and so this will give us more peaks as a starting point to evaluate the differences.", 
            "title": "Sort peaks and then merge"
        }, 
        {
            "location": "/09_data_visualization/#looking-for-differences-in-enrichment-between-nanog-and-pou5f1", 
            "text": "The  bedtools intersect  will report back the peaks that are overlapping in the file defined in  b  with respect to the file defined as  a  in the command. However, if we add the modifier  -v , this will report only those entries in A that have  no overlaps  with B. In this first example, we will obtain peaks that are only present in Nanog samples.   $ bedtools intersect -h\n\n$ bedtools intersect -a bedtools/Nanog_merged.bed -b bedtools/Pou5f1_merged.bed -v   bedtools/Nanog_only_peaks.bed  If we reverse the files listed for  -a  and  -b , this will now give us peaks that are only present in Pou5f1:  $ bedtools intersect -a bedtools/Pou5f1_merged.bed -b bedtools/Nanog_merged.bed -v   bedtools/Pou5f1_only_peaks.bed  How many peaks are unique to Nanog?  $ wc -l bedtools/Nanog_only_peaks.bed  How many peaks are unique to Pou5f1?  $ wc -l bedtools/Pou5f1_only_peaks.bed  We could  visualize read density at these sites by using some of the  deepTools  commands we had explored previously.  \n $ computeMatrix scale-regions \\\n-R ~/chipseq/results/bedtools/Nanog_only_peaks.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Pou5f1*.bw /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Nanog*.bw \\\n--skipZeros -p 6 \\\n-a 500 -b 500 \\\n-o ~/chipseq/results/visualization/matrixAll_Nanog_binding_sites.gz\n\n\n$ plotProfile -m visualization/matrixAll_Nanog_binding_sites.gz \\\n-out visualization/figures/Allsamples_NanogSites_profile.png \\\n--perGroup  --plotTitle   \\\n--samplesLabel  Pou5f1-Rep1   Pou5f1-Rep2   Nanog-Rep1   Nanog-Rep2  \\\n-T  Nanog only binding sites   -z   \\\n--startLabel   --endLabel   \\\n--colors red red darkblue darkblue   \n $ computeMatrix scale-regions \\\n-R ~/chipseq/results/bedtools/Pou5f1_only_peaks.bed \\\n-S /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Pou5f1*.bw /n/groups/hbctraining/chip-seq/full-dataset/bigWig/Encode_Nanog*.bw \\\n--skipZeros -p 6 \\\n-a 500 -b 500 \\\n-o ~/chipseq/results/visualization/matrixAll_Pou5f1_binding_sites.gz \n\n\n$ plotProfile -m visualization/matrixAll_Pou5f1_binding_sites.gz \\\n-out visualization/figures/Allsamples_Pou5f1Sites_profile.png \\\n--perGroup  --plotTitle   \\\n--samplesLabel  Pou5f1-Rep1   Pou5f1-Rep2   Nanog-Rep1   Nanog-Rep2  \\\n-T  Pou5f1 only binding sites   -z   \\\n--startLabel   --endLabel   \\\n--colors red red darkblue darkblue    This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Looking for differences in enrichment between Nanog and Pou5f1"
        }, 
        {
            "location": "/10_qualitative_assessment_IGV/", 
            "text": "Approximate time: 25 minutes\n\n\nLearning Objectives\n\n\n\n\nUse IGV to visualize BigWig, BED and data from ENCODE\n\n\n\n\nQualitative assessment using IGV\n\n\nAnother method for assessing the quality of your alignment is to visualize the alignment using a genome browser. For this workshop we will be using the \nIntegrative Genomics Viewer (IGV)\n from the Broad Institute. \nYou should already have this downloaded on your laptop.\n IGV is an interactive tool which allows exploration of large, integrated genomic datasets. It supports a wide variety of data types, including array-based and next-generation sequence data, and genomic annotations, which facilitates invaluable comparisons.\n\n\nTransfer files\n\n\nIn order to visualize our ChIP-seq enrichment we will first need to move over the bigWig files. We previously used \nFileZilla\n to transfer files from O2 to your laptop and so we will do the same for these files.\n\n\n\n\nNOTE:\n  There is another way to do so using the command line interface. Similar to the \ncp\n command to copy there is a command that allows you to securely copy files between computers. \nThe command is called \nscp\n and allows files to be copied to, from, or between different hosts.\n It uses ssh for data transfer and provides the same authentication and same level of security as ssh. The first argument is the location on the remote server and the second argument is the destination on your local machine. \n\n\n$ scp username@transfer.rc.hms.harvard.edu:/path/to/file_on_O2 Path/to/directory/local_machine\n\n\n\n\nOpen up \nFileZilla\n and connect to the transfer node on O2. Navigate to the correct directory on the cluster panel and copy over the following files:\n\n\n\n\nNanog-Rep2: \n~/chipseq/results/visualization/bigWig/H1hesc_Nanog_Rep2.bw\n\n\nPou5f1-Rep1: \n~/chipseq/results/visualization/bigWig/H1hesc_Pou5f1_Rep1.bw\n\n\nNanog-only BED: \n~/chipseq/results/bedtools/Nanog_only_peaks.bed\n\n\nPou5f1-only BED: \n~/chipseq/results/bedtools/Pou5f1_only_peaks.bed\n\n\n\n\n\n\nNOTE:\n We are copying over only a single sample bigWig from each group. Since we observed that in each case there was a stronger replicate (high read density) that is what we used to make our selection.\n\n\n\n\nStart \nIGV\n \nYou should have this previously installed on your laptop.\n\n\n\n\nLoad the Human genome (hg19) into IGV using the dropdown menu at the top left of your screen. \nNote: there is also an option to \"Load Genomes from File...\" under the \"Genomes\" pull-down menu - this is useful when working with non-model organisms\n\n\nLoad the bigWig files and BED files using the \n\"Load from File...\"\n option under the \n\"File\"\n pull-down menu. \n\n\n\n\nYour IGV interface should now look something like the screenshot below. By default, you will be in a zoomed out view. You will notice that for both bigWig tracks there appears to be a dense blue chunk at the beginning of chromosome 12, which makes sense considering the subsetted toy dataset we are working with. \nUse the pulldown menu to zoom into chromosome 12.\n\n\n\n\n\n\nBefore we start looking at specific genes you will want to \nAutoscale\n each track.\n\n\n\n\nRight click on the left-hand side panel. You should see many options available to you. If \"Autoscale\" is not checked go ahead and do this. You can do the same for both bigWig tracks.\n\n\n\n\n\n\nThe interaction between Pou5f1, and Nanog is supported by immunoprecipitation, functional analysis, and co-localization of binding sites, and so it is not surprising that they share the \nsame target genes\n. While our BED files congtain peaks that are non-overlapping, the target genes that harbor these peaks can still be overlapping.\n\n\n\n\nFor example take a look at \nSox5\n (use the search box to zoom into the gene). How many peaks are associated with this gene for Nanog? For Pou5f1?\n\n\nHow convincing is the difference in enrichment?\n\n\n\n\nFor the \nErc1\n gene, there is only one peak associated with this gene. Which factor binds here? Is the differential enrichment obvious?\n\n\n\n\n\n\nFinally, we are going to visually \ncompare our data\n to the output from the \nfull dataset from ENCODE\n, by loading that data from the IGV server.\n\n\n\n\n\n\n\n\nThis lesson has been developed by members of the teaching team at the \nHarvard Chan Bioinformatics Core (HBC)\n. These are open access materials distributed under the terms of the \nCreative Commons Attribution license\n (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Qualitative assessment using IGV"
        }, 
        {
            "location": "/10_qualitative_assessment_IGV/#learning-objectives", 
            "text": "Use IGV to visualize BigWig, BED and data from ENCODE", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/10_qualitative_assessment_IGV/#qualitative-assessment-using-igv", 
            "text": "Another method for assessing the quality of your alignment is to visualize the alignment using a genome browser. For this workshop we will be using the  Integrative Genomics Viewer (IGV)  from the Broad Institute.  You should already have this downloaded on your laptop.  IGV is an interactive tool which allows exploration of large, integrated genomic datasets. It supports a wide variety of data types, including array-based and next-generation sequence data, and genomic annotations, which facilitates invaluable comparisons.", 
            "title": "Qualitative assessment using IGV"
        }, 
        {
            "location": "/10_qualitative_assessment_IGV/#transfer-files", 
            "text": "In order to visualize our ChIP-seq enrichment we will first need to move over the bigWig files. We previously used  FileZilla  to transfer files from O2 to your laptop and so we will do the same for these files.   NOTE:   There is another way to do so using the command line interface. Similar to the  cp  command to copy there is a command that allows you to securely copy files between computers.  The command is called  scp  and allows files to be copied to, from, or between different hosts.  It uses ssh for data transfer and provides the same authentication and same level of security as ssh. The first argument is the location on the remote server and the second argument is the destination on your local machine.   $ scp username@transfer.rc.hms.harvard.edu:/path/to/file_on_O2 Path/to/directory/local_machine   Open up  FileZilla  and connect to the transfer node on O2. Navigate to the correct directory on the cluster panel and copy over the following files:   Nanog-Rep2:  ~/chipseq/results/visualization/bigWig/H1hesc_Nanog_Rep2.bw  Pou5f1-Rep1:  ~/chipseq/results/visualization/bigWig/H1hesc_Pou5f1_Rep1.bw  Nanog-only BED:  ~/chipseq/results/bedtools/Nanog_only_peaks.bed  Pou5f1-only BED:  ~/chipseq/results/bedtools/Pou5f1_only_peaks.bed    NOTE:  We are copying over only a single sample bigWig from each group. Since we observed that in each case there was a stronger replicate (high read density) that is what we used to make our selection.   Start  IGV   You should have this previously installed on your laptop.   Load the Human genome (hg19) into IGV using the dropdown menu at the top left of your screen.  Note: there is also an option to \"Load Genomes from File...\" under the \"Genomes\" pull-down menu - this is useful when working with non-model organisms  Load the bigWig files and BED files using the  \"Load from File...\"  option under the  \"File\"  pull-down menu.    Your IGV interface should now look something like the screenshot below. By default, you will be in a zoomed out view. You will notice that for both bigWig tracks there appears to be a dense blue chunk at the beginning of chromosome 12, which makes sense considering the subsetted toy dataset we are working with.  Use the pulldown menu to zoom into chromosome 12.    Before we start looking at specific genes you will want to  Autoscale  each track.   Right click on the left-hand side panel. You should see many options available to you. If \"Autoscale\" is not checked go ahead and do this. You can do the same for both bigWig tracks.    The interaction between Pou5f1, and Nanog is supported by immunoprecipitation, functional analysis, and co-localization of binding sites, and so it is not surprising that they share the  same target genes . While our BED files congtain peaks that are non-overlapping, the target genes that harbor these peaks can still be overlapping.   For example take a look at  Sox5  (use the search box to zoom into the gene). How many peaks are associated with this gene for Nanog? For Pou5f1?  How convincing is the difference in enrichment?   For the  Erc1  gene, there is only one peak associated with this gene. Which factor binds here? Is the differential enrichment obvious?    Finally, we are going to visually  compare our data  to the output from the  full dataset from ENCODE , by loading that data from the IGV server.     This lesson has been developed by members of the teaching team at the  Harvard Chan Bioinformatics Core (HBC) . These are open access materials distributed under the terms of the  Creative Commons Attribution license  (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", 
            "title": "Transfer files"
        }, 
        {
            "location": "/11_functional_analysis/", 
            "text": "Approximate time: 40 minutes\n\n\nLearning Objectives\n\n\n\n\nto explore web-based tools for motif discovery and functional enrichment analysis of the peak calls\n\n\n\n\nWeb-based functional analysis and motif discovery\n\n\n\n\nWe have identified regions of the genome where the number of reads aligning to these areas differ significantly between our Nanog IP samples and the input controls. These enriched regions represent the likely locations of where Nanog binds to the genome. \n\n\nAfter identifying likely binding sites, downstream analyses will often include: \n\n\n\n\ndetermining the binding motifs for the protein of interest\n\n\nidentifying which genes are associated with the binding sites and exploring whether there is any associated enrichment of processes, pathways, or networks.\n\n\n\n\nWe will explore a few useful web-based tools for performing these analyses using our Nanog peak calls.\n\n\nSince the motif and functional enrichment analyses are unlikely to give reliable results using only the 32.8 Mb of reads mapping to chr12,  we will use the \nfull set of peak calls output from the IDR analysis\n.\n\n\nSet-up\n\n\nStart an interactive session:\n\n\n$ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash    \n\n\n\n\nExtract the first three columns of the IDR peak calls for the whole genome of Nanog:\n\n\n$ cd ~/chipseq/results\n\n$ mkdir functional_analysis\n\n$ cd functional_analysis\n\n$ cp /n/groups/hbctraining/chip-seq/full-dataset/idr/*.bed .\n\n$ cut -f 1,2,3 Nanog-idr-merged.bed  \n Nanog-idr-merged-great.bed\n\n\n\n\nTo extract the sequences corresponding to the peak coordinates for motif discovery, we will use the \nbedtools\n suite of tools. The \ngetfasta\n command extracts sequences from a reference fasta file for each of the coordinates defined in a BED/GFF/VCF file. \n\n\n$ module load gcc/6.2.0 bedtools/2.26.0\n\n$ bedtools getfasta -fi \\\n/n/groups/shared_databases/igenome/Homo_sapiens/UCSC/hg19/Sequence/WholeGenomeFasta/genome.fa \\\n-bed Nanog-idr-merged-great.bed \\\n-fo Nanog-idr-merged-dreme.fasta\n\n\n\n\nUsing \nscp\n or \nFileZilla\n on your local computer, transfer \nNanog-idr-merged-great.bed\n and \nNanog-idr-merged-dreme.fasta\n to your Desktop.\n\n\n$ scp username@transfer.rc.hms.harvard.edu:~/chipseq/results/functional_analysis/*merged-* Desktop/\n\n\n\n\nFunctional enrichment analysis\n\n\nWe will use \nGREAT\n to perform the functional enrichment analysis. GREAT takes a list of regions, associates them with nearby genes, and then analyzes the gene annotations to assign biological meaning to the data.\n\n\nOpen \nGREAT\n, and perform the following steps:\n\n\n\n\n\n\nChoose the \nNanog-idr-merged-great.bed\n file and use the \nWhole genome\n for Background regions. Click Submit. GREAT provides the output in HTML format organized by section.\n\n\n\n\n\n\nExpand the \nJob Description\n section. Click on \nView all genomic region-gene associations\n. Note that each associated gene is listed with location from the transcription start site as shown below:\n\n\n\n\nWithin this section, you have the option to download the list of genes associated with Nanog binding sites or you could view all of the binding sites as a custom track in the UCSC Genome Browser.\n\n\n\n\n\n\nScroll down to the \nRegion-Gene Association Graphs\n. Observe the graphics displaying the summary of the number of genes associated with each binding site and the binding site locations relative to the transcription start sites of the associated genes\n\n\n\n\n\n\n\n\nBelow the \nRegion-Gene Association Graphs\n are the \nGlobal Controls\n, where you can select the annotation information to display. Keep the default settings and scroll down to view the information displayed. \n\n\n\n\n\n\nExplore the GO Biological Process terms associated with the Nanog binding sites. Notice the options available at the top of the tables for exporting data, changing settings, and visualization.\n\n\n\n\nGREAT calculates two measures of statistical enrichment: \"one using a binomial test over genomic regions and one using a hypergeometric test over genes\" [\n2\n]. Each test has its own biases, which are compensated for by the other test. \n\n\n\n\n\n\nClick on the term \nnegative regulation of stem cell differentiation\n:\n\n\n\n\nNote that summary information about the binding sites of Nanog for genes associated with this GO term are displayed.\n\n\n\n\n\n\nExpand the section for \nThis term's genomic region-gene association tables\n. Notice that you have the option to download the gene table.\n\n\n\n\n\n\nClick on \nNOTCH1\n. Explore the binding regions directly within the UCSC Genome Browser.\n\n\n\n\n\n\nMotif discovery\n\n\n\n\nTo identify over-represented motifs, we will use DREME from the MEME suite of sequence analysis tools. \nDREME\n is a motif discovery algorithm designed to find short, core DNA-binding motifs of eukaryotic transcription factors and is optimized to handle large ChIP-seq data sets.\n\n\nDREME is tailored to eukaryotic data by focusing on short motifs (4 to 8 nucleotides) encompassing the DNA-binding region of most eukaryotic monomeric transcription factors. Therefore it may miss wider motifs due to binding by large transcription factor complexes.\n\n\nDREME\n\n\nVisit the \nDREME website\n and perform the following steps:\n\n\n\n\nSelect the downloaded \nNanog-idr-merged-dreme.fasta\n as input to DREME\n\n\nEnter your email address so that DREME can email you once the analysis is complete\n\n\nEnter a job description so you will recognize which job has been emailed to you and then start the search\n\n\n\n\nYou will be shown a status page describing the inputs and the selected parameters, as well as links to the results at the top of the screen.\n\n\n\n\nThis may take some time depending on the server load and the size of the file. While you wait, take a look at the \nexpected results\n.\n\n\n\n\nDREME\u2019s HTML output provides a list of Discovered Motifs displayed as sequence logos (in the forward and reverse complement (RC) orientations), along with an E-value for the significance of the result. \n\n\nMotifs are significantly enriched if the fraction of sequences in the input dataset matching the motif is significantly different from the fraction of sequences in the background dataset using Fisher\u2019s Exact Test. Typically, background dataset is either similar data from a different ChIP-Seq experiment or shuffled versions of the input dataset [\n1\n].\n\n\nClicking on \nMore\n displays the number of times the motif was identified in the Nanog dataset (positives) versus the background dataset (negatives). The \nDetails\n section displays the number of sequences matching the motif, while \nEnriched Matching Words\n displays the number of times the motif was identified in the sequences (more than one word possible per sequence).\n\n\nTomtom\n\n\nTo determine if the identified motifs resemble the binding motifs of known transcription factors, we can submit the motifs to Tomtom, which searches a database of known motifs to find potential matches and provides a statistical measure of motif-motif similarity. We can run the analysis individually for each motif prediction by performing the following steps:\n\n\n\n\nClick on the \nSubmit / Download\n button for motif \nATGYWAAT\n in the DREME output\n\n\nA dialog box will appear asking you to Select what you want to do or Select a program. Select \nTomtom\n and click \nSubmit\n. This takes you to the input page. \n\n\nTomtom allows you to select the database you wish to search against. Keep the default parameters selected, but keep in mind that there are other options when performing your own analyses.\n\n\nEnter your email address and job description and start the search.\n\n\n\n\nYou will be shown a status page describing the inputs and the selected parameters, as well as a link to the results at the top of the screen. Clicking the link displays an output page that is continually updated until the analysis has completed. Like DREME, Tomtom will also email you the results.\n\n\nThe HTML output for Tomtom shows a list of possible motif matches for the DREME motif prediction generated from your Nanog regions. Clicking on the match name shows you an alignment of the predicted motif versus the match.\n\n\n\n\nThe short genomic regions identified by ChIP-seq are generally very highly enriched with binding sites of the ChIP-ed transcription factor, but Nanog is not in the databases of known motifs. The regions identified also tend to be enriched for the binding sites of other transcription factors that bind \ncooperatively or competitively\n with the ChIP-ed transcription factor.\n\n\nIf we compare our results with what is known about our transcription factor, Nanog, we find that Sox2 and Pou5f1 (Oct4) co-regulate many of the same genes as Nanog. \n\n\nhttps://www.qiagen.com/us/shop/genes-and-pathways/pathway-details/?pwid=309\n\n\nMEME-ChIP\n\n\nMEME-ChIP is a tool that is part of the MEME Suite that is specifically designed for ChIP-Seq analyses. MEME-ChIP performs DREME and Tomtom analysis in addition to using tools to assess which motifs are most centrally enriched (motifs should be centered in the peaks) and to combine related motifs into similarity clusters. It is able to identify longer motifs \n 30bp, but takes much longer to run.\n\n\n\n\n\n\n\n\nOther functional analysis tools\n\n\nChIPseeker is an R package for annotating ChIP-seq data analysis. It supports annotating ChIP peaks and provides functions to visualize ChIP peaks coverage over chromosomes and profiles of peaks binding to TSS regions. Comparison of ChIP peak profiles and annotation are also supported, and can be useful to compare biological replicates. Several visualization functions are implemented to visualize the peak annotation and statistical tools for enrichment analyses of functional annotations. If interested, there are \nmaterials\n available for using ChIPseeker for functional analysis.", 
            "title": "Functional analysis"
        }, 
        {
            "location": "/11_functional_analysis/#learning-objectives", 
            "text": "to explore web-based tools for motif discovery and functional enrichment analysis of the peak calls", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/11_functional_analysis/#web-based-functional-analysis-and-motif-discovery", 
            "text": "We have identified regions of the genome where the number of reads aligning to these areas differ significantly between our Nanog IP samples and the input controls. These enriched regions represent the likely locations of where Nanog binds to the genome.   After identifying likely binding sites, downstream analyses will often include:    determining the binding motifs for the protein of interest  identifying which genes are associated with the binding sites and exploring whether there is any associated enrichment of processes, pathways, or networks.   We will explore a few useful web-based tools for performing these analyses using our Nanog peak calls.  Since the motif and functional enrichment analyses are unlikely to give reliable results using only the 32.8 Mb of reads mapping to chr12,  we will use the  full set of peak calls output from the IDR analysis .", 
            "title": "Web-based functional analysis and motif discovery"
        }, 
        {
            "location": "/11_functional_analysis/#set-up", 
            "text": "Start an interactive session:  $ srun --pty -p short -t 0-12:00 --mem 8G --reservation=HBC bash      Extract the first three columns of the IDR peak calls for the whole genome of Nanog:  $ cd ~/chipseq/results\n\n$ mkdir functional_analysis\n\n$ cd functional_analysis\n\n$ cp /n/groups/hbctraining/chip-seq/full-dataset/idr/*.bed .\n\n$ cut -f 1,2,3 Nanog-idr-merged.bed    Nanog-idr-merged-great.bed  To extract the sequences corresponding to the peak coordinates for motif discovery, we will use the  bedtools  suite of tools. The  getfasta  command extracts sequences from a reference fasta file for each of the coordinates defined in a BED/GFF/VCF file.   $ module load gcc/6.2.0 bedtools/2.26.0\n\n$ bedtools getfasta -fi \\\n/n/groups/shared_databases/igenome/Homo_sapiens/UCSC/hg19/Sequence/WholeGenomeFasta/genome.fa \\\n-bed Nanog-idr-merged-great.bed \\\n-fo Nanog-idr-merged-dreme.fasta  Using  scp  or  FileZilla  on your local computer, transfer  Nanog-idr-merged-great.bed  and  Nanog-idr-merged-dreme.fasta  to your Desktop.  $ scp username@transfer.rc.hms.harvard.edu:~/chipseq/results/functional_analysis/*merged-* Desktop/", 
            "title": "Set-up"
        }, 
        {
            "location": "/11_functional_analysis/#functional-enrichment-analysis", 
            "text": "We will use  GREAT  to perform the functional enrichment analysis. GREAT takes a list of regions, associates them with nearby genes, and then analyzes the gene annotations to assign biological meaning to the data.  Open  GREAT , and perform the following steps:    Choose the  Nanog-idr-merged-great.bed  file and use the  Whole genome  for Background regions. Click Submit. GREAT provides the output in HTML format organized by section.    Expand the  Job Description  section. Click on  View all genomic region-gene associations . Note that each associated gene is listed with location from the transcription start site as shown below:   Within this section, you have the option to download the list of genes associated with Nanog binding sites or you could view all of the binding sites as a custom track in the UCSC Genome Browser.    Scroll down to the  Region-Gene Association Graphs . Observe the graphics displaying the summary of the number of genes associated with each binding site and the binding site locations relative to the transcription start sites of the associated genes     Below the  Region-Gene Association Graphs  are the  Global Controls , where you can select the annotation information to display. Keep the default settings and scroll down to view the information displayed.     Explore the GO Biological Process terms associated with the Nanog binding sites. Notice the options available at the top of the tables for exporting data, changing settings, and visualization.   GREAT calculates two measures of statistical enrichment: \"one using a binomial test over genomic regions and one using a hypergeometric test over genes\" [ 2 ]. Each test has its own biases, which are compensated for by the other test.     Click on the term  negative regulation of stem cell differentiation :   Note that summary information about the binding sites of Nanog for genes associated with this GO term are displayed.    Expand the section for  This term's genomic region-gene association tables . Notice that you have the option to download the gene table.    Click on  NOTCH1 . Explore the binding regions directly within the UCSC Genome Browser.", 
            "title": "Functional enrichment analysis"
        }, 
        {
            "location": "/11_functional_analysis/#motif-discovery", 
            "text": "To identify over-represented motifs, we will use DREME from the MEME suite of sequence analysis tools.  DREME  is a motif discovery algorithm designed to find short, core DNA-binding motifs of eukaryotic transcription factors and is optimized to handle large ChIP-seq data sets.  DREME is tailored to eukaryotic data by focusing on short motifs (4 to 8 nucleotides) encompassing the DNA-binding region of most eukaryotic monomeric transcription factors. Therefore it may miss wider motifs due to binding by large transcription factor complexes.", 
            "title": "Motif discovery"
        }, 
        {
            "location": "/11_functional_analysis/#dreme", 
            "text": "Visit the  DREME website  and perform the following steps:   Select the downloaded  Nanog-idr-merged-dreme.fasta  as input to DREME  Enter your email address so that DREME can email you once the analysis is complete  Enter a job description so you will recognize which job has been emailed to you and then start the search   You will be shown a status page describing the inputs and the selected parameters, as well as links to the results at the top of the screen.   This may take some time depending on the server load and the size of the file. While you wait, take a look at the  expected results .   DREME\u2019s HTML output provides a list of Discovered Motifs displayed as sequence logos (in the forward and reverse complement (RC) orientations), along with an E-value for the significance of the result.   Motifs are significantly enriched if the fraction of sequences in the input dataset matching the motif is significantly different from the fraction of sequences in the background dataset using Fisher\u2019s Exact Test. Typically, background dataset is either similar data from a different ChIP-Seq experiment or shuffled versions of the input dataset [ 1 ].  Clicking on  More  displays the number of times the motif was identified in the Nanog dataset (positives) versus the background dataset (negatives). The  Details  section displays the number of sequences matching the motif, while  Enriched Matching Words  displays the number of times the motif was identified in the sequences (more than one word possible per sequence).", 
            "title": "DREME"
        }, 
        {
            "location": "/11_functional_analysis/#tomtom", 
            "text": "To determine if the identified motifs resemble the binding motifs of known transcription factors, we can submit the motifs to Tomtom, which searches a database of known motifs to find potential matches and provides a statistical measure of motif-motif similarity. We can run the analysis individually for each motif prediction by performing the following steps:   Click on the  Submit / Download  button for motif  ATGYWAAT  in the DREME output  A dialog box will appear asking you to Select what you want to do or Select a program. Select  Tomtom  and click  Submit . This takes you to the input page.   Tomtom allows you to select the database you wish to search against. Keep the default parameters selected, but keep in mind that there are other options when performing your own analyses.  Enter your email address and job description and start the search.   You will be shown a status page describing the inputs and the selected parameters, as well as a link to the results at the top of the screen. Clicking the link displays an output page that is continually updated until the analysis has completed. Like DREME, Tomtom will also email you the results.  The HTML output for Tomtom shows a list of possible motif matches for the DREME motif prediction generated from your Nanog regions. Clicking on the match name shows you an alignment of the predicted motif versus the match.   The short genomic regions identified by ChIP-seq are generally very highly enriched with binding sites of the ChIP-ed transcription factor, but Nanog is not in the databases of known motifs. The regions identified also tend to be enriched for the binding sites of other transcription factors that bind  cooperatively or competitively  with the ChIP-ed transcription factor.  If we compare our results with what is known about our transcription factor, Nanog, we find that Sox2 and Pou5f1 (Oct4) co-regulate many of the same genes as Nanog.   https://www.qiagen.com/us/shop/genes-and-pathways/pathway-details/?pwid=309", 
            "title": "Tomtom"
        }, 
        {
            "location": "/11_functional_analysis/#meme-chip", 
            "text": "MEME-ChIP is a tool that is part of the MEME Suite that is specifically designed for ChIP-Seq analyses. MEME-ChIP performs DREME and Tomtom analysis in addition to using tools to assess which motifs are most centrally enriched (motifs should be centered in the peaks) and to combine related motifs into similarity clusters. It is able to identify longer motifs   30bp, but takes much longer to run.", 
            "title": "MEME-ChIP"
        }, 
        {
            "location": "/11_functional_analysis/#other-functional-analysis-tools", 
            "text": "ChIPseeker is an R package for annotating ChIP-seq data analysis. It supports annotating ChIP peaks and provides functions to visualize ChIP peaks coverage over chromosomes and profiles of peaks binding to TSS regions. Comparison of ChIP peak profiles and annotation are also supported, and can be useful to compare biological replicates. Several visualization functions are implemented to visualize the peak annotation and statistical tools for enrichment analyses of functional annotations. If interested, there are  materials  available for using ChIPseeker for functional analysis.", 
            "title": "Other functional analysis tools"
        }
    ]
}